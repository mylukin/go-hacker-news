# Hacker News 故事摘要 - 2025-10-21

## 今日概述

Today’s top Hacker News stories focus on saving money by leaving big cloud platforms, failed predictions about the end of the world, and how to build a simple database. Other themes include new tools for 3D graphics, AI getting worse from bad data, and AI learning to understand audio. There are also stories about math tricks, debates on if computers can really “understand,” and NASA’s plans to send people to the moon.

---

## Replacing a $3000/mo Heroku bill with a $55/mo server

- 原文链接: [Replacing a $3000/mo Heroku bill with a $55/mo server](https://disco.cloud/blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55-server/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45661253)

This article talks about how Idealist.org cut their Heroku bill from $3,000 a month to just $55 by moving staging environments to their own server. The main problem was Heroku’s high per-environment cost, especially for critical staging setups that needed to copy production closely.

Heroku charged about $500 per staging environment, mostly because each needed its own web server, worker, and database add-on. Even with small servers, the price stayed high because costs grew with the number of environments, not just how big they were. Idealist.org only ran two permanent environments at first but wanted more for development and testing, which would have cost even more.

To save money, the team tried using a Hetzner server for $55 a month. With the Disco tool, they kept their easy “git push to deploy” workflow. All staging environments shared one Postgres database on the server, which was fine for testing and saved a lot by skipping Heroku’s managed database fees.

Disco gave them more than just Docker Compose. It handled deployments with no downtime, created free SSL certificates, and offered a web UI for managing logs and environments—all features that made life easier for developers.

After the switch, they could run six full staging environments on that one server, where they would have paid $3,000 monthly on Heroku. The server’s CPU and memory usage stayed very low, showing they still had plenty of room.

There were some downsides. They had to set up DNS and CDN by hand and manage their own server security and updates. Hetzner’s servers are mostly in Europe, which could be a problem for US production, but it was fine for staging. The team also needed to adjust their app’s network setup for Docker, which took about a day.

Six months later, this setup became their normal way of working. The biggest change was how staging environments felt: they were now easy and cheap, so any developer could create one when needed without asking for permission or worrying about cost.

In the comments, many readers praised the big cost savings and liked how staging became easy and disposable. Some said this kind of move makes sense for non-critical workloads, but warned that running production this way is riskier because you lose Heroku’s backups and support. Others pointed out that managing your own servers takes extra work, like security updates and monitoring, which some teams might not want to do.

A few people mentioned that Heroku’s high price is mostly for their support, uptime, and ease of use, which can be worth it for businesses with less technical staff or who want zero hassle. Some users suggested cheaper alternatives to Heroku, like Render or Fly.io, but agreed that a simple server can be much cheaper if the team is comfortable managing it.

One commenter noted that Hetzner’s network location could be a problem for US users if used in production but agreed it’s fine for staging. A few others shared similar stories of saving thousands by leaving big platforms for self-hosted solutions.

On the other hand, some readers warned about the hidden costs of self-hosting: time spent patching, backups, or handling outages. They said these tasks can distract from product work, so teams should think carefully before leaving a PaaS.

Overall, most agreed that for staging and test environments, this approach gives a big win in both cost and developer freedom.

---

## Doomsday Scoreboard

- 原文链接: [Doomsday Scoreboard](https://doomsday.march1studios.com/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45661084)

This website tracks all the times people have predicted the end of the world and been wrong. It lists hundreds of failed doomsday predictions, from ancient times to today, showing who made each prediction, what they said would happen, and what actually happened. For example, it explains how some people thought the world would end in the year 1000, or that a comet would destroy Earth, or that war or disaster would mark the end.

The site also shows predictions that are still "active" or pending, like the one from the book "The Fourth Turning," which says the U.S. will face a major crisis by 2026, or the "Limits to Growth" forecast, which warns about resource limits and collapse between 2020–2030. For each prediction, it gives the name of the person or group, the type of disaster expected (like war, plague, or judgement), and the belief system behind it (religious, scientific, or political). 

There are extra features, like a “scoreboard” counting all the failed and still pending predictions, and a timeline showing when each doomsday was supposed to happen. The site even lists scams and hoaxes, like the 1806 “prophetic hen,” where someone faked messages on eggs to trick people, or cases where leaders confessed that they made up predictions for fame or gifts. 

It’s clear from the data that almost every kind of prediction has failed: from Christian, Jewish, Muslim, Hindu, Buddhist, and New Age prophets, to political thinkers and even scientists. Some people kept moving the date when nothing happened, others said the event was “spiritual” instead of physical, or that their followers’ faith saved the world. New predictions keep appearing, but so far, none have come true.

In the Hacker News comments, many people say the site is funny and a bit sad, showing how people keep falling for the same ideas. Some say it’s a lesson in human psychology—people want to believe in big endings or in being special. Others note that even smart or well-educated people have made these predictions, and sometimes whole societies join in. Some users wonder why apocalyptic thinking is so common, while others joke that the only successful prediction would mean no one is left to record it. There are also comments about how prediction errors are a part of human nature, and some compare it to failed tech predictions or market “bubbles.” A few point out that even scientific forecasts can be wrong, but at least science updates its views, while doomsday cults just move the goalposts. Finally, some users say the site is a good reminder to be careful with bold predictions and to look for real evidence before believing big claims.

---

## Build Your Own Database

- 原文链接: [Build Your Own Database](https://www.nan.fyi/database)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45657827)

This article explains how to build a simple key-value database from scratch. It starts by showing how we can use a file to save and find key-value pairs, like a small dictionary where you look up a value using a key.

At first, you might just write all your data to one file. To update or delete something, you have to find it in the file and change it, which can be slow because you might need to move other data around. To fix this, the article suggests making your database “append-only.” This means you only add new records to the end of the file, and for updates or deletes, you just add new entries or special “delete” markers.

But this makes the file grow very big, and searching gets slow, because you need to scan the entire file for every lookup. To solve this, the article introduces splitting data into segments and “compacting” old files to remove useless or deleted records, which keeps the database smaller.

To make searching faster, you can use an in-memory index—a hash table that maps each key to its place in the file. This index lets you find data much faster, but it uses computer memory, so you can’t have too many keys. Also, it doesn’t help with range queries, like finding all keys between two numbers.

To do range queries better, the article suggests keeping your data sorted. If your data is sorted, you can use a “sparse” index, which keeps only some key positions in memory and still lets you find data faster. But sorting data on disk is slow, so the trick is to keep new data sorted in memory, and when you have enough, write it out to disk in order.

If the program crashes, you can lose the in-memory data, so you also write changes to a special file as a backup. Then you can flush sorted data to disk, and use indexes to find things quickly. Updates and deletes are still treated as new records, and sometimes you “compact” the disk files to clean up old data.

This method is called an LSM tree (Log-Structured Merge Tree), which is used in real systems like LevelDB and DynamoDB. LSM trees are fast and handle lots of data, but there are other ways to build a database, like using B-Trees.

In the comment section, many people liked the clear step-by-step approach. Some said building a database from scratch is a great learning exercise. Others pointed out that real-world databases solve even more problems, like handling crashes, concurrent access, and scaling for millions of users. A few readers wished for code examples to follow along. Some users discussed the trade-offs between LSM trees and B-Trees, saying each has its good and bad sides depending on the use case. One commenter explained that LSM trees are great for write-heavy workloads, but B-Trees can be better for read-heavy queries or range scans.

There were also comments about memory use: keeping an index in memory is fast but limits the number of keys you can store. Others suggested using more advanced indexing or compression to save space. A few people shared real-life stories about building simple databases for hobbies or work, and how tricky it can get as things grow. Finally, some readers pointed out that many modern databases mix ideas from both LSM trees and B-Trees to get the best of both worlds.

---

## rlsw – Raylib software OpenGL renderer in less than 5k LOC

- 原文链接: [rlsw – Raylib software OpenGL renderer in less than 5k LOC](https://github.com/raysan5/raylib/blob/master/src/external/rlsw.h)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45661638)

This post is about rlsw, a software renderer for OpenGL 1.1 that works inside the raylib library. It is written in C and has less than 5,000 lines of code, giving raylib programs a way to run on computers with no GPU. 

The rlsw renderer tries to copy the style and features of OpenGL 1.1, but everything runs in software on the CPU. It supports basic drawing modes like points, lines, triangles, and quads. It has its own framebuffer and can use different color and depth formats. You can use textures, and it supports several texture formats that raylib uses, plus features like point and bilinear filtering, and wrapping modes for S and T coordinates. The code also manages vertex arrays and a matrix stack, so you can do transforms like in OpenGL. 

Other features include scissor clipping, depth testing, blend modes, and face culling. It even has some GL-style getter functions and can resize the framebuffer. You can configure things like buffer sizes and texture limits with #defines before including the code. The design is meant to be simple to use and easy to plug into raylib or other projects. The license is MIT, so it is free and open to everyone.

In the top Hacker News comments, many people are impressed that such a renderer can be made in so few lines of code. Some users like that it gives raylib a way to work on old hardware, virtual machines, or servers without a GPU. Others remember writing similar software renderers in the past and feel nostalgic. Some ask about how fast it runs or if it supports 3D games—most agree it will be slower than a GPU but still useful for learning, testing, or specific cases.

A few people note how useful this is for debugging or for running in cloud environments, where you might not have a graphics card. There is some technical talk about the features supported, such as the depth buffer and blending modes, and how these compare to “real” OpenGL. One person points out that keeping the code simple and readable is a big benefit for those who want to learn how 3D rendering works. Others discuss possible limits, like how it might not be enough for modern games, but could be a good fit for simple visuals, tools, or retro-style games.

Overall, the community response is positive, with people happy to see open-source software rendering kept alive, and some plan to try it out or use it in their own projects.

---

## LLMs can get "brain rot"

- 原文链接: [LLMs can get "brain rot"](https://llm-brain-rot.github.io/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45656223)

This article is about how large language models (LLMs), like ChatGPT, can suffer from “brain rot” if they are trained on lots of low-quality, junk internet content. The authors wanted to see if feeding LLMs a steady diet of popular but shallow and sensational posts from Twitter/X would make them worse at things like reasoning, memory, and ethical behavior.

They ran tests using two types of “junk” data: one based on how popular and short a tweet is, and another based on the actual quality of the writing (like clickbait or fact-based). They took four LLMs and trained them with either junk or control (better quality) data. Then, they checked how good the models were at solving hard puzzles, handling long texts, following safe behavior, and avoiding bad personality traits like narcissism or psychopathy.

The LLMs that saw more junk content got much worse at reasoning and understanding long texts. For example, their score on a tough challenge dropped from about 75 to 57 when only junk data was used. These models also showed more “thought-skipping”—they skipped steps in their answers, which led to more mistakes. The negative effects did not go away, even after trying to fix the models with more training on good data.

The researchers say this shows that the quality of training data really matters. Junk content can cause lasting problems in LLMs, and popular posts are actually a strong sign of “brain rot” risk. They believe that AI teams should regularly check their models’ cognitive health and be careful about which internet data they use in training.

Hacker News commenters had a lot of thoughts. Some agreed and said this matches how people feel after too much social media—they also get “brain rot.” Others thought this result was obvious and that using junk data should obviously harm an AI. A few people worried that since so much of the internet is low-quality, it might be hard to avoid this problem for future AIs. Some were more skeptical, asking if the specific way the junk data was chosen might have affected the results, or if the test tasks really show “cognitive decline.” A few pointed out that popular posts aren’t always junk, so filtering only by engagement could throw out good data too. Others suggested that this finding makes a strong case for better data curation and more careful training methods as LLMs get bigger and more widely used. Someone even joked that maybe LLMs are just becoming more like humans, copying our own habits online. Overall, most agreed that the study is a good reminder that what you feed an AI matters a lot—just like with people.

---

## Neural audio codecs: how to get audio into LLMs

- 原文链接: [Neural audio codecs: how to get audio into LLMs](https://kyutai.org/next/codec-explainer)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45655161)

This article talks about why it is hard for large language models (LLMs) to work with audio, and how neural audio codecs help. Today, most LLMs use text—if you talk to them, your voice is turned into text, and their answers are turned back into speech. But this misses things like emotion, tone, or sarcasm in your voice.

The article explains that modeling audio is much harder than text. Audio has many more data points per second than text, making it slow and hard for models to learn long-term patterns. Early models like WaveNet tried to generate audio one sample at a time, but this was slow and sounded bad. To fix this, researchers use neural audio codecs, which compress audio into tokens. These tokens are easier for LLMs to handle, just like text tokens.

The article walks through how these codecs work. First, an autoencoder compresses audio into a smaller form. Then, vector quantization turns this compressed data into discrete tokens. The article explains a method called residual vector quantization (RVQ), which uses several layers of quantization to keep audio quality high while still compressing well.

After building a simple codec, the author shows experiments where audio is turned into tokens, fed into a language model, and then turned back into audio. More quantization levels mean better audio, but still not perfect. The article then introduces Mimi, a modern neural audio codec, which uses advanced techniques like adversarial loss and RVQ dropout. Mimi can produce much better audio and is used in new models like Moshi and Sesame CSM.

The article also talks about “semantic tokens,” which carry the meaning of speech without details about the speaker’s voice. These help models focus on “what” is said, not just “how” it sounds. Experiments show that models trained with semantic tokens are better at generating real words and making sense, though they still make mistakes.

Finally, the article says that even with the best codecs, audio LLMs are still behind text LLMs in understanding and reasoning. Most current systems depend too much on text, and models still struggle with tasks like understanding voice pitch.

People in the Hacker News comments have many thoughts. Some praise the article for its clear explanations and useful code examples. Others are excited about how neural audio codecs could help LLMs understand emotion and tone—something missing in today’s systems. A few point out that speech is much more complex than text, and argue that models will need even better ways to capture meaning, emotion, and context from audio.

Some readers ask if these advances could help with music, sound effects, or audio search. Others worry about the size and cost of training these models, since audio data is much bigger than text. There are questions about privacy, too—if models can “hear” emotion or other details, could they be misused? A few people say that focusing on semantic tokens is a smart move, since it lets models handle meaning more like text.

A few are skeptical, saying that real understanding of speech, emotion, or sarcasm will need more than just better codecs—it might need new model designs or training methods. Some share their own experiences trying to build audio models and agree that handling long audio sequences is still a big challenge. Others point out that the field is moving fast, and hope that open-source tools and code will help more people experiment.

Many agree that audio LLMs are still far from perfect, but they find the work promising and are interested to see what comes next.

---

## We rewrote OpenFGA in pure Postgres

- 原文链接: [We rewrote OpenFGA in pure Postgres](https://getrover.substack.com/p/how-we-rewrote-openfga-in-pure-postgres)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45661547)

The article talks about how the team rewrote parts of OpenFGA, an authorization tool, to run fully inside Postgres instead of using a separate system. They did this because keeping their main database and OpenFGA’s database in sync was hard and caused problems, especially for a small team.

The writer starts by explaining that authentication (who you are) is easy now, but authorization (what you can do) is still tricky. There are several common models: RBAC (role-based), PBAC (policy-based), ABAC (attribute-based), and ReBAC (relationship-based). Their app, Rover, needed ReBAC because users can be part of many organizations, and permissions depend on those relationships. They chose OpenFGA, which uses “relationship tuples” like {user, relation, object} to manage permissions.

OpenFGA works well, but since it uses a separate Postgres database, it became a headache to keep data synced. Every time someone added or deleted a user, organization, or repo, they had to update both their main database and OpenFGA. Rolling back changes was also hard, and deleting users (for privacy laws) was even messier because they had to manually sync deletions across both systems. The CLI tool for OpenFGA also lacked some helpful features, like easy schema checks and migrations.

The team considered using tools like Debezium to stream database changes to OpenFGA, but this added too much complexity. Instead, they realized they could build the same relationship-based model directly in Postgres using SQL and its procedural features. This way, all authorization checks and data changes happen in one place, using the same tables. They created an “authz_model” table for rules, a view for relationship tuples, and a recursive function to check permissions. This setup lets them use Postgres features like cascading deletes, and it’s much easier to maintain. They also wrote scripts to help manage schema changes.

In the end, this rewrite made their authorization system simpler and better for their needs. If they need to scale up, they can use Postgres replicas. They open-sourced their work on GitHub for others to use.

From the comments, many people agreed that keeping authz and main data in sync is a pain, and liked the idea of just using Postgres. Some pointed out that for small teams or simple needs, a single database is easier to manage and debug. Others worried about performance if the relationship checks get too complex, but the author noted this can be solved by materialized views. A few said that having everything in one database is not always possible for big companies or high-security needs, but for most SaaS apps it makes sense. Some liked that this method uses standard SQL and doesn’t lock you into a special tool. Others warned that as the system grows, you might still need to split services, but starting simple is smart. There were questions about edge cases and how the recursive function works, and the author answered by linking to the code. Overall, most commenters liked the practical, no-nonsense approach, especially for small teams.

---

## Mathematicians have found a hidden 'reset button' for undoing rotation

- 原文链接: [Mathematicians have found a hidden 'reset button' for undoing rotation](https://www.newscientist.com/article/2499647-mathematicians-have-found-a-hidden-reset-button-for-undoing-rotation/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45606853)

This article talks about a new math idea where you can undo a rotation with a special move, like hitting a "reset button." The researchers found that if you rotate an object and then do a hidden trick, you can bring it back to its original state.

Usually, when you spin something, like a ball or a plate, you need to spin it back the other way to undo the move. But in some cases, this new method lets you reset the spin in a clever way, without just turning it backward. The trick uses special math called "quaternions," which help explain rotations in three or four dimensions. Quaternions are often used in computer graphics and robotics to make smooth movements.

The mathematicians found that if you do a certain path—a kind of loop with the object—it can undo the spin. It's like if you twist your arm, then move it over your head in a circle, and your arm ends up untwisted, even though you didn't turn it back directly. This is called the "plate trick," and it's famous in physics for showing how some spins work differently than we expect.

The new discovery is that there's a hidden way to reset the spin, which wasn't known before. This could be useful for robotics, video games, or even quantum computers, where controlling spins is important. The article explains that this reset button works because of deep math rules about how objects can move and return to their starting point.

Some top comments on Hacker News are excited, saying this shows how cool and strange math can be. A few people share stories about learning the plate trick in school or seeing it in physics class. Others talk about how quaternions are helpful for programming 3D games or animations, and how this reset could make those tools better.

One commenter points out that the trick connects to quantum physics, where particles also have strange spin rules. Another person says this research could help make robots move more smoothly or fix problems in computer models. Some users ask for simple examples or animations to help them understand the reset button, while others compare it to magic tricks or puzzles.

A few people say math like this is hard to picture, but it's exciting when it leads to new technology. Some are surprised that such a basic move was hidden for so long. Many agree this discovery is a good reminder that math can still surprise us, even with ideas we thought we understood.

---

## Minds, brains, and programs (1980) [pdf]

- 原文链接: [Minds, brains, and programs (1980) [pdf]](https://home.csulb.edu/~cwallis/382/readings/482/searle.minds.brains.programs.bbs.1980.pdf)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45564821)

This article is about John Searle’s famous 1980 paper “Minds, Brains, and Programs.” Searle wanted to show that computers running programs do not really “understand” in the way people do.

Searle explains the difference between “strong AI” and “weak AI.” Weak AI says computers are just useful tools for studying the mind. Strong AI says that a computer with the right program actually has a mind and can understand things, just like humans do. Searle disagrees with strong AI.

To show his point, Searle uses the “Chinese Room” thought experiment. Imagine a person who does not know Chinese sitting in a room. This person gets Chinese symbols and a rulebook (in English) explaining how to respond with other Chinese symbols. From outside, it looks like the person understands Chinese—he gives correct answers. But inside, he is only following rules and does not understand anything.

Searle argues that computers work the same way—they only process symbols based on rules, but they do not know what the symbols mean. He says that real understanding (intentionality) needs more than just symbol manipulation. The brain’s physical and chemical processes are important for understanding, not just the programs running on it.

Searle answers some common replies. Some people say that the whole “system” (person plus rulebook plus room) understands Chinese, but Searle disagrees: even if the person memorizes everything, he still does not understand. Others say maybe a robot with sensors and a body could understand, but Searle thinks this does not help unless the robot has the same kind of causal powers as the human brain.

Searle also says that simulating a brain’s structure with water pipes or other machines would still not be enough. The important thing is not just the pattern, but what causes real understanding—the brain’s biology.

He ends by saying that only machines with the same causal powers as brains can truly think. Running a computer simulation of a mind is not the same as having a real mind, just like a simulation of a fire does not actually burn.

In the Hacker News comments, people have different opinions. Some agree with Searle and say computers just follow instructions—they do not “know” anything. Others think the line between simulating understanding and real understanding might not be so clear. A few suggest that, with the right sensors and experience, a robot could develop real understanding over time.

Some commenters point out that humans also follow rules when learning, especially in math or language, and maybe understanding emerges from enough rule-following. Others argue that Searle’s idea of “intentionality” is not well defined, and it may not be the only important thing.

A few people find the Chinese Room example convincing and say it shows the limits of today’s AI. Others reply that if a computer or robot behaves exactly like a human, maybe that is all understanding means.

Some mention that Searle’s argument is philosophical and based on intuition, which might not match how science works. Others say we should keep an open mind: perhaps future computers will have new ways to “understand” that we cannot imagine yet.

A commenter points out that brains are also just networks processing signals, and maybe the difference is not as big as Searle thinks. Someone else says that Searle’s view could lead to “biological chauvinism”—the idea that only brains made of cells can think.

Finally, some say this debate shows why AI is still a big mystery, and it’s important to keep discussing what it means to “understand” and “think.”

---

## NASA chief suggests SpaceX may be booted from moon mission

- 原文链接: [NASA chief suggests SpaceX may be booted from moon mission](https://www.cnn.com/2025/10/20/science/nasa-spacex-moon-landing-contract-sean-duffy)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45655188)

NASA might remove SpaceX from the mission to bring astronauts back to the moon by 2027. The NASA chief said SpaceX is behind schedule, and the U.S. is in a race with China to land people on the moon first. NASA had given SpaceX a $2.9 billion contract to build the lunar lander, but delays with their Starship rocket make the timeline risky. SpaceX’s Starship has had several failed flights, and only a few short test flights have worked this year. 

Because of these problems, NASA is thinking of letting other companies compete for the job. Blue Origin, started by Jeff Bezos, is already working on a lander for later missions, but may now be asked to step in sooner. NASA wants both SpaceX and Blue Origin to say how they could speed up their work. NASA is also asking the whole U.S. space industry for new ideas to help get to the moon faster. Some experts worry that both SpaceX and Blue Origin’s landers are very complex, and no one has ever tried the kind of in-space refueling they need. This could make things even slower. Another company, Dynetics, had tried to win a lander contract before, but it is not clear if they will join again.

The main reason for the rush is that China says it will send astronauts to the moon by 2030. U.S. leaders want to make sure the U.S. does it first. Blue Origin says they are ready to help if needed, but NASA might even look for totally new companies. The rules for picking a new company are not clear yet, and it might take time to make this kind of choice.

In the Hacker News comments, some people think NASA is right to keep its options open and not trust just one company. They say competition can lead to better and faster results. Others worry that changing plans now will only add more delays, since every new company must start from scratch or catch up. Some users say that both SpaceX and Blue Origin are facing very hard technical problems, so delays may not be anyone’s fault. A few commenters think NASA is making the moon mission too political and not focusing enough on what is possible with today’s technology. There are also people who support SpaceX, saying their track record with rockets is strong, and that Starship’s test failures are normal in rocket science. Some worry that if NASA splits the job, no one will have enough funding to finish. Others wonder if the rush to beat China is worth the risk to safety. A few think the real problem is NASA’s process, which can be slow and full of paperwork, making it hard for any company to move fast. Some commenters hope more companies join, to bring new ideas and maybe new technology. Finally, some are just excited to see real progress toward going back to the moon, no matter who does it.

---

