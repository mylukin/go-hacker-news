# Hacker News 故事摘要 - 2025-06-26

## 今日概述

Today’s top Hacker News stories cover fast file storage using RAM, DeepMind’s new AI for genetics, an app for language speaking practice, creative text layout scripts, a big DHCP server update, debates on native web templating, robots that learn by watching once, memory speed tests, a new AI browser automation tool, and an update to the Matrix messaging protocol. Themes include new AI tools, creative software, better developer tools, and open-source updates. If you like AI, programming, or system tools, today’s stories have something interesting for you.

---

## Save your disk, write files directly into RAM with /dev/shm

- 原文链接: [Save your disk, write files directly into RAM with /dev/shm](https://hiandrewquinn.github.io/til-site/posts/save-your-disk-write-files-directly-into-ram-with-dev-shm/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44392304)

The article explains how you can use `/dev/shm` to write files straight into your computer’s RAM instead of your disk. This can help save the life of your hard drives or SD cards, especially if you are worried about too many writes. `/dev/shm` is a special folder found on most Unix systems, and anything you save there stays only in memory and is lost when you reboot. The author shows how using `/dev/shm` makes working with files much faster, because RAM is much quicker than disks. For example, if you save small files—like text files from Wiktionary—they open and edit almost instantly. Even bigger files, up to a few gigabytes, can be handled easily if you have enough RAM. The article gives a simple command-line example for downloading and editing word data, and says that thousands of these files can fit in just 1 GB of RAM. The main warning is that files in `/dev/shm` disappear when you turn off or restart your computer, so only use it for temporary work. The author also notes that tools like `grep` work even faster when files are in RAM.

In the comments, some users think this is not a big improvement, especially for small files. They say using `/tmp` is enough because Linux already keeps recently used files in RAM for speed. Others point out that you can mount `/tmp` as a RAM disk, so all temporary files go into memory anyway, and some Linux systems already do this. One person warns that copying large files into `/dev/shm` can fill up your memory and cause problems for other programs. Another user says using `/dev/shm` might not always be supported, so it’s safer to stick to normal ways like `/tmp`. The author replies that `/dev/shm` is usually world-writable and easy to use, so he prefers it. Some people share stories about using RAM disks in the past, like on old computers, and one person likes using Jupyter notebooks to keep data in memory during experiments. Another comment simply says that `/tmp` is meant for this purpose and there’s no need to get fancy. Overall, most agree that writing files into RAM is fast, but you need to understand the risks and trade-offs.

---

## AlphaGenome: AI for better understanding the genome

- 原文链接: [AlphaGenome: AI for better understanding the genome](https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44387659)

AlphaGenome is a new AI tool from DeepMind that helps scientists understand how changes in DNA affect gene regulation and disease. The model can look at very long DNA sequences—up to one million letters—and predict thousands of features, like where genes start and end, how genes are spliced, and how much RNA is made in different cell types.

AlphaGenome uses advanced neural network layers, like convolutional layers and transformers, to find patterns in DNA and make precise predictions. It can quickly compare normal DNA with mutated DNA to see how a single change might impact gene activity. The model is trained on huge datasets from projects like ENCODE and GTEx, which include data from many types of cells and tissues.

Compared to older models, AlphaGenome can handle longer DNA sequences at high detail, predict more types of gene regulation, and score genetic variants faster. It is also the first model to predict RNA splicing junctions directly from DNA, which helps study diseases caused by splicing errors.

AlphaGenome outperforms other models in most tests, both in predicting DNA features and in scoring the effects of genetic changes. It can help researchers study disease mechanisms, design synthetic DNA, and map important parts of the genome. For example, it has already helped explain how a mutation can activate a cancer gene in leukemia.

Still, AlphaGenome has limits. It struggles with very distant DNA interactions and cannot be used for personal health predictions yet. It does not fully explain how DNA changes lead to complex diseases, because many factors outside DNA are involved.

In the Hacker News comments, some people are excited about AlphaGenome and see it as good progress, while others think the field needs even bigger breakthroughs, like full cell simulations. Some say AlphaGenome is not fully new, since similar work has been happening for years, but they agree it is a strong demonstration.

A few users discuss the challenge of moving from prediction to understanding which DNA changes actually cause disease (causality). They mention that tools like AlphaGenome help, but more steps are needed, such as combining AI predictions with statistical methods.

People also talk about DeepMind’s strong track record and the benefits of having lots of resources from Google. Some think DeepMind’s success is not just about money, but also about talent and focus. There is debate about whether black-box AI models are enough, or if science needs more transparent, physical models.

Overall, the community welcomes AlphaGenome as an important tool, but they hope for even more transparent and powerful AI in biology. Some wish it tackled more problems, like identifying which mutations actually matter for disease. Others point out that as genome sequencing gets cheaper, tools like AlphaGenome will become even more useful.

---

## Launch HN: Issen (YC F24) – Personal AI language tutor

- 原文链接: [Launch HN: Issen (YC F24) – Personal AI language tutor](item?id=44387828)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44387828)

Issen is a new AI-powered app that helps people practice speaking foreign languages using voice conversations. The creators built it because they wanted a tool that feels like talking to a real tutor, but is easier to use and less expensive than hiring a human teacher.

The app uses advanced AI tech to turn speech into text, understand it, and talk back in the target language. It supports many languages and lets you customize things like lesson speed, conversation topics, and formality. Issen focuses on real conversations, not just games or streaks, to help you learn faster. It remembers new words you learn and gives you practice with those words using flashcards. The team says it works best for people who already know some of the language, not total beginners. They also mention that making speech recognition work well with different accents and noisy places was hard. For quality, they combine several top AI models and use different speech voices, depending on the language.

Users can try Issen for free for 20 minutes, then pay a monthly fee. The app is available on web, iOS, and Android. The team is looking for feedback and is aware that some languages are better supported than others.

In the comments, many people like the idea of practicing real conversations instead of just playing language games. Some users say that the app feels more helpful than ChatGPT or Gemini for voice learning, but beginners still get lost because the AI doesn't guide them enough or simplify things when needed. There are reports that the app sometimes gives too long or hard sentences, ignores users' struggles, or doesn't correct their pronunciation well. Advanced users wish for more interesting topics, like news or debates, instead of simple questions. Some people compare Issen to other tools like Duolingo, Memrise, or LingQ, saying those apps are either too focused on streaks or not personal enough.

A few users point out problems with pronunciation, errors, and app bugs—like being called the wrong name or getting stuck during signup. Others suggest adding features like better error corrections, visuals, or integration with tools like Anki. Some think the app should focus on fewer languages to improve quality, while others believe supporting rare languages is valuable. People discuss if AI can really replace human tutors, especially for beginners, and if apps like this will disrupt older platforms. Overall, many agree Issen is a promising step for language learners, but needs better support for beginners, improved corrections, and more engaging content.

---

## Alternative Layout System

- 原文链接: [Alternative Layout System](https://alternativelayoutsystem.com/scripts/#same-sizer)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44390501)

The article talks about a set of scripts that change how text looks and fits in a block, inspired by old manuscripts and special design needs. These scripts are for people working with layouts in tools like InDesign.

One script, “Same Sizer,” makes every word take up the same space, no matter how many letters it has. This is like using a monospaced font, but for whole words instead of letters. Another script, “Wiggle Out,” helps when a word is too long to fit; it moves the word into the margin, curving it if needed. “Fill the Space” fills empty space at the end of a line with marks or repeated letters, copying styles from old texts. “Hyphen Out” removes hyphens by putting the second part of the word outside the text frame, which can be resized or moved. “Hyphenator” shrinks the last part of a long word so it fits without a hyphen, keeping lines tidy. “Last is First” shows a preview of the first word on the next line, a trick from Hebrew manuscripts. “Ext. Word & Letter” stretches out the last letter or word to fill the line, and can vectorize it to fit perfectly. “Variable Gradient” lets you fade between two colors or styles across a block of text.

In the comments, one person wants to change the “Hyphenator” to fit more words by shrinking text, like students do when squeezing notes onto a line. Another thinks “Same Sizer” looks odd because the stretching feels forced and lines aren’t even; they suggest looking at Vietnamese calligraphy for a better style. Someone asks why Hebrew manuscripts use these methods so much. A few people say these layout ideas are very creative and could look good in titles or body text if the right symbols are used. One user says they dislike the style, joking that it slows down reading, like the “Dotsies” font that uses dots instead of letters. Overall, commenters are split—some love the creativity for design, others find the styles hard to read or strange. There is also interest in the manuscript history and how these ideas connect to old writing systems.

---

## Kea 3.0, our first LTS version

- 原文链接: [Kea 3.0, our first LTS version](https://www.isc.org/blogs/kea-3-0/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44390962)

Kea 3.0 is a new version of a DHCP server, and it is the first one to get long-term support (LTS) for three years. This means users do not need to upgrade as often, and they get more stable updates and help from the developers. The update brings many changes: most of the Kea “hook” plugins are now open source and free, except for two paid ones. The way you install Kea has changed, and users need to read the notes before upgrading because some settings and file locations are different. Security is better now, but you must update passwords and protect remote management more carefully. Kea no longer needs a Control Agent to handle remote access; instead, it uses APIs over HTTP and HTTPS, making things simpler. Some tools, like the Role-Based Access Control hook, need new settings. The way Kea groups and manages clients is more flexible, which helps people moving from older ISC DHCP servers. The build system now uses Meson instead of autotools, so building Kea is faster and easier. Database tools for MySQL and PostgreSQL are now separate, so you only install what you need. There are new guides and manuals to help people upgrade, install, or use the new features.

In the comments, people explain that Kea is replacing the old ISC DHCP server, which has reached end-of-life and will not get updates anymore. Some users talk about how different firewall systems, like pfSense and OPNsense, are handling the switch to Kea. A few say the move has been messy with bugs, but recent updates have fixed many problems. Others mention that OPNsense updates faster and is now using Dnsmasq as the default DHCP server for new installs, although Kea is available too. One user shares a story about switching from Dnsmasq to Kea for a big project with 20,000 devices; Kea worked better under heavy network traffic. Some people did not know what Kea was and had to look it up, while others explain that it is a modern DHCP server with strong IPv6 support. There are also mentions of Kea’s history and how it started as part of another project. A few users remember older debates about other network software, like BIND and djbdns, and talk about how every popular tool has critics. Overall, the comments show some confusion, some praise for Kea’s performance, and some worries about the switch, but most agree this is a big and needed update for DHCP users.

---

## The time is right for a DOM templating API

- 原文链接: [The time is right for a DOM templating API](https://justinfagnani.com/2025/06/26/the-time-is-right-for-a-dom-templating-api/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44390452)

The article says the web is missing a built-in way to turn data into dynamic HTML safely and easily, like all popular frameworks do. It explains how a native DOM templating API could make web apps faster, safer, and easier to build, by letting developers update the page with new data without writing lots of code or depending on big libraries. Templating lets you mix HTML and data in a clear way, helps avoid security problems like XSS, and can be fast and easy for both users and developers.

The author points out that today, every major web framework—React, Vue, Svelte, and others—has its own way to do templates, but the web platform itself offers nothing standard. This means every project needs to download extra code, which can slow down websites and make them less secure. Developers have to learn different template systems for different frameworks, and building new frameworks is harder because they must create these features themselves.

The article says the time is right to fix this, because the community now agrees on what good templating looks like, mostly using JavaScript template literals or JSX-like syntax. It suggests that a JavaScript-based API would be easier to add than a pure HTML one, and could later help build more complex features. The author also talks about different ways frameworks handle updates—like React’s virtual DOM, Lit’s “template identity,” and new ideas like signals—and thinks a mix of these ideas would work best for a native API.

The article shares that earlier attempts at adding templating to the web failed, but now, lessons from frameworks show a clear path forward. A new API could help not just “vanilla” JavaScript developers, but also framework authors and users. It would make apps load faster, make code safer, and let more people build interactive sites easily.

In the comments, some developers agree that native templating is needed and would save lots of CPU and bandwidth now wasted downloading big libraries like React. Others remember older technologies like XSLT and XUL, saying they had strong templating and transformation features, but were too hard for most people to learn or use.

Many commenters debate what kind of API would help the most. Some say a built-in virtual DOM and patch function would be better than just templating, letting the browser handle updates more like React does. Others wish for a more visual or designer-friendly system, like Dreamweaver or Photoshop, instead of code-focused templates. A few people want the syntax to be more flexible, maybe using ideas from other languages like Kotlin or Jetpack Compose, not just JSX.

Some worry that any new standard will quickly become outdated, as needs always change and developers find ways around old APIs. A commenter notes that browsers added strict security rules (CSP) before making templates safer, which seems backwards. Others mention that features like <template> and <slot> in today’s web are not reactive and can’t match what frameworks do, so reactivity is still a hard problem.

Several people discuss React’s approach versus what the article suggests, noting differences in event handling and updates. There’s also talk about how Web Components are still not as easy or popular as frameworks like React, but they could benefit from better native templating. Finally, some dream of a future where HTML, CSS, and JavaScript are unified into one simple syntax, making development much smoother for everyone.

---

## Robots that learn

- 原文链接: [Robots that learn](https://openai.com/index/robots-that-learn/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44390915)

This article talks about robots that can learn a new task just by watching it once. OpenAI has made a robot system that learns in simulation, then works in the real world.

The robot’s vision is trained only with fake (simulated) images, using many different colors, backgrounds, and lighting, and it never sees real images during training. The main new idea is “one-shot imitation learning,” which means a person does the task one time in virtual reality, and the robot can copy it from any starting point. Their system uses two neural networks: one for vision (to see where things are) and one for imitation (to copy the task). They train these networks with many examples of tasks, showing different ways to do the same thing. For example, they use block stacking: the robot learns to make towers of blocks in the same order as shown, even if the starting setup is different. The imitation network uses a method called “soft attention” so it can handle different numbers of blocks or demonstration steps. They also add noise to the training data, so the robot learns how to fix mistakes. This helps the robot finish the task even when something goes wrong.

The comments section brings up many different ideas. Some users notice this work is from 2017 and wish OpenAI had more recent robotics news. Others say they were excited at first, thinking this was a new breakthrough, but then felt disappointed it was old. A few people joke about robots folding laundry and how useful that would be, while others point out that real household robots are still far away. Some commenters discuss how most tech startups ignore everyday chores and focus on other things, while real automation for chores is still missing. There’s also a comment about how hard tasks like folding laundry are for robots, and that we are still waiting for a big moment in robotics, like what we saw with large language models. A few users talk about the robot’s old hardware, and others wonder if we really need general intelligent robots now or if these are just proof-of-concept ideas. One person brings up concerns about robots being used by the military. Finally, some users share links to more recent robotics research, showing that the field keeps growing.

---

## How much slower is random access, really?

- 原文链接: [How much slower is random access, really?](https://samestep.com/blog/random-access/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44356385)

This article is about how much slower random access is compared to first-to-last (sequential) access when reading large arrays in memory. The author wanted to answer questions like how big an array gets before you notice a slowdown, how much time each method takes, and how things change when arrays are bigger than RAM.

To test this, the author wrote code in Rust to sum an array of floating-point numbers using two different index orders: first-to-last and random. For small arrays, both methods are fast and similar in speed because everything fits in the fast cache (L3 or smaller). As arrays get bigger and can't fit in the cache, random access becomes much slower—about 4 times slower on a MacBook, and up to 8–16 times slower on a Linux desktop. When arrays are too big for RAM, both methods slow down a lot, but random access gets much worse, especially on Linux. The author also found that the Fisher-Yates shuffle is too slow for huge arrays, so they used a two-pass shuffle.

The author tried using memory-mapped files to see if that would help, but the results were mostly the same, except on macOS, where things looked a bit different. As a final check, they summed the numbers by reading chunks directly from disk, which showed that memory-mapping wasn’t always smart about performance. The author shares code, graphs, and detailed results for both MacBook and Linux machines, and points out that the main bottleneck is memory bandwidth, not the CPU speed.

In the comments, one reader notes that this is not true random access, since the index array is still linear and can be prefetched by the CPU, making things faster than in real random access cases (like when each value points to the next index). Another commenter says this is why arrays and linked lists perform differently—linked lists can’t be prefetched and are even slower. Someone else mentions the GUPS benchmark from high-performance computing, which measures random memory access and was important for certain supercomputers. One person was surprised that random access was only 4 times slower, not much worse, and thought the analysis was great. Another commenter shared their own blog post on memory bandwidth but disagreed with using "time per element" as the main metric, saying it might not be the best way to show the results.

Overall, the post gives real numbers for random vs. sequential access and shows that, for big data, memory access patterns and hardware details can matter a lot. Commenters add context about how this fits into computer science and high-performance computing, and some suggest different ways to look at or measure the problem.

---

## Show HN: Magnitude – Open-source AI browser automation framework

- 原文链接: [Show HN: Magnitude – Open-source AI browser automation framework](https://github.com/magnitudedev/magnitude)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44390005)

Magnitude is a new open-source framework that helps automate browsers using AI. It uses a “vision-first” method, so it works more like a human, by looking at the page instead of just reading the code behind it.

The framework can handle complex browser tasks, such as drag-and-drop, working with charts, or dealing with websites that use lots of graphics or strange layouts. It is not limited to testing websites; you can use it to collect data, link apps that don’t have APIs, or build your own browser agents. Magnitude uses smart models to decide what to do and then acts by clicking or moving the mouse to the right spot, just like a person would. The makers suggest using a model called Claude Sonnet 4 for best results, but you can also use an open-source model called Qwen, which is cheaper but sometimes less reliable. 

One special thing about Magnitude is that you get a lot of control. You can tell it to do high-level tasks, like “create an issue,” or very specific ones, like “drag this item to another place.” You can also mix its actions with your own code and control how it understands each step. There’s also an easy setup: just run a simple command to get started.

In the comments, some people say the setup is very easy and it works well for simple tasks, but they worry about the cost and reliability for bigger jobs. One person points out that, since every step can fail, long tasks might not finish often, and it could get expensive if you run many jobs. Others ask if it’s really better than older tools like Playwright, because using AI models makes it slower and pricier. The makers answer that it can save developers’ time, especially for messy websites where old tools break or are hard to update, and you can use cheaper AI models if needed. Another user suggests just using AI to write scripts, but the Magnitude team explains that scripts can break when websites change, so their tool tries to be smarter and adjust itself. Someone else asks why not use the AI model by itself, but the authors say their system gives more control, is faster, and works better with browsers. There are also a few jokes and short comments about the tool handling graphics-heavy sites, and a small side discussion about the tone of some comments.

---

## Matrix v1.15

- 原文链接: [Matrix v1.15](https://matrix.org/blog/2025/06/26/matrix-v1.15-release/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44390740)

Matrix v1.15 is out now, bringing new features to the Matrix messaging protocol, especially in how users log in, see room info, and use rich text in room topics. The update adds next-generation authentication with support for OpenID Connect (OIDC), makes it easier for users to get detailed summaries of rooms before joining, and lets rooms have topics with bold text and lists.

The new authentication system is a big step toward industry standards, making Matrix safer and closer to a “Matrix 2.0” release. This change helps with large migrations, like moving over 100 million users in half an hour, and gets Matrix ready for more secure and easy sign-ins. The update also introduces a room summary API, so clients can show more details about rooms that users haven’t joined yet—helpful for invites and browsing. Rich topics let room creators use bold, lists, and links in their room descriptions, making them easier to read and more useful.

Other technical changes include new endpoints for getting room summaries and authentication metadata, adding device keys to encrypted events for better security, and more options to describe room properties like allowed room IDs, encryption, and room version. The update also fixes many small issues, clarifies how “public” rooms are defined, and updates documentation examples and server behavior details.

On the server side, similar changes allow for richer room topics and new properties in federated room hierarchies. Application services and identity services get minor clarifications. There are also internal changes to how the specification is built and displayed, but no big changes to room versions or push notifications. The Matrix Foundation reminds users it relies on donations to keep running and to support digital privacy.

In the Hacker News comments, many users wish Matrix would add features like Discord’s detailed permissions and voice channels. Some are disappointed these aren’t here yet, blaming funding and focus on enterprise needs. Matrix developers reply that the current focus is on serving organizations, not competing directly with Discord, but say Matrix does allow fine-grained permissions using “power levels,” though it isn’t a full role-based system. Some users find this approach confusing and want simple role-based permissions like Discord, saying Matrix’s system is hard to manage and doesn’t match how people think about roles.

Others argue that people are just used to Discord’s way and switching is hard, but agree that role-based permissions are easier to use. Some like that Matrix lets you run your own server and care about privacy, but note that most people just want something that works simply, like Discord or WhatsApp. A few mention that Matrix’s official client (Element) used to be slow and clunky, though the new Element X is faster, but missing some features. Problems with bridging Matrix to IRC, resource-heavy servers, and confusing encryption key management are also raised as barriers.

Some users say Matrix is mostly used by people who really care about privacy or have privacy-focused friends, while others stick with popular apps for convenience and features. There’s talk that Matrix’s focus on government and enterprise customers has left regular users behind, and that the project’s future depends on getting enough funding to support both. Still, some are hopeful that as Matrix grows and gets more stable, it will become a better choice for everyone.

---

