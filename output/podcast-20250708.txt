Hello everyone, this is the 2025-07-08 episode of Hacker News Daily Podcast. Today, we have eight stories for you, covering security, AI, science, music, and more.

Our first story looks at a new security problem with Supabase’s Model Context Protocol, or MCP, when used with large language models. The issue is that the AI assistant, which helps with support tickets, cannot tell the difference between normal text and commands hidden inside user messages. Attackers can trick the assistant into running secret SQL commands and leaking private database data. This is possible because the AI runs with a powerful “service_role” that can access all the data, even things normal users cannot see. The article suggests two ways to reduce the risk: use a read-only mode for the AI, and add filters to block suspicious commands. But the writer warns this is a deep problem that needs more care.

Hacker News readers were shocked by the basic security mistake of letting an AI have full database access. Many said this is a known risk with LLMs and prompt injection, and tools should not treat user data as instructions. Some suggested using strict roles and more context separation. Others shared stories about similar risks and agreed that mixing LLMs with critical systems like databases needs better safety tools and design.

Next, we have a security bug in Git, known as CVE-2025-48384. This bug lets attackers run code on your computer if you use “git clone --recursive” on a bad repository. The root of the problem is how Git handles old control characters, like carriage return, in config files. On Unix systems, filenames can have these special characters, and a bad repo can set up submodules with tricky paths. This can confuse Git and let attackers put files in the wrong place, leading to code execution. Windows is safe, but macOS and Linux are at risk, especially with tools like GitHub Desktop, which use “--recursive” by default. The fix is to always put values with carriage returns in quotes. There is already a patch out.

In the comments, people were surprised that such an old character from typewriters can still cause trouble. Some blame the way Git tries to support many old formats. Others say quoting values or using stricter formats would help. There’s debate about the risks of “--recursive,” and some thank the bug finder for keeping software like Git under review.

Our third story is about ProjectionLab, a personal finance tool that went from a side project to a business making over one million dollars a year, all without outside funding. The creator started in 2021, working at night and on weekends. Growth was slow, with many doubts and setbacks, but he learned that just showing up every day was key. After two years, he found a marketing partner and started hiring helpers from the user community. Now, ProjectionLab has a strong Discord group and keeps growing by focusing on quality, not fast trends.

Many Hacker News readers praised the steady growth and the honest story. Some said it matched their own slow journeys. There were questions about the tech and support, and some discussed the risks of not taking funding. Many found the story hopeful, showing that patience and daily effort can pay off.

The fourth story is about Radium, a music editor and DAW with a unique interface that mixes ideas from piano rolls and trackers. Radium started in 1999 and works on Linux, Windows, and Mac. It is open source, has many features like granular synthesis, scripting in Python or Scheme, and supports plugins like VST and LADSPA. You can zoom, undo, work with MIDI, and more.

Comments on Hacker News were mixed. Many liked the creative interface and open source nature. Some found the look confusing at first, and others said it is powerful but has a steep learning curve. People liked the scripting support and regular updates, but some wanted more tutorials. Overall, Radium is respected for trying new ideas in music editing.

Our fifth story looks at new research about Theia, the giant object that hit Earth to form the Moon. Using computer models, scientists tested how Theia could have formed and moved. Their model matches what we see today—Earth and Mars’ orbits, the mix of materials on each planet, and the timing of the Moon’s creation. The results suggest there was a 50-50 chance Theia was carbon-rich, which fits with older chemical studies.

Readers were impressed by the detail in the models. Some asked how this changes our ideas about planet formation and the chances for life. There was talk about how this explains the differences between Earth and Mars, and how new Moon rock data might help. Many liked seeing computer models and chemical studies agree, and enjoyed learning about the chaotic, random history of planet building.

Story six compares two AI models for handling sequences—State Space Models, or SSMs, and Transformers. SSMs use a hidden state to remember the past, like a brain, while Transformers store everything in a big cache, like a database. SSMs are now close to Transformers in language tasks, and do better with raw or noisy data. Transformers are still best when you need exact recall. The article also says mixing both types can give better results, and big labs are working on hybrid models.

In the comments, readers discuss the trade-offs and future of both models. Some like SSMs for long, fuzzy data, while others say Transformers are still king for precise work. There is debate about tokenization and the value of hybrid approaches. Most agree both models have a place, and more research is needed.

Our seventh story is about how plants keep their protective barrier strong by sensing how gases move in and out. Scientists studied the plant Arabidopsis and found that when the outer layer is hurt, gases like ethylene and oxygen move in and out, acting as signals to start repair. If you block the gas movement, the barrier heals less well. The process is different in other plant parts, but the main idea is the same: plants use gas diffusion to sense and fix damage.

Hacker News readers were impressed by the simple way plants use gas as a signal. Some saw uses for farming or making better plant-based materials. Others compared plant and animal healing. There was interest in using this idea for new sensors and in learning more about plant self-repair.

The eighth story is a practical trick—using animated SVGs in your GitHub README files instead of GIFs. By recording a terminal session with asciinema and turning it into an SVG animation with svg-term-cli, you get smaller, sharper images that work directly on GitHub. SVG animations use built-in elements like “animate” and “animateTransform.” Most modern browsers support them, and they are good for code demos.

Comments were positive, with people excited to try it and happy about the better quality and speed. Some worried about browser support, but others confirmed it works well. There were tips on customizing SVGs, and questions about security, but most agreed it is safe on GitHub.

Finally, we have SmolLM3, a new, small language model with 3 billion parameters. SmolLM3 is open source, supports six languages, reasons well, and handles very long text—up to 128,000 tokens. The team used special training tricks like grouped query attention and intra-document masking, plus a new method called Anchored Preference Optimization. SmolLM3 has two modes: quick answers and detailed reasoning. It supports tool use, and is easy to run on normal hardware.

Tests show SmolLM3 beats other models its size, and even matches some bigger ones, in tasks like knowledge, reasoning, math, and code. Comments on Hacker News are mostly positive. People like the open data and clear instructions, and many are happy to have a powerful, open model they can run themselves. Some want more real-world tests, and others compare it to models like Llama and Qwen.

That’s all for today’s episode. Thank you for listening to Hacker News Daily Podcast. See you next time.