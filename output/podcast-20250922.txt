Hello everyone, this is the 2025-09-22 episode of Hacker News Daily Podcast. Today, we have news and stories from the world of AI, programming, tech hardware, and even a surprising take on laptop bags. Let’s get started.

First up, Alibaba Cloud has released Qwen3-Omni, a new open source AI model that can handle text, images, audio, and video all at the same time. Qwen3-Omni can reply in real time, using both text and speech, and it supports 119 languages for text input, 19 for speech, and can answer in 10 languages with different voice types. The model uses a special MoE design with two main parts: a “thinker” for understanding and a “talker” for speaking. You can give it many types of input at once, like showing a picture, playing a sound, and asking a question in text, and it will figure out the answer.

Qwen3-Omni is open source and comes with clear guides, or “cookbooks”, for tasks such as speech recognition, image analysis, video description, and more. You can run the model using tools like Hugging Face Transformers, vLLM, and Alibaba’s DashScope API. There are web demos and Docker images, but keep in mind that the full model needs strong hardware—processing long videos can use over 100 GB of GPU memory.

In benchmarks, Qwen3-Omni gets top or near-top scores in many areas, sometimes even beating models like GPT-4o and Gemini 2.5 Pro, especially for audio and video tasks. It also has a “Captioner” version for detailed audio captions.

Hacker News users are excited about the broad abilities of Qwen3-Omni and happy to see open models catching up to or beating big tech’s closed ones. People like the clear guides and Docker support, but some worry about the high hardware needs—most users can’t run the biggest versions at home. Developers praise the strong multi-language support and ask about privacy, since running models locally helps keep data safe, but needs big GPUs. There are tips for speeding up the model and saving memory, and some compare Qwen3-Omni to other models like LLaVA, noting that strong multimodal support is becoming the new AI standard.

Next, let’s visit the world of interactive stories. There’s a look back at the “Choose Your Own Adventure” book series, which became popular in the late 1970s and 1980s. These books let readers make choices that changed the story, which was new and exciting for many kids. The first book, “The Cave of Time,” allowed children to pick what happened next, making them feel part of the adventure. Edward Packard started the idea by making up stories with his daughters, and after some trouble finding a publisher, the books took off. They sold millions, especially to boys, and inspired later series and even computer games.

Sales dropped in the late 1980s as computers offered more advanced interactive stories, but the books are still remembered fondly. Some digital successors, like “Choice of Games,” keep the spirit alive today.

Hacker News users share their childhood memories, saying these books made them love reading and even led some to video games or programming. Some note the format’s limits, and a few preferred gamebooks with more rules. There’s debate about sales numbers and how the books handled violence and choices—many remember the funny or silly endings. Others talk about the legal battles over the brand and the shift to digital interactive fiction. Overall, there’s strong nostalgia and respect for how “Choose Your Own Adventure” inspired both readers and game makers.

Now, onto web development. Cap’n Web is a new RPC library from Cloudflare, written in TypeScript and inspired by Cap’n Proto, but made for the web. It uses object-capability security and lets you call functions both ways—from client to server and back. You can pass objects and functions as references, and the protocol is based on JSON, making it easy to debug. Cap’n Web works over HTTP, WebSocket, and postMessage, and supports browsers, Node.js, and Cloudflare Workers. It’s open source, under 10 kB, and supports promise pipelining, so you can chain calls without waiting for each one. TypeScript support is strong, with type checking for interfaces.

Users like the easy setup and clear TypeScript support. Some worry that it’s hard to connect with non-JS systems since there are no schemas. Security is also a concern if type checks only happen at compile time. Compared to GraphQL and gRPC, people wonder if Cap’n Web will scale and if the tooling is ready for big projects. Some like the open source license and small size, but want to see performance numbers and more real-world examples. There’s also debate about RPC versus REST, but many developers are eager to try Cap’n Web, even if it is still early days.

Let’s talk about offline apps. Even though “local-first” or “offline-first” apps sound great—they should be fast, private, and work without the internet—most apps don’t offer strong offline support. The main problem is syncing data between devices. If you let users change data on different devices, sometimes offline, syncing and handling conflicts is hard.

The article explains Hybrid Logical Clocks, or HLCs, which help devices agree on the order of events without a central server. But ordering is not enough—sometimes two devices change the same data, so you need CRDTs, or Conflict-Free Replicated Data Types, to make sure all devices end up with the same result. A simple method is “last-write-wins,” keeping the newest change. The author’s team built a SQLite extension to store all changes with timestamps, making syncing easy and reliable.

In the comments, users agree syncing is hard, and that’s why most apps skip offline support. Some say users don’t demand offline features, so developers don’t build them. Others share their struggles with testing and debugging sync issues. There are questions about complex data and business rules, and some worry about trusting apps to keep data safe offline. Still, people hope new open source tools will make local-first apps easier to build.

Now, some big AI infrastructure news. OpenAI and Nvidia are working together to build massive new AI data centers, planning at least 10 gigawatts of computing power, possibly worth up to $100 billion. The first systems will use Nvidia’s new Vera Rubin platform and should be ready in 2026. OpenAI will use Nvidia as its main hardware partner, and both will design hardware and software together. The goal is to train and run the next generation of AI models, aiming for “superintelligence.”

People are amazed by the scale—10 gigawatts is more power than some countries use. There are worries about energy use and the impact on the environment. Some say this shows how dominant Nvidia is in AI hardware. Others worry that such big investments will make it hard for smaller companies to compete. There are also questions about whether this much compute is really needed, and concerns about too much power in the hands of a few companies. Some are excited about the tech, but others want to watch for negative effects, like high energy use or less competition.

Switching to AI safety, there’s a new tool called httpjail, designed to control and filter HTTP(S) requests from coding agents, like Claude Code. Coding agents are getting stronger, but security tools have not kept up. Httpjail blocks all network traffic except what you allow, using rules you write in JavaScript or shell scripts. It works on macOS, using HTTP_PROXY for weak control, and does TLS interception for HTTPS. Stronger protection uses Docker containers with network limits.

Some users like the idea and think httpjail is a helpful step for safer AI agents. Others say the current weak mode is not very secure, and more OS-level controls are needed. TLS interception can also break some features and cause trust issues. Keeping network rules up to date is also a challenge. Some ask for better support on Windows and more real-world examples. While the tool may be complex for some, many agree the security problems it addresses are real.

Next, let’s look at AI model training. The article compares autoregressive (AR) models, like GPT, and diffusion models, which are popular for images. The main point is that, as we run out of new data but have more compute, we need models that can do more with less data. Diffusion models learn by hiding parts of the data and trying to guess them, which is like data augmentation. In tests, diffusion models do better than AR models when data is limited and training goes for many rounds. AR models overfit quickly, but diffusion models keep improving.

Comments say this makes sense since data augmentation helps with small datasets. Some wonder if the results will hold for bigger models or real-world tasks, and if diffusion’s higher compute cost is worth it. There are questions about hybrid models, and some people are excited about using diffusion in fields like robotics or healthcare. Others want to see more tests outside of language, and some note that practical choices still depend on the task and available resources.

SWE-Bench Pro is our next topic. It’s a test to see if AI tools can fix real software problems in big codebases, not just simple bugs. The AI gets a codebase and an issue, and must make a patch that solves the problem, sometimes needing to change code in many places. The dataset is open, and you can use Docker and Modal to run tests. There are public and private leaderboards for teams to compare results.

People are excited that AI is being tested on real software engineering problems. Some say this is an important step, since real bug fixes need understanding many files and project context. Others point out that AI still struggles with very complex issues, and human review is still needed. There’s debate about whether these tests show enough to trust AI with important code, and some wonder what will happen if AI tools start submitting patches to real open-source projects.

Switching to hardware, there’s a comparison between an Apple MacBook with Apple Silicon and a Framework 13 laptop with an AMD processor. The author finds that the MacBook holds its battery much better during sleep—after three weeks, it still had 90% left, while the Framework loses 3-4% per hour of sleep. The author tried different Linux systems on the Framework, but the battery problem stayed. He likes Framework’s repairability and upgrade options, but feels sad that outside of Apple, modern laptops still have poor battery life. Switching to ARM might help, but it’s not easy today.

Many users agree that Apple Silicon laptops have amazing battery life, especially in sleep mode. Some say Linux laptops often have sleep and battery drain problems, and this is not just a Framework issue. Others explain that Apple controls both hardware and software, allowing better power management. Some defend Framework for its repairability, even if battery life is not perfect. There are tips for saving battery on Linux, and some hope future Framework models with ARM will solve this problem.

Finally, a fun story about laptop bags. The author tells why he uses a movie prop grocery bag as his laptop bag. After showing up at a Cloudflare meeting with his laptop in a real paper grocery bag, he found a fake grocery bag for movies, which looks like paper but is made from strong fabric. He likes that it doesn’t look like a laptop bag, drawing less attention. Some readers agree, saying plain bags can help avoid theft. Others share their own tricks, like using diaper bags or lunch boxes. There are risks—plain bags might get thrown away, or look suspicious at airports. Some prefer strong, padded bags for protection. Others joke about the idea, calling it “peak nerd,” and enjoy the creativity.

That’s all for today. We covered new AI models, coding tools, hardware, and even creative laptop bags. Thanks for listening to Hacker News Daily Podcast. See you next time.