Hello everyone, this is the 2025-12-20 episode of Hacker News Daily Podcast. Today, we bring you the biggest stories and discussions from the tech world, chosen for software developers and tech enthusiasts. Let’s get started.

First, let’s talk about spreadsheets as an esport. Diarmuid Early from Ireland just won the 2025 Microsoft Excel World Championship in Las Vegas, beating 256 top players from around the world. The event looked like a big sports final, with lights, music, cheering fans, and live commentators—but all the action happened inside Excel. Diarmuid is now called the “LeBron James of Excel” thanks to his skills. He took the title from Andrew Ngai, the three-time champion from Australia. The competition offered a $60,000 prize pool, and Diarmuid won $5,000 and a championship belt. The finals are tough: every five minutes, the slowest player is kicked out, so you must be both fast and smart. The challenges are not only about math or finance; some are puzzles, like finding your way through a maze or sorting playing cards. Diarmuid says you need to think quickly and use Excel in creative ways. He shares his knowledge on YouTube, though he’s not a fan of TikTok or Instagram. Now, he runs his own company in New York, and many clients come to him because of his Excel fame.

The Excel esports scene is growing fast, with a big online community and more people joining every year. Diarmuid enjoys the fun side and says he doesn’t take himself too seriously, even with the NBA star comparisons.

On Hacker News, many readers were surprised that Excel competitions are so popular and exciting. Some joked about watching a “sport” based on office software, but most were impressed by the skills needed. Many people use Excel every day but never thought it could be this intense. Some liked the idea of turning a work tool into a competition. A few worried that focusing so much on Excel might make people ignore more powerful tools, but others pointed out that Excel is everywhere and very important for many jobs. There were discussions about how these competitions show that “nerdy” skills can be cool, too. Some wanted to know how to join or watch future matches, and others shared tips for working faster in Excel. Overall, the story made many readers smile and look at spreadsheets in a new way.

Next, let’s talk about a huge backup of music. Anna’s Archive, known for saving books, has now made a massive backup of Spotify’s music and data. They scraped metadata and music files for about 256 million tracks, making everything available through torrents. The total size is close to 300 terabytes. The backup includes almost all Spotify metadata and about 86 million music files, covering 99.6% of songs people actually listen to. The files are sorted by popularity, with the most popular songs kept in higher quality and less popular ones saved in smaller formats. Anna’s Archive wanted to help preserve music for everyone, not just focus on famous artists or perfect quality.

The collection is being released in steps: first metadata, then music files, then more details like album art. The metadata is saved as SQLite databases, and the music files have extra info inside. They fixed problems like duplicate tracks and missing data, covering almost everything up to July 2025.

They also analyzed the data and found interesting facts: most Spotify songs are never played, and just a few get most of the listens. Most songs are singles, not part of albums. There are many AI-generated tracks, which makes it hard to find valuable music. They looked at genres, song lengths, and other features.

On Hacker News, some people think this is great for saving music history, especially if Spotify removes songs or goes away. Others worry about legal issues and say this could be seen as piracy. Some ask if it’s really “preservation” if people can’t legally use the files. There are concerns about the huge size—few people have hundreds of terabytes to download or share. Some are impressed by the technical work, like getting so much metadata and making it easy to search. Others hope this can help with music research or finding lost tracks. Some wish there was more focus on rare or deleted music. Others think this project might push Spotify or labels to rethink their control of music. People also shared ideas to improve the archive, like easier ways to get single tracks or help share the torrents.

Now, let’s go to the hardware side. One article talks about building graphics and music demos on a silicon chip with no CPU and no memory, using only about 4,000 logic gates. The author joined the Tiny Tapeout 8 competition and created two demos: a C64/Amiga-style intro and a Nyan Cat animation. The main challenge was to make all visuals and music in real time, using just logic gates and flip-flops—no ROM, no RAM, and every bit of state costs precious gates. The chip outputs 2-bit RGB to VGA and 1-bit audio. Everything, like 3D checkerboards, starfields, scrolling text, and music, is calculated on the fly using math tricks. For example, there are no lookup tables for sine waves; instead, they use vector rotation and shift registers.

The author built prototypes on an FPGA, then made a real silicon chip using the Skywater 130nm process. Fitting all the code and data was hard, so the author used clever tricks to reduce state and reuse logic. Music is generated with simple waves, and all patterns are encoded as logic. Audio is output using a simple sigma-delta DAC. For the Nyan Cat demo, the author extracted frames and music from GIF and MIDI files and mapped them to basic tables.

After much work, the chips arrived and the demos worked as expected. In the comments, many readers called this “real hardware wizardry.” Some asked about practical uses, but others said this is about learning and pushing limits. There were discussions about the math tricks used, and some shared links to classic demoscene intros. Some wondered if this could be used in commercial ASICs or devices, but most noted the lack of flexibility without a CPU or memory. There was praise for open silicon tools, but also talk about the hard, slow process of chip design. Some people were inspired to try Tiny Tapeout, and others shared their own old-school chip projects. While there are clear limits in color and sound quality, most agree that the creativity and skill are what really matter.

Let’s move on to a topic familiar to almost every developer: software logs and errors. One article says that when a program logs an “error,” it should mean something is really wrong and needs fixing. Today, many programs log too many errors for small problems, making it hard to know what is important. Logs usually have levels like info, warning, and error. Info is for normal events, warning is for something strange but not broken, and error is for things that must be fixed. If there are too many errors, people ignore them, and real problems get missed.

Many Hacker News readers agreed, saying they have seen too many useless error logs. Others said it can be hard to decide what counts as an error, since every system is different. Some think warnings are also important, and a few said that business needs sometimes cause developers to log extra errors. Some pointed out that what is an error for one team may not be for another. There were suggestions to use tools to filter logs and to teach new developers how to use log levels well. Most agree that error logs should be useful and focused on real problems.

Next, we have a story about using OpenSCAD, a tool where you code to make 3D models. The author wanted to learn OpenSCAD by making a simple battery holder. Unlike traditional CAD programs like Fusion 360, OpenSCAD lets you change your model by changing numbers in the code, like how many rows or columns you need. The author shared a short script that builds a box and uses loops and math to place holes for batteries. Changing a few variables updates the whole model. The author found OpenSCAD great for simple and practical parts, and much faster than using big, expensive CAD software—though it may not work well for complex shapes.

In the comments, many people love OpenSCAD for making models that are easy to update and share. Some think it’s perfect for programmers, but hard for those who don’t like to code. A few say it gets tricky or slow with complex shapes. There are tips about using libraries or mixing OpenSCAD with other tools, and some say OpenSCAD is great for teaching code and 3D thinking. Some wish for better errors or a nicer editor, and a few warn that reading other people’s code can be tough if it isn’t clear. Overall, OpenSCAD is seen as a neat tool for simple designs and people who like to code.

Now, let’s look at running powerful GPUs with small computers like the Raspberry Pi. The article tests if the Pi can work well with high-end graphics cards for AI, media transcoding, and GPU tasks. The Pi has only one PCIe Gen 3 lane, so much less bandwidth than a desktop. Still, the author found that for tasks done mostly on the GPU, the Pi can almost match a modern PC’s speed. For AI with large language models, a Pi setup with Nvidia GPUs was only 2% slower than a big Intel server. Media transcoding and 3D rendering also worked well, and the Pi uses much less power. The whole Pi setup costs around $400, much less than a PC. Using two GPUs at once helps only if they are the same model. Some trouble happened with AMD cards and drivers, and gaming is still hard on the Pi.

In the comments, people were surprised the Pi could handle such big jobs, calling it a good way to save money and power. Others pointed out the low bandwidth is still a problem for some tasks, like heavy video or games. Some think this is more for hobbyists and experiments than businesses. Some shared tips for better hardware or software setups, and a few hope for new ARM boards with better PCIe soon. Many see it as a fun project and a good way to learn, not a full replacement for a real server.

Next, there’s a story about a network engineer who used only IPv6 for a week to test how well it works and how to move from IPv4. They blocked all IPv4 traffic and used tools like NAT64 and 464XLAT to reach sites that only support IPv4. The article explains that IPv6 networks are different and simpler, since you don’t need tricks like NAT. IPv6 addresses are long, so every device can have its own. Devices can have many addresses for different uses, helping with home labs and servers.

The author tried different ways to move from IPv4 to IPv6. Dual stack means running both, but it’s hard in big networks. NAT64 lets IPv6-only devices reach IPv4 sites, and DNS64 helps by making fake IPv6 addresses. 464XLAT is used by some ISPs and Apple devices. After a week, the author found IPv6 works well, but only about half of websites support it. Apple devices work best, while others are less reliable. The author thinks networks should be designed for IPv6 first.

In the comments, some people agreed that IPv6 is the future and already use it. Others said they tried but stopped because of missing support or bugs. Some worry IPv6 is too complex, and NAT gives some safety. Some ISPs don’t offer IPv6, so people have no choice. There were also comments about clever tools like NAT64, but that they can break some apps. Some are hopeful more people will switch as IPv4 runs out, but others think it will take many years. Many think better guides and router support are needed.

Now, let’s talk about self-hosting Postgres. The article says you do not have to be afraid to run your own Postgres database instead of using a cloud service. The writer found self-hosting much easier and cheaper than expected. After two years and millions of queries, he only had one small problem during a migration. Postgres runs smoothly, and he saves a lot of money compared to AWS RDS. Cloud services became popular after 2009, but now their prices are much higher, and the main thing they offer is management, not better technology.

The move from RDS to self-hosted was simple, with no drop in performance. Daily tasks are easy: check backups, look at slow queries, and update software. The main risk is fixing the server if it goes down, but managed services also have outages, and you often still have to fix things yourself.

Self-hosting may not be best for everyone, especially new or very large teams, or if you need special certifications. For most people, though, self-hosting Postgres is not as scary as it sounds.

Hacker News commenters had many views. Some agreed, saying they have self-hosted for years with few problems and think cloud databases are too expensive. Others warned about the risks of missed backups or updates. Some said cloud services help with scaling and compliance, and some shared stories of cloud outages where managed services did not help. A few worried that new teams might not have the skills, but others said good automation makes self-hosting easy. Many agreed: for most projects, self-hosting Postgres is a good choice.

Now, let’s compare AI models playing games. The article looks at Gemini 3 Pro and Gemini 2.5 Pro playing Pokémon Crystal. Both models used the same setup and rules, with tools to help remember places, plan steps, and write code for puzzles. Gemini 3 Pro finished the game and became Johto Champion without losing a battle, while 2.5 Pro got stuck early and had trouble moving forward, especially in Olivine Lighthouse. Both models had tools like a mental map and notepad, and were told to act like scientists: make guesses, test them, and not trust their memories unless they saw proof.

Gemini 3 Pro was much more efficient, using fewer moves and less computer power. It made accurate maps, marked important places, found clever workarounds, and planned ahead. It even used vision to help solve problems. Still, Gemini 3 Pro made mistakes, like guessing without checking, or missing chances to do more at once. In the final fight, it used a smart plan called "Operation Zombie Phoenix" to win with a weak team.

In the comments, people were impressed by the progress in AI, saying it’s exciting to see AI solve real games. Some like the idea of teaching AI to make and test guesses, which could help in other areas. Others think the results are due to better coding or training data, not just the model. Some pointed out the models still lack common sense and can waste time on small mistakes. There were suggestions to test AI with only vision, and some hope to see tests on harder games next. Some debate if these models are truly learning or just following patterns. Most see hope for using agents like this in coding or robotics, but note that real-world tasks are much harder than Pokémon.

Next, let’s look at Biscuit, a new PostgreSQL index made to speed up LIKE and ILIKE queries, especially with wildcards like `%` and `_`. Biscuit works better than the usual pg_trgm index by removing the need to check results again, and it supports searching across more than one column. Biscuit uses bitmap indexes to track which records have certain characters at certain positions, both from the start and the end, and in a case-insensitive way. It also tracks string lengths for faster filtering.

When you do a pattern search, Biscuit breaks the pattern into parts, looks up each part in its bitmaps, and uses fast set operations to find matches. Everything happens in memory with fast bitmap math. Biscuit has many speed tricks and can handle multi-column indexes. It uses Roaring Bitmaps for memory efficiency. Benchmarks show Biscuit builds indexes much faster than pg_trgm, and queries with wildcards run much quicker. It’s simple to install and needs little tuning, but does not support regular expressions or locale-based matching, and it uses more memory.

Hacker News commenters are excited about Biscuit’s speed, especially for apps with lots of wildcard searches. Some are impressed by the clear explanations and benchmarks. Users who have struggled with slow LIKE queries like that Biscuit removes these pains. Others note that Biscuit does not support regex or fuzzy searches, so it won’t replace pg_trgm for every use. Some worry about memory usage for huge tables. There are questions about support for complex characters or collations. Some say Biscuit looks “almost too good to be true” and want to test it on their own data. Many are happy to see new index types for PostgreSQL and hope for more innovation in the future.

That’s all for today’s episode. Thank you for listening to Hacker News Daily Podcast. We’ll be back tomorrow with more top stories and discussions from the world of software and technology.