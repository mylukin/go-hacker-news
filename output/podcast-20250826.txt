Hello everyone, this is the 2025-08-26 episode of Hacker News Daily Podcast. Today, we have a full lineup of stories covering AI in the browser, new tools for programmers, science mistakes, open standards, medical research, and more. Let’s get started.

First, Anthropic is testing a new Chrome extension that lets their AI, Claude, work directly inside your browser. Claude can now click buttons, fill out forms, and help with daily tasks like managing your calendar, replying to emails, or testing websites. Anthropic says this could make Claude much more useful at work, but it also brings new safety risks. One of the biggest dangers is "prompt injection," where a website or email hides malicious instructions that trick the AI into doing something bad—like deleting emails or leaking private data. In testing, Claude even deleted emails after reading a fake security message.

To address these problems, Anthropic added new safety steps. Users can now pick which sites Claude can access, and Claude double-checks with users before taking risky actions, like buying things or sharing information. High-risk sites are blocked, and special tools look for suspicious requests. These changes cut successful attacks by more than half. For now, only a small group of trusted users are testing the extension in real-world situations. Anthropic wants to find more attacks and improve safety before letting everyone use it, and asks testers not to use Claude for sensitive sites like banks.

Hacker News users are split. Some are excited about the time-saving potential of browser AIs, while others worry about giving so much control to an AI, especially with open security problems like prompt injection. Many point out this is a big, unsolved issue for all browser agents. Some users think Anthropic is moving too fast, but others like the careful, slow rollout. Privacy is also a concern, with questions about whether browsing data is safe and if Claude could leak private info. Some users see big benefits for automating boring work or helping people with disabilities, but most agree users still need to be careful, especially with sensitive data. There’s also doubt about whether these defenses will be enough as attackers get smarter.

Next, let’s look at GNU Artanis, a web framework for the Scheme programming language. Artanis is designed to help people build web apps quickly and easily, using GNU Guile’s version of Scheme. The article gives a simple example where you can start a server and get a “hello world” reply with very little code. Artanis supports JSON, CSV, XML, WebSockets, and databases like MySQL, SQLite, and PostgreSQL. It is open source, using the GPLv3+ and LGPLv3+ licenses, and aims to be both lightweight and powerful. Artanis started in 2013 at a GNU event, later became an official GNU project with support from Richard Stallman, and is now maintained by a team led by Mu Lei. The project invites people to contribute or use Artanis for their own apps.

On Hacker News, some users are surprised and happy to see a modern web framework for Scheme, especially those who like Lisp languages. People liked the simple code and thought it looked easy to start. However, some wonder if the small user base will limit the community. There was discussion about Artanis’s use of advanced techniques like delimited continuations, which might be hard for beginners. Some users ask about performance compared to Python or Node.js frameworks. Others are happy that it is open source and fits GNU ideals, but a few worry about finding help or hiring developers. Many see it as a good tool for learning web servers and functional programming, even if not used at work. Some want better docs and more real-world examples. Overall, people respect the project but see it as a niche tool.

Now, Google has released Gemini 2.5 Flash Image, a new tool for creating and editing images with AI. This tool can combine different pictures, keep the same character looking the same across images, and make changes using simple text instructions. It is faster, cheaper (about four cents per image), and more powerful than previous models. Developers can use it through API, Google AI Studio, or Vertex AI. It is easy to build apps with it—for example, users can upload a photo and add filters with simple prompts.

A big new feature is “character consistency”—the same person or product can look the same in many pictures, even in different poses or places. This is helpful for ads, games, or brand images. The tool can also follow visual templates for things like ID badges or product cards. You can edit images by just telling the AI what you want, like blurring a background or changing a pose. It also supports “multi-image fusion,” letting you blend several photos into one. All images get a hidden digital mark to show they were made by AI, and Google shared a Python code example for using the model.

Hacker News users are excited about the easy image creation and low price. Some worry about misuse for fake news or deepfakes, even with watermarks. Many are interested in whether the tool keeps character consistency, which is hard for AI art. Some compare the quality to tools like Midjourney or DALL-E. There are questions about commercial use, privacy for uploaded photos, and if the watermark can be removed. People also wonder about speed for large jobs and if the price will stay low. Some think this could change how images are made for the web, marketing, or games.

Next, a story about diplomacy. When an ambassador is summoned by the host country’s foreign ministry, it usually means the host country is unhappy about something. This is a common tool in international relations, mostly to send a message. The process starts with a formal note asking the ambassador to come in. Sometimes the press is told to show action is being taken. Meetings can be very formal or relaxed, but both sides follow strict diplomatic rules. Sometimes, small gestures—like making the ambassador wait or skipping coffee—show displeasure. The main goal is to share concerns and make positions clear, not to start a fight. If things get very bad, the ambassador can be declared “persona non grata” and must leave, but usually, meetings end without major action.

Commenters note that details depend on the country and situation. Some see it as a theatre where everyone knows their role. Others say the real purpose is often to show the public that something is being done, not to solve the problem right away. Some point out that the seriousness can be seen by who the ambassador meets. Refusing a summons is rare and can harm relations. Most agree it’s an important part of diplomacy.

Next, a science story about a typo that spread through many research papers. A paper mentioned “Cr2Gr2Te6,” but this is not a real material—it was a mistake for “Cr2Ge2Te6” (chromium germanium telluride). The error is easy to make, since “Ge” and “Gr” are close on the keyboard. The fake formula was repeated in many later papers and even books, making it look real. The author warns that such mistakes can spread even faster with AI tools, which might copy errors without checking facts. By pointing out the mistake, the author hopes to stop it from spreading further.

In the comments, people blame the academic system, where the pressure to publish leads to mistakes. Others say no one is paid to proofread, so errors slip through. Some worry AI will make this worse, while others say ChatGPT only avoided the error because it had seen the correction. There is debate about whether the real material is a compound or an alloy, and someone shares an example of another nonsense phrase that spread because of bad AI training data. Many agree that if mistakes are repeated often, they can look like facts, and both humans and AI must check details carefully.

Now, let’s talk about a new Ruby tool called “rv.” The creator wanted a better way to manage Ruby versions and gem dependencies, inspired by the Python tool “uv.” Most Ruby tools today require several steps to install Ruby and gems. “rv” aims to do all this in one step, installing pre-built versions of Ruby quickly and letting you run scripts or tools with the right Ruby version and dependencies each time. It is written in Rust for speed, and takes ideas from tools like “cargo” and “npm.” You can run any gem as a CLI tool, and it will handle dependencies in their own space. You can also put Ruby version info in your scripts, and “rv” will set everything up.

The team already has a working version that can install Ruby 3.4.x in one second on macOS and Ubuntu. In the comments, many are excited about a modern Ruby tool inspired by “uv” and “cargo.” Some hope it will solve the slow and confusing process of managing Ruby versions and gems. Others ask about support for older projects, other operating systems, and if “rv” can fully replace “rvm,” “rbenv,” and “bundler.” Security is also a concern, and some want to know about Windows support. Users also gave feedback on speed, naming, and tried the alpha version, finding it easy to use. Overall, the community seems hopeful and curious to see if “rv” can make Ruby development faster and easier.

Next, we have a story about a proposed new HTTP standard called “AI-Disclosure.” This header lets websites say if and how AI was used to make their content. It uses a simple dictionary format, with keys like “mode” (how much AI was used), “model” (which AI), “provider,” “reviewed-by,” and “date.” The header is optional, and if it’s missing, you can’t tell if AI was used or not. It is just for information and is not a security feature. Stronger systems like C2PA exist for cryptographic proof, but “AI-Disclosure” is meant to be easy for most sites to use.

In the comments, some people like the idea for transparency, while others say it is too easy to lie or skip the header. Some developers like its simplicity, but worry about “banner blindness” if warnings become too common. Others point out that similar efforts have failed before, and this will only work if big sites and browsers support it. Some think cryptographic signing is the only real solution, but see this header as a first step. There are also concerns about privacy and tracking, since headers can sometimes be used to gather extra info. Some joke that AI content will just label itself as human to get around the rule.

Now, about LiteLLM, a startup that builds tools to help developers use many large language models (LLMs) through one simple API, similar to OpenAI’s. LiteLLM is open source and trusted by big names like NASA and Adobe. It lets you use more than 100 LLM APIs through the same code, making it easy to switch between models. The backend uses Python and FastAPI, the frontend uses JavaScript and TypeScript, and there’s support for tools like Redis and Postgres.

LiteLLM is hiring a founding backend engineer to make their product better and faster. The job includes keeping the API compatible across many providers, adding features like cost tracking, and helping the system scale. The team is very small, so new hires will have a big impact. LiteLLM recently raised $1.6 million and offers good pay and equity. In the comments, people like how LiteLLM helps avoid vendor lock-in and makes it easy to try different models. Some are interested in the technical challenges, like keeping features and bugs in sync across providers. Others discuss the company’s growth, pay, and whether remote work is possible. There is also debate about whether companies will use many models or stick with just one in the future, but most agree that tools like LiteLLM make experimenting much easier right now.

Moving to medical research, a recent study looked at a possible “universal antiviral” drug. The idea comes from people with a rare immune mutation in the ISG15 gene, who never seem to get sick from viruses. Their bodies show constant, mild inflammation, but they don’t get viral illnesses like flu or chickenpox. Scientists created a therapy that can turn on the same antiviral state for a few days using lipid nanoparticles and mRNA for ten special proteins. This gives strong protection, but with less inflammation.

In animal tests, the therapy protected mice and hamsters from viruses like flu and COVID-19. In the lab, no virus could get past it. The hope is this could be used during pandemics or for people at high risk, especially before vaccines are ready. But there are challenges: getting enough drug to the right place is hard, the effect only lasts three to four days, and safety in humans is not proven yet.

In the comments, many are excited about the idea of a universal antiviral, especially for future pandemics. Some worry about risks from inflammation and side effects. Others point out that viruses may adapt, so nothing will be truly universal for long. There is also talk about the challenge of delivering mRNA drugs deep into the lungs, possible patent and pricing issues, and how animal results may not mean it will work in humans. Some say this could change how outbreaks are handled, buying time while vaccines are made. Many like that curiosity-driven research into rare diseases can lead to big advances.

Finally, there is a story about financial conflicts of interest in the group who wrote the DSM-5-TR, the main book used to diagnose mental illnesses. The study found that about 60% of US doctors on the DSM-5-TR panels got payments from drug or medical companies between 2016 and 2019, totaling over $14 million. Most payments were for research, but many also got money for travel, meals, consulting, and speaking. The DSM affects what gets diagnosed and what drugs are prescribed, so these financial ties could have a big effect.

This is not a new problem—the same issue existed with earlier DSM versions. The American Psychiatric Association has made the process more transparent but still does not share full details. The study authors argue that DSM panels should avoid industry ties, or at least not let those with conflicts make final decisions. The study only covered US doctors, so the full picture is unclear.

Hacker News users were not surprised. Some say financial conflicts are common in medicine, not just psychiatry. Others point out it is hard to find experts with no industry ties. Some think the key problem is lack of transparency, and suggest including people from outside the medical industry or making decisions fully public. There are worries that industry influence leads to overdiagnosis and too many prescriptions. But some say as long as ties are disclosed, readers can decide for themselves. Most agree the DSM should be as fair and unbiased as possible.

That’s all for today’s episode. Thanks for listening to Hacker News Daily Podcast. We’ll be back tomorrow with more top stories and discussions from the tech world.