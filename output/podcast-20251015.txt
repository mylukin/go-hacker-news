Hello everyone, this is the 2025-10-15 episode of Hacker News Daily Podcast. Today, we have a packed show with news from Apple, Anthropic, and F5, plus stories about hardware, security, and programming ideas. Let’s get started.

First, Apple just announced the M5 chip, their newest processor for MacBook Pro, iPad Pro, and Vision Pro. This chip focuses on much better AI performance and faster graphics. The M5 uses new 3-nanometer technology, making it more efficient and powerful. It has a 10-core GPU, and each core includes a special Neural Accelerator for running AI tasks over four times faster than the M4. Graphics performance is up to 45% better, with new features like third-generation ray tracing for more realistic visuals in games and 3D apps.

The CPU has up to 10 cores, split between performance and efficiency, and is now the fastest Apple has made, with up to 15% better multitasking speed. The Neural Engine is now 16 cores, offering more speed and better energy use. The memory system is improved, with bandwidth up to 153GB/s, almost 30% more than before, which helps run bigger AI models locally. This means users can run advanced AI apps, like image generators and language models, directly on their devices without the cloud.

Apple says these improvements help creative work, gaming, and daily tasks, thanks to better AI in photo editing and writing tools. The M5 also uses less energy, supporting Apple’s goal to be carbon neutral by 2030.

On Hacker News, commenters have mixed views. Some are impressed by the technical jump, especially the AI accelerators in the GPU, thinking this can help Apple lead in AI features. Others note that Apple’s benchmarks are always against older Apple chips, not against Intel, AMD, or Nvidia, so it’s hard to know how the M5 really compares. Some worry that hardware upgrades may not matter if software doesn’t keep up, since many AI and developer tools do not work well on Apple yet.

Some developers are happy about faster on-device AI, expecting more privacy and better performance. But others want to see real-world tests first. There are comments about the high price of new Apple devices and how each year’s chip upgrade can feel small for most users. A few people question if the new AI features will really change daily use, or if it’s just marketing. In short, the M5 is a strong update, but many want to see if software and real use will match Apple’s promises.

Next, Anthropic released Claude Haiku 4.5, a new AI model focused on fast and cheap coding and real-time chat tasks. Haiku 4.5 gives almost the same quality as their top model, Sonnet 4.5, but at one-third the price and more than double the speed. It’s even better than Sonnet 4 in some areas, like using computers and working with other AI agents. This makes it useful for chatbots, coding helpers, and customer service bots. Developers can use it for rapid prototyping and big coding projects, with quick responses for pair programming. The model is available through the Claude API, Amazon Bedrock, and Google Cloud, at $1 per million input tokens and $5 per million output tokens.

Benchmarks show Haiku 4.5 reaches about 90% of Sonnet 4.5’s coding ability and is up to five times faster in some tests. It does well in following instructions and generating code for tools like GitHub Copilot. Haiku 4.5 is also safer than earlier models, with fewer risky behaviors and lower chances of harmful outputs, so it is rated at a less strict safety level.

In the comments, many are excited about the speed and low price, saying this will help more developers use strong AI, especially with limited budgets. Some warn that real-world results might differ from benchmarks, especially for bigger jobs. A few are happy about better safety but ask what “safety” really means and if the tests are good enough. Some compare Haiku 4.5 to OpenAI and Google models, wondering if Anthropic can keep up. There are worries about job loss if coding AIs keep getting better and cheaper. Some developers say switching to Haiku 4.5 was easy and made their apps faster, but others want more API features. One user suggests using several smaller, faster AIs in parallel could create new ways to build software. Still, some skeptics say there’s a lot of AI hype, and only time will tell if Haiku 4.5 changes real developer workflows.

Now for a security warning. A developer was almost hacked after getting a fake job interview from a company that looked real on LinkedIn. The attacker used a professional company profile, a normal-looking coding challenge, and a test project sent before the meeting. The codebase came from Bitbucket, had clean documentation, and even a company logo. In a rush, the developer checked the code but did not run it right away. Before starting the app, he asked his AI assistant to scan for problems. The AI found hidden, obfuscated code in the user controller file. If run, this code would fetch and run malware to steal files, passwords, and crypto wallets. The hacker’s server went offline after 24 hours, making it hard to trace. VirusTotal confirmed the payload was very serious.

The scam was clever: a fake LinkedIn profile matched to a real company, professional language, a reasonable coding test, and good scheduling tools. It used pressure, authority, and familiarity to gain trust.

The main lesson: always sandbox unknown code, use AI to scan for malware, and double-check everything, even if it looks real. Developers are easy targets because they often run code from strangers and have access to valuable data.

In the comments, people were shocked by how advanced and believable the scam was. Some said this kind of attack is becoming common, especially for developers in crypto. Others shared stories of fake interviews with hidden malware. Many stressed the need to use containers or virtual machines for unknown code. Some say even advanced users can make mistakes when rushed. There are calls for LinkedIn and others to do more to stop fake profiles. Most agree: be paranoid, check everything, and never trust code from strangers.

Next, let’s talk about a new idea in physics. Sabine Hossenfelder has a theory about how gravity might explain the “collapse” of the wavefunction in quantum physics. The wavefunction collapse is a big mystery, since it seems that measuring something changes reality in a strange way.

Hossenfelder suggests that by adding gravity to quantum theory, we can explain the collapse without extra rules. In her model, gravity and matter are connected at a basic level. When a quantum system gets big enough, gravity acts to “select” one outcome, causing the collapse. This happens locally, not faster than light. The model uses only what we already know about gravity and quantum physics, and gives predictions that can be tested in experiments.

People on Hacker News have mixed views. Some are excited to see a simple idea connecting gravity and quantum mechanics. Others are skeptical, saying similar models have been tried before, and it is hard to get real proof because testing gravity at the quantum level is difficult. Some wonder if the idea fits with other theories like string theory. Others ask if the predictions are clear enough to test soon. A few think adding gravity might just move the mystery around. Still, some hope this idea will make more physicists think in new ways. Most agree that physics needs experiments, not just clever ideas.

Now, a story about security in open source. Researchers found a security problem in the Nix ecosystem, using GitHub Actions. Some workflows used the `pull_request_target` trigger, which allows workflows to access secrets and write to the repository—even from forks. If not careful, attackers could use this power to run bad code or steal tokens.

The authors found two dangerous examples. In the first, a workflow checked changed files using `xargs` and `editorconfig-checker`. If someone named a file in a tricky way, they could inject commands. In the second, a workflow checked a file called OWNERS. Attackers could replace this file with a link to another file, such as a credentials file, and steal tokens with write access.

Once found, maintainers quickly fixed the problems by disabling risky workflows and changing how they handle untrusted data. They also set permissions more safely.

The main lessons: never mix untrusted data with secrets, only give workflows the permissions they need, and always read the GitHub Actions security docs. If worried, you can disable all actions in your settings.

In the comments, some are surprised such mistakes were in a big project like Nixpkgs. Others say these problems are common because GitHub Actions is easy to misuse. Some argue that YAML and Actions’ design make mistakes easy, and want better tooling to scan for risky workflows. Many praise the quick fix and clear write-up. There is a debate about whether CI systems can ever be truly safe, or if risk will always be there. Most see this as a good lesson for the open source community.

Moving on to hardware, Backblaze shared data from their data centers over 13 years to look at hard drive reliability. The classic “bathtub curve” says drives fail a lot at the start, then work well, then fail again near the end. But Backblaze’s data shows today’s drives last much longer and fail less often, especially early and mid-life. In 2013, failure rates peaked at about 13-14% after 3-7 years. In 2025, the peak is just over 4% and after more than 10 years—a big improvement.

Backblaze now uses more drives, better decommissions, and buys in bulk, making their data more stable. Drives are often retired before they fail, which lowers the failure rates shown. The article says the bathtub curve is too simple and does not account for workload, manufacturing, or drive management.

Most commenters agree hard drives are getting better and last longer. Some point out that Backblaze’s data is from data centers, so home users might see different results. Others wonder if retiring drives early makes the numbers look better than real results. There is debate on whether Backblaze’s methods are special, and some worry about rare sudden failures. Many share stories of old drives lasting over a decade, while others mention new problems like higher costs or firmware bugs. Most like the open data and hope to see more updates.

Next, let’s look at Halloy, a new open-source IRC client built in Rust for Mac, Windows, and Linux. It aims to be simple, fast, and easy to use, with a modern interface using the Iced GUI library. Halloy supports many IRCv3.2 features, including account notifications, chat history, message tags, and SASL authentication. Users can join multiple servers and channels, use keyboard shortcuts, get notifications, and enjoy auto-complete for nicknames and channels. Halloy also has custom themes and a portable mode. It’s available from Flathub, Snap Store, or can be built from source, with clear online documentation.

In the comments, people are happy to see a modern IRC client written in Rust. Some like that it’s cross-platform and open-source. Others ask about features like DCC file transfers and bouncer support. Some share stories of building IRC clients or discuss the challenges of GUI programming. A few wish for mobile versions, while others think desktop is best for IRC. Some are concerned about missing features or bugs but most agree Halloy is a good project with promise. Many hope this client will help new users try IRC.

Now, a look at the Princeton Engineering Anomalies Research (PEAR) program, which studied if human minds can affect machines and physical systems. PEAR ran for almost 30 years at Princeton University, and now its work continues with other groups and a company called Psyleron, which sells random event generators.

PEAR tested if people could influence random machines just by focusing their thoughts. They also researched things like remote viewing and what their findings mean for science and even spiritual ideas. Today, PEAR works to archive their research and share it, offering lots of material online and a DVD with lectures and a virtual tour.

On Hacker News, some people are very skeptical and think results are more about wishful thinking than real science. Others find the research interesting and say it’s good to question what we know about consciousness. Some mention that it’s important to test strange ideas, because sometimes they lead to new discoveries. A few are excited the data and methods are open and want to try the experiments. Still, many worry the research mixes science with spiritual beliefs too much. Overall, there’s a mix of curiosity, doubt, and hope that we might learn something new about the mind.

Next, a discussion about monads in functional programming. The article asks if monads are too powerful for most needs. Monads let you build programs where each step can depend on the last, which is great for flexible code in Haskell. But this makes static analysis harder, since you can’t always predict what effects will happen just by looking at the code.

The article explains there’s a spectrum: on one side, strong static analysis and less expressive code; on the other, more expressive code that’s hard to analyze. Monads are very expressive, but sometimes you don’t need all that power. Using simpler tools, like applicatives, can help. Applicatives are more limited but make it easier to analyze what a program will do before running. There’s also something in between called selective applicatives, which give a balance between power and safety, but are harder to write and read.

In the comments, some agree monads are often used when something simpler would work, and that more static analysis helps with safety and understanding. Others say monads are still needed for real-world programs needing lots of flexibility. Some think it’s up to the developer to pick the right tool. A few mention that new research might bring better solutions. Many like the idea of finding a “sweet spot” between too much and too little power.

Finally, big security news from F5, a major U.S. cybersecurity company. Hackers broke into their systems and stole secret flaws and source code for their main product, BIG-IP. The attack was found in August 2025 and lasted long enough for hackers to get into product development tools and engineering documents. BIG-IP is used by many top companies to manage web traffic and keep applications secure.

F5 says hackers did not change their software supply chain or put bad code into products. Customer data, like financial or support info, was not touched. Other products, such as NGINX and Silverline, were not affected. After the hack, F5 made systems more secure, changed passwords, improved controls, and added better monitoring. They worked with outside experts to check for problems, and have released updates and patches. Customers are told to update their systems as soon as possible and follow new security best practices.

On Hacker News, some say this shows the risk of closed-source security products. Others point out that many companies depend on F5, so even a small leak is serious. Some are surprised F5 did not notice the breach sooner, but others say such attacks are hard to spot. A few worry that the stolen flaws might be sold or used later. Some praise F5 for working with outside experts and sharing updates quickly, while others question if the company is telling the full truth. There are suggestions to use open-source tools for more review, and reminders that no system is perfect—attacks like this are a warning to stay alert and keep software up to date.

That’s all for today’s episode. Thank you for listening to Hacker News Daily Podcast. We’ll be back tomorrow with more news and stories from the tech world.