# Hacker News 故事摘要 - 2025-12-29

## 今日概述

Today’s top Hacker News stories talk about changes in online ads, new ways to manage computer memory, and ways to show disk space. There are also stories about AI and programmers, internet blocks in Germany, preserving old games, and issues with bias in large language models. Some stories show new tech projects, like faster hash tables and better tools to detect proxies. Many stories are about finding new solutions when old ones stop working.

---

## Google is dead. Where do we go now?

- 原文链接: [Google is dead. Where do we go now?](https://www.circusscientist.com/2025/12/29/google-is-dead-where-do-we-go-now/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46425198)

The article talks about how Google Ads stopped working for the author’s business, which sells circus and magic shows. For many years, Google Ads brought in new customers, but now, even after spending a lot more money and getting bonus ad credits from Google, the business saw no new leads or sales.

The author explains that they used to check their ad campaigns once a month, but as things got worse, they started checking every week and tried many different settings. Even with a huge increase in ad budget, nothing changed. Even when Google gave extra money for ads during the holidays, there was still no effect. After spending all the money, the author decided not to use Google Ads anymore because it just did not work.

Now, the author is trying new ways to find customers. They are testing ads on TikTok and Instagram, because young people spend time there. Half of their customers are old customers, so they started sending out more email newsletters to remind people about their business. The author also plans to do in-person marketing, like going to a market and doing free shows while handing out flyers. They are working on selling new products, like their Magic Poi project. The author ends by saying that things are tough right now and even offers to build websites or IoT projects for anyone interested.

In the Hacker News comments, some people agree that Google Ads are not as good as they used to be, and many small businesses are seeing the same problems. Others say that Google’s ad system has become too complex and expensive for small companies. Some point out that search results on Google are also getting worse, so people may be using Google less. A few think that social media ads are also hard and can waste money, but they might work better for the younger audience.

Some users suggest old-school methods like email newsletters and in-person networking, saying these are still powerful. Others share stories about using word of mouth or building a strong community around their business. A few warn that moving to TikTok and Instagram might not help if the product is not right for those platforms. Some developers joke that maybe it’s time to build a new kind of search or ad tool. But most agree that finding new customers is hard now, and there is no easy answer. Many feel that small businesses need to try many things and not just trust one big platform like Google.

---

## Static Allocation with Zig

- 原文链接: [Static Allocation with Zig](https://nickmonad.blog/2025/static-allocation-with-zig-kv/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46422009)

This article is about building a simple key/value server in Zig that only allocates memory once at startup, never during normal operation. The author made a Redis-like server called "kv" to learn about static memory allocation and how it can make systems more predictable and stable.

The main idea is to ask the operating system for all the memory the server will ever need during startup, and then never allocate or free memory again until the server stops. This avoids problems like use-after-free bugs and makes it easier to reason about how much memory is used. It also means the server has clear limits—like a maximum number of connections or data it can handle at once.

For handling client connections, the code uses pools of connection objects and data buffers. When a client connects, a connection object is taken from the pool; when the client leaves, it goes back into the pool. If the pool is empty (too many clients), new connections are rejected. The same method is used for buffers that hold request and response data.

Parsing commands is done with a fixed-size buffer allocator that is reset after each request. This avoids copying data and allows fast processing, since the server is single-threaded and only deals with one request at a time.

For storing keys and values, the code uses a hash map that does not own any memory itself; instead, it uses a big pool of pre-allocated memory for keys and values. This means the server must know how many keys, how big each key can be, and how many values each key can hold. To be safe, it has to allocate enough memory for every key to use the maximum space, even if most keys use much less.

A drawback is that static allocation can waste memory if the actual usage is less than the maximum allowed. Deleting keys can also be tricky, as the map must track which spaces are empty. The author suggests a custom map might work better in the future.

The amount of memory the server uses is set by a configuration, such as the number of connections, max key size, value size, and list size. For example, with 1000 connections and 1000 keys, the server used about 750 MB of memory right away, before storing any data. If you raise the limits, memory use goes up fast.

In the comment section, some users praised Zig for making manual memory management easier and safer, saying it’s a good fit for this kind of low-level work. Others pointed out that static allocation is common in embedded systems or high-reliability servers, but not always needed for general applications. A few people worried about wasted memory or the risk of setting limits too low, making the server unusable under heavy load. One commenter liked the clear, predictable behavior, while another asked if there are ways to reclaim memory when clients disconnect or data is deleted. Some suggested using more advanced data structures or smarter allocation patterns to save memory. A few shared their own stories about building servers with static allocation, saying it helped catch bugs early. Finally, several agreed that thinking through memory needs up front leads to better code, even if it feels limiting at first.

---

## Flame Graphs vs. Tree Maps vs. Sunburst (2017)

- 原文链接: [Flame Graphs vs. Tree Maps vs. Sunburst (2017)](https://www.brendangregg.com/blog/2017-02-06/flamegraphs-vs-treemaps-vs-sunburst.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46401052)

The article compares three types of data visualizations—flame graphs, tree maps, and sunburst charts—for showing disk space usage in file systems. The author uses the Linux kernel source code as an example to show how each method works and what details you can see quickly.

Flame graphs show data with long, labeled rectangles. The length of each box tells you how much space a folder or file uses. You can quickly spot the biggest folders, like “drivers,” which takes up over half the space. Even if you print the graph or use it in a slide, you still get a clear view of the main parts. Small files and folders appear as thin lines and are less important at a glance.

Tree maps use space differently. Each box’s size shows how much space a file or folder uses, but it’s harder to compare exact sizes quickly. Some tools, like GrandPerspective and Baobab, let you hover for details, but screenshots don’t help with that. Labels can be missing in tree maps, which makes it harder to know what you’re looking at. However, tree maps use space well and can show many items at once.

Sunburst charts are round flame graphs. They look nice and can impress people. But the way slices are drawn can be confusing—sometimes a smaller-looking slice is actually bigger because of how the angles work. It’s harder to judge size by angles than by lengths or area, so you might get the wrong idea about what’s big or small.

The author also mentions two text tools: ncdu and du. Ncdu gives you a simple text view with bars for each folder but only shows one level at a time. Du is even simpler—it just lists sizes, so you have to read the numbers.

The author’s advice is to offer all three visualizations if you’re building a tool. But for a default view, flame graphs are often best for seeing the big picture fast.

In the Hacker News comments, some people say they like flame graphs because they’re easy to read and great for finding problems in code or disk use. Others prefer tree maps, saying they’re better for finding big files quickly. A few think sunburst charts look nice but can be misleading. Several users mention that labels are important—without them, you get lost. Some say that simple text tools like du and ncdu are still the fastest for daily use. One commenter points out that the best tool depends on what you’re trying to find: if you want to see the biggest folders, flame graphs and tree maps both help, but in different ways. Another user says that flame graphs are best for showing call stacks in code, not just disk usage. There are also suggestions to combine these methods or let users pick which one they like. Finally, some users say that visualizations can be hard for colorblind people, so clear labels and good design are key.

---

## Left Behind: Futurist Fetishists, Prepping and the Abandonment of Earth

- 原文链接: [Left Behind: Futurist Fetishists, Prepping and the Abandonment of Earth](https://www.boundary2.org/2019/08/sarah-t-roberts-and-mel-hogan-left-behind-futurist-fetishists-prepping-and-the-abandonment-of-earth/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46424217)

This article looks at why some people, especially the rich and powerful, focus on escaping Earth or preparing for disaster instead of fixing problems here. It talks about “preppers” who get ready for the end of the world and about tech elites who want to leave the planet, like Elon Musk with SpaceX.

The authors start by discussing the “Left Behind” books, which are about a Christian idea called the Rapture, where believers go to heaven and others must survive on a ruined Earth. The books and movies made “prepping” and end-times survival very popular, even for people who are not religious. The article connects this with modern preppers, who build bunkers and store food and weapons, much like people did during the Cold War. Preppers are often shown on TV as obsessed or strange, but they are part of a bigger worry about the future.

The article then moves to talk about rich people and tech leaders who are preparing in different ways. Instead of building bunkers, they fund big projects like Mars missions, closed environments like Biosphere 2, or even new company headquarters designed like spaceships. These projects, the authors say, show a wish to escape Earth rather than fix its problems. They also mention “accelerationism,” a belief that speeding up problems like capitalism or environmental damage will make society collapse faster—so a new world can start.

Some projects, like Apple’s “Spaceship” campus or the NSA’s control room, look futuristic and safe, but really only help a few people. The authors argue that these ideas are not about saving everyone, but about protecting the rich and powerful. The article says this is a kind of “secular Rapture,” where only some get to escape, and most people are “left behind.”

In the comment section, some readers agree with the article and say it is worrying that the rich are planning to leave Earth or save only themselves. Others think prepping makes sense because governments are not doing enough to fix big problems like climate change. Some commenters point out that space travel projects have given us useful technology and can inspire people, but they agree that we should not give up on Earth. A few users feel that the article is too harsh on tech leaders, saying that trying to leave Earth is a normal human dream, not just selfishness.

There are also comments about how prepping and survivalism are not new, and that people have always worried about disaster. Some readers say that focusing on leaving Earth is just another way for companies to make money or power. Others mention that not everyone can afford to prepare, so these projects can make inequality worse. A few commenters say that it is possible to work on both space exploration and fixing problems on Earth at the same time. Overall, the discussion shows people are divided: some feel we are ignoring real problems, while others think looking to the future is part of human progress.

---

## The Future of Software Development Is Software Developers

- 原文链接: [The Future of Software Development Is Software Developers](https://codemanship.wordpress.com/2025/11/25/the-future-of-software-development-is-software-developers/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46424233)

This article talks about the idea that new technologies, like AI and no-code tools, will make software developers unnecessary. The writer has been a programmer for over 40 years and says he has seen these claims many times before, but they never come true.

The article lists past examples: visual programming tools, macros, no-code platforms, and now large language models (LLMs) like AI code assistants. Each time, people said programmers would not be needed anymore. But, instead, software development kept growing, and more programmers were needed, not fewer.

The author explains that the hard part of programming is not writing code, but turning human thoughts into precise instructions that a computer can follow. This is difficult because human language is often unclear, and computers need very clear and exact commands. Even if AI or tools help with writing code, people still need to decide exactly what the software should do.

He says there is no real proof that AI is replacing programmers. Layoffs in tech are more about business reasons, like over-hiring or rising costs, not because AI is doing all the work. AI code tools, he explains, do not really understand the code like a human does—they just try to predict what code should come next. This often leads to code that has mistakes or does not work as expected, and a real programmer is needed to fix it.

The writer does not believe that AI will soon become smart enough to do the thinking that human programmers do. He suggests that, in the future, programmers will use AI as a helper, maybe for small tasks or to make simple code faster, but the important work will still need real people.

He also doubts that the big, expensive AI models will last very long because they cost too much to keep running and updating. In the end, he thinks companies will still need more programmers and should keep hiring and training them.

Hacker News commenters share many different views. Some agree with the author, saying they have seen the same cycles and hype before, and that every new tool just means more demand for programmers. Others think that AI will change some parts of programming, making simple or boring tasks easier, but not replacing the skill of good developers.

A few commenters are more hopeful about AI, saying it might eventually handle more complex jobs, but even they admit it cannot fully replace human judgment or creativity right now. Some point out that as tools get better, the problems we try to solve get harder, so humans are always needed at the edge of what is possible.

There are also comments about business, with people saying that layoffs are often about saving money, not about AI. Some warn that believing the hype about AI replacing developers can be risky for both companies and workers.

A couple of commenters add that while AI can help write code faster, it cannot talk to users, understand their needs, or fix big design problems. Many agree that good programmers are needed to check AI’s work and to make sure software stays reliable and safe.

Some worry that too much dependence on AI could lead to software that is harder to understand and fix. Others think the future will see a mix—AI helping with routine work, but humans still in charge when it matters most.

Overall, the comments show that most people believe software developers will stay important, even as new tools come and go.

---

## List of domains censored by German ISPs

- 原文链接: [List of domains censored by German ISPs](https://cuiiliste.de/domains)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46423566)

This article lists websites blocked by German internet providers, as tracked on cuiiliste.de. The list includes domains like Anna’s Archive, bs.to, and several streaming or file-sharing sites, along with the dates they were blocked.

The CUII (Clearingstelle Urheberrecht im Internet) is the German group that suggests which domains should be blocked, mainly for copyright reasons. The table shows each domain and when it was added to the block list. Some blocked domains are well-known for sharing books, movies, or music without permission from rights holders. For example, Anna’s Archive is a site for finding free books, and bs.to is a streaming site for TV shows. The dates in the list sometimes mean when the block was suggested, or when the site first appeared in the database. This is not always the same as when the block started. The list is updated regularly and tries to stay current. The article does not explain the technical method of blocking, but in Germany, ISPs often use DNS blocking, which stops users from visiting these domains by name. The list helps people see what is being censored and when. It shows that content blocking is happening for many types of websites, not just one kind.

In the comment section, some people are worried about internet censorship. They say blocking whole domains can stop access to legal content too, not just pirated stuff. Others point out that DNS blocks are easy to bypass with VPNs or different DNS servers, so the blocks may not work well. A few commenters think blocking is needed to protect copyrights, especially for small creators. Some users share ways to get around the blocks, while others warn that making it easy for people to avoid blocks just leads to more strict controls in the future. One person says that blocking lists should be public, so everyone knows what is being censored. There are also comments about how blocking can hurt internet freedom and open access to information. Some users compare Germany’s approach to other countries and worry it could become more strict over time. A few people ask why legal streaming services are not enough to stop piracy. Others argue that blocking does not fix the real problems, like high prices or bad service, which make people look for alternatives.

---

## Which Humans?

- 原文链接: [Which Humans?](https://osf.io/preprints/psyarxiv/5b26t_v1)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46424782)

This article looks at how large language models (LLMs) like ChatGPT are compared to “humans” in research, and asks who these humans really are. The main point is that most studies use test data from people in Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies, but humans everywhere are not the same.

The authors say humans around the world have a lot of cultural and psychological differences. When LLMs are tested, their answers are most similar to people from WEIRD countries, but much less like people from other backgrounds. They show that the more different a culture is from Western countries, the less the LLM’s answers match that group (with a strong negative correlation, r = -0.70). This means LLMs are not really “universal” human models—they are more like models of WEIRD humans.

Ignoring this problem can lead to mistakes in science and ethics. For example, using LLMs in places with different cultures could cause misunderstandings or unfair results. The article warns that most of the text used to train these models comes from English and from Western sources, missing out on the knowledge and ways of thinking found in other cultures. The authors suggest that future language models should use more data from a wider set of people and cultures to reduce this bias. They hope this will make AI systems more fair and accurate for everyone, not just for people from WEIRD societies.

In the comments, some readers agree this is a big problem and say it’s similar to earlier issues in psychology and medicine, where Western data was used for everyone. Others point out that it’s hard to find enough data from less-represented cultures, especially in digital text form. Some think this bias is not easy to fix and might always be part of AI, since the internet itself is dominated by Western sources. A few people wonder if it’s possible or even helpful to make LLMs act like “average humans” worldwide, since every culture is different. There is also debate about whether it is better to have specialized AIs for each culture, or try to make one big, global model.

One commenter says that users should always know about these biases when using AI tools, especially in important areas like health or law. Another person asks if adding more translated non-English texts would help, but others reply that translation itself can change the meaning and miss local ideas. Some are hopeful that as more of the world goes online, future LLMs can become more inclusive. But others warn that just adding more data is not enough; AI creators must also understand and respect cultural context. Overall, most agree this is a hard but important problem if we want AI to serve everyone fairly.

---

## All Delisted Steam Games

- 原文链接: [All Delisted Steam Games](https://delistedgames.com/all-delisted-steam-games/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46424262)

This article is about a website that lists all the games removed, or “delisted,” from Steam, the biggest PC game store. The page shows over 1,000 games that you can’t buy on Steam anymore, along with their publishers and developers.

The main idea is that even in the digital age, games can disappear. The site organizes these lost games by name and gives some details about each one. Sometimes, games are delisted because of expired licenses, business problems, or legal issues. Some popular titles—like Alan Wake, DuckTales: Remastered, and many Activision games—have been removed in the past. A few of these have later come back, but many stay gone for years. The site also tracks which games are about to be delisted soon, so collectors and fans can act fast if they want to buy them before they vanish. The website even helps people try to get their lost games back if possible. It offers support and news about the latest removals. There are links for donations and ways to help keep the site running, showing it’s a resource for people who care about game history. The article reminds us that digital games are not forever, and once taken down, they can be hard or impossible to find again. This makes some people worry about game preservation.

In the comments, some people say it’s sad to see so many games disappear. They point out that it’s not only small games—big, famous titles can also be removed. Others share their frustration that, even if you paid for a game, you might not be able to download it again if it gets delisted. Some worry this will get worse as more games move to digital-only sales. A few users mention that piracy or unofficial backups might become the only way to keep old games alive. Others think game companies should do more to give players access to their purchases, or at least warn people before a removal. Some suggest that Steam or publishers should have an official “vault” where delisted games can live, maybe without updates or support, but still be legal to own. A few comment that this problem also affects movies, music, and books—not just games. There are also people who thank the website for keeping track of all these changes, saying it helps preserve gaming history.

---

## Show HN: Aroma: Every TCP Proxy Is Detectable with RTT Fingerprinting

- 原文链接: [Show HN: Aroma: Every TCP Proxy Is Detectable with RTT Fingerprinting](https://github.com/Sakura-sx/Aroma)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46386878)

Aroma is a tool that shows you can detect any TCP proxy just by measuring round-trip time (RTT) differences. The creator uses Fastly’s edge servers and simple math to check if someone is using a TCP proxy, without looking at IP lists or databases.

The main idea is to watch two numbers: the minimum RTT and the smoothed RTT of your connection. If you’re using a normal connection, these numbers should be close. But if you’re using a proxy, especially far away, the numbers are different—proxies make some RTTs go up. Aroma gets this data from Fastly servers, which can read these RTT values from the Linux kernel. The tool then divides the minimum RTT by the smoothed RTT to get a score. If your score is low, the tool thinks you’re using a TCP proxy. For most people, the score is between 0.7 and 1. Lower than 0.3 means you’re likely on a proxy.

Aroma can even catch commercial services like Cloudflare WARP when it acts as a TCP proxy. However, it won’t always catch VPNs or proxies that work at other network layers. The code is just a proof of concept and not for production use yet. You can try it yourself and see if your connection is “allowed” (not a proxy) or blocked. The creator also explains that network timings are complex—there are many RTTs at different layers (TCP, HTTP, TLS). Proxies make these timings different, which is how Aroma detects them. The explanation uses examples, like someone in Australia using a US proxy, to show how RTTs change.

In the comment section, some people are impressed and find the idea clever. Others point out that this kind of fingerprinting is already used by big services to block bots or suspicious traffic. Some users worry this could make it even harder to use proxies for privacy or censorship circumvention. A few commenters discuss ways to defeat this detection, like using proxies that add artificial delay to hide RTT differences, but they note this would make connections slower. Others mention that timing attacks have been used in security for a long time, and Aroma is a neat demonstration. There are also comments about how the method might not work well with mobile or flaky connections, since their RTTs jump around naturally. Some people are glad the code is open and hope it leads to more research. Others debate if this kind of detection is really fair to users who need proxies for good reasons. Finally, a few developers share ideas for improving proxy tools to avoid being detected by Aroma in the future.

---

## High-performance C++ hash table using grouped SIMD metadata scanning

- 原文链接: [High-performance C++ hash table using grouped SIMD metadata scanning](https://github.com/Cranot/grouped-simd-hashtable)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46371120)

This project is about a new C++ hash table that uses grouped SIMD metadata scanning to get better speed for big tables. It claims to beat other top hash tables when the table has more than 500,000 items, mainly in lookup-heavy cases.

The key idea is to change how the hash table searches for a key. Normal probing jumps around the table (like h, h+1, h+4, h+9...), which makes using SIMD (fast CPU instructions that handle many data points at once) slow because memory access is random. In this new design, the table checks 16 slots in a row (a group), then jumps to another group that is farther away, using a quadratic pattern (h, h+16, h+64, etc.). This way, it can use SIMD to read all 16 slots at once, making lookups much faster. Each slot has a small "metadata" byte to quickly skip over impossible matches. For big tables, especially when looking up data more than inserting, this method is faster than the popular ankerl::unordered_dense hash table. The results show that for tables with 1 million or more elements, lookup hits are up to 1.69 times faster, and lookup misses are 1.21 times faster. However, inserting is a bit slower (about 0.72x the speed) compared to the baseline. The table is header-only, works with C++17, and needs SSE2 (most modern x86-64 CPUs have this). There is no support for deleting or resizing yet—once you create the table, its size is fixed.

In the comment section, some users are excited about the clever use of SIMD and mention similar ideas from Google's Swiss Tables. Others ask about the lack of deletion and resizing, saying these are important for real-world use. A few people worry about the fixed table size and SSE2-only support, since many servers use ARM CPUs now. Some users compare this to other open source hash tables and share their own experiences with performance tuning, like using memory prefetching or Robin Hood hashing. There is a discussion about the trade-off between insert and lookup speed; some think the slower insert is okay for read-heavy situations, while others say it could be a problem for mixed workloads. A few comments point out that real-world performance also depends on key size, memory layout, and the kinds of data used. Some users hope the author will add deletion and resizing soon, while others just like seeing new research in hash tables. Overall, people find the grouped probing trick interesting and think it is a good addition to the open source hash table world.

---

