# Hacker News 故事摘要 - 2026-01-23

## 今日概述

Today’s top Hacker News stories are about new AI tools that help with coding and real-world tasks, privacy concerns with Microsoft giving encryption keys to the FBI, and changes to Y Combinator’s website. There are also stories on network problems, strict coding rules in Chromium, and tips for better thinking with mental models. If you like AI, privacy, or tech news, today’s stories have something for you.

---

## Unrolling the Codex agent loop

- 原文链接: [Unrolling the Codex agent loop](https://openai.com/index/unrolling-the-codex-agent-loop/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46737630)

This article explains how the Codex CLI works, focusing on its “agent loop”—the main logic that lets Codex talk with users, run tools, and write code. Codex CLI is a tool from OpenAI that makes safe, reliable changes to software on your computer by connecting user input, AI models, and system tools.

The agent loop starts when you give Codex an instruction. Codex turns your input into a prompt, which is sent to the AI model (like GPT). The model takes in this prompt, turns it into numbers (called tokens), and creates a response one token at a time. Sometimes, the model gives a direct answer. Other times, it asks Codex to run a tool (for example, running a command in your terminal). If a tool is called, Codex runs it, adds the result to the prompt, and sends everything back to the model for another try. This cycle repeats until Codex gets an answer for you.

A single question and answer is called a “turn.” Each new turn adds more history to the prompt, including past messages and tool calls. But AI models can only handle a certain amount of tokens (the “context window”), so Codex needs to manage how much information is sent each time. To keep things efficient, Codex uses “prompt caching,” reusing parts of old prompts to avoid extra work. However, if you change tools, models, or settings during a conversation, this can break caching and slow things down.

Codex does not store past chat states on the server, to support privacy and “Zero Data Retention” (ZDR). This means every request includes all needed information, making things stateless but sometimes less efficient. To deal with long conversations, Codex can “compact” old messages, summarizing them to save space within the context window.

Codex builds prompts using different roles (like system, developer, user, assistant), and includes instructions, available tools, and user input. Codex can run locally, in the cloud, or with different APIs, and can use both built-in and custom tools. Managing prompt size, caching, and context is important for speed and accuracy.

In the Hacker News comments, some users are impressed by the technical depth and careful design of Codex, especially around prompt management and caching. Others point out the challenge of handling large prompts and context limits, wondering if there are better ways to keep conversations efficient. A few users worry about privacy and whether statelessness really protects user data, or if there are hidden trade-offs. Some developers like that Codex is open source and appreciate the detailed documentation and diagrams. Others are skeptical about using AI agents for complex coding work, saying that human review and control are still needed, especially when tools can change files or run commands. There are also questions about how Codex handles errors, unexpected tool outputs, or different project setups. A few users suggest improvements, like smarter compaction or more flexible prompt building, to make Codex faster and easier to use in real projects. Some wonder how Codex will compare to other AI coding assistants as the field grows. Overall, the discussion mixes excitement about Codex’s features with caution around safety, privacy, and practical use.

---

## Proof of Corn

- 原文链接: [Proof of Corn](https://proofofcorn.com/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46735511)

This article talks about a project called "Proof of Corn," where a team uses AI to manage the growing of real corn from seed to harvest. The main idea is to show that AI, like Claude Code, can make all the decisions needed to run a farm, even if it never touches the soil itself.

The project started after someone said AI can write code but cannot affect the real world. In response, the team set up a case study to see if AI could manage a farm, just like a human farm manager does. The AI collects data from sensors, weather forecasts, and satellites. It uses this data to decide when to plant, water, and harvest the corn. Human workers still do the actual physical work, but every choice comes from the AI.

The project's workflow is clear: the AI ("Claude Code") is the brain. It gets data from sensors and APIs. It tells people what to do through commands and logs every decision. The first test is in Texas, with Iowa and Argentina planned for later. The team is still looking for more land and has only spent about $13 so far. The whole process, code, and budget are public online.

The goal is not for AI to drive tractors, but to act as a smart manager. This AI manager can work all day, use more data than a human, and make quick decisions. The project is open-source, so anyone can see how the AI thinks, what it decides, and how money is spent.

In the Hacker News comments, some people are excited and find the project fun and clever. They like how the project is open and transparent. Others are more skeptical. They point out that the AI is only giving orders—it isn't doing anything physical itself. These people say that humans are still needed for real farm work, so the AI is more like a boss than a worker.

Some commenters say this is not new; farm management software has existed for years. They wonder if AI actually adds much value over existing tools. Others think the project could be useful for large farms in the future, where fast decisions are important. A few people ask about the risks: what happens if the AI makes a bad call? Who is responsible if the crop fails?

One person says that AI might soon be able to control robots, not just give orders to humans. Others discuss if AI managers might someday replace middle managers in other jobs, not just farming. Some are curious about the technical details and want to see the code and data logs. In general, the comments show a mix of excitement, doubt, and curiosity about using AI for real-world management.

---

## Gas Town's agent patterns, design bottlenecks, and vibecoding at scale

- 原文链接: [Gas Town's agent patterns, design bottlenecks, and vibecoding at scale](https://maggieappleton.com/gastown)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46734302)

This article talks about Gas Town, a new system where many AI agents work together to write code. The big idea is that when AI agents can code for us, the main problem is not writing the code, but planning and designing what needs to be built.

The author explains that Gas Town was made by Steve Yegge. It is a messy, fast-built tool, full of funny names for agents like "Mayor," "Polecat," and "Refinery." Each agent has a special job. For example, the Mayor talks to the user and assigns work, Polecats do the simple tasks, and the Refinery merges changes into the main code. These agents work in a hierarchy, so there is less confusion. Tasks are tracked as “beads” in a system similar to issue trackers. If an agent gets stuck, other agents nudge them to keep working. When many agents work in parallel, merge conflicts happen often, so the Refinery agent manages these. The article also mentions a better way to handle changes—using stacked diffs, which are small changes merged one after another.

A big challenge is cost. Gas Town burns through thousands of dollars in API fees each month. The author says that as these systems improve, costs should go down. Still, if they save enough developer time, they could be worth the price for some companies. Another key point is the debate about "vibecoding"—coding without ever looking at the code. Yegge never checks the code his agents write. The author thinks this is risky for most real projects and expects the debate over how close developers should be to their code to grow in the future.

The article also says that agent tools work better in some cases than others. For example, it’s easier to trust them with small, new projects than with big, old ones. It’s also safer when agents can test their own work and when the risks of mistakes are low. More experienced developers can use these tools better. The author thinks that while Gas Town itself may not be the future, its ideas—like agent roles, task tracking, and agent-managed merges—will show up in the next wave of dev tools.

In the comments, people have many opinions. Some say Gas Town is fun and creative, but too chaotic and hard to use for real work. Others like the idea of agents with special roles, but think the system is too complex right now. One top comment says the tool feels like "a stream of consciousness turned into code," and guesses that only a few ideas from Gas Town will be used in other, simpler tools. Another commenter points out that cost is a big problem—most teams can’t spend thousands per month just to try new ideas.

There is also a debate about whether developers should ever stop looking at code. Some believe it’s too risky, while others think it’s the future. People agree that agent coding works better for greenfield (new) projects than for brownfield (old, messy) ones. Many say that as agent tools get better, developers will move further away from direct code, but only if there are good safety checks. Some commenters enjoy the wild and humorous side of Gas Town, while others see it as a warning about moving too fast and forgetting good design. Overall, the discussion shows both excitement and worry about where agent coding is going next.

---

## New YC homepage

- 原文链接: [New YC homepage](https://www.ycombinator.com/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46735644)

Y Combinator has launched a new homepage to show off its history, top companies, and the value it brings to startup founders. The page highlights famous YC companies like OpenAI, Airbnb, Stripe, and Reddit, and shows how much these companies are worth today. It explains how YC works: startups get $500,000, move to San Francisco for three months, and work hard on their business before presenting to investors. YC says its help does not stop after Demo Day; the alumni network and YC partners keep supporting founders for years. Many big names in tech, like Sam Altman (OpenAI) and Brian Chesky (Airbnb), started in YC. The site also lists current partners, most of whom were YC founders themselves, and shares stories of how YC made a difference for them. There are easy links to apply, find co-founders, get startup advice, and read famous essays by Paul Graham. YC repeats that you can apply even if you have no product yet. The site also has startup news, blog posts, podcasts, and resources for both founders and investors.

In the comments, some people like the new design, saying it is modern and easy to use. Others think it is too flashy and not as simple as the old YC site. A few users miss the old, plain style, which felt more “hacker friendly” and less like a big company. Some say the site now feels more like a sales pitch than a resource for new founders. Others point out that showing off billion-dollar companies is inspiring for some, but can feel out of reach for beginners. There are mixed feelings about focusing so much on huge valuations instead of the process or early struggles. Some users are happy to see more stories from real founders and like the links to startup advice. A few people wish there were more details on how to apply or what YC looks for. Some commenters wonder if YC is changing or if it is just “branding” for new times. Others say the new site makes it easier to find resources, jobs, and events. There is also talk about the value of the YC network and how it helps beyond just money. Finally, some users point out the site works well on phones, while others report small bugs or missing pages.

---

## Microsoft gave FBI set of BitLocker encryption keys to unlock suspects' laptops

- 原文链接: [Microsoft gave FBI set of BitLocker encryption keys to unlock suspects' laptops](https://techcrunch.com/2026/01/23/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops-reports/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46735545)

This story is about Microsoft giving the FBI a set of BitLocker encryption keys to help unlock suspects’ laptops. BitLocker is a tool from Microsoft that protects files on a computer by encrypting them.

The article explains that police wanted to read the data on some laptops, but BitLocker made this hard. To solve this, Microsoft provided the FBI with special keys that unlock the encrypted files. These keys let the FBI get past the BitLocker protection. The article says that Microsoft has access to these keys if a laptop is set up in a certain way, like when users back up their recovery keys to the cloud with a Microsoft account. If the government asks, Microsoft can look up and share these keys. This is possible because many people do not keep their keys private or offline. The article also mentions that this is not a secret, but many users do not realize it. The story raises questions about how safe encrypted data really is if a big company controls the keys. It also talks about the balance between user privacy and law enforcement needs.

In the comments, some people are worried that Microsoft can give away these keys so easily. They say users should have full control of their encryption keys, not the company. Others point out that many people do not understand how BitLocker works, and they trust default settings without thinking. Some think it is important for police to have access in serious cases, but others fear this power can be abused. There is also debate about whether using BitLocker is safe, or if people should use other tools where only the user has the key. A few say this is not new, and anyone using cloud backup should expect this risk. Some commenters suggest using open-source encryption that does not depend on companies. Others warn that if companies can give keys to the government, hackers might also find ways to get them. Finally, some people say it is up to users to learn about their devices and make smart choices about security.

---

## Banned C++ Features in Chromium

- 原文链接: [Banned C++ Features in Chromium](https://chromium.googlesource.com/chromium/src/+/main/styleguide/c++/c++-features.md)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46737447)

The article explains which C++ features are allowed or banned in the Chromium codebase. Chromium does not quickly adopt new C++ features; instead, every new feature must be reviewed before being used.

Some C++ features are banned for reasons like security, build tool limits, or because Chromium has its own version. For example, exceptions, `std::shared_ptr`, `std::function`, and most of the thread library are not allowed. This is because Chromium has its own memory and threading systems (like `base::WeakPtr` and `base::Thread`). Some standard libraries, like `<chrono>`, `<regex>`, and `<filesystem>`, are banned since Chromium uses its own libraries for these. Features that depend heavily on templates or have unclear behavior across platforms are also avoided.

There are also bans on some C++ language features. For example, `inline namespaces`, the `long long` type, and user-defined literals are not allowed. Some new C++17 and C++20 features, like `char8_t` and modules, are also banned due to lack of support or possible problems. Chromium also bans many features in the Abseil library if they overlap with Chromium’s own tools, or if they could cause bugs or extra code size.

Some newer features are allowed, like C++20 concepts, default comparisons, and range algorithms. Chromium likes features that help with safety and clarity but avoids ones that could cause bugs, security holes, or confusion.

In the Hacker News comments, many developers agree with the bans, especially on features like exceptions and `std::shared_ptr`, because Chromium needs high performance and stable memory use. Some comment that banning features like `<regex>` and `<thread>` makes sense since Chromium has custom, cross-platform tools. Others think banning too many features can hurt code readability and make it harder for new people to join the project. A few wish Chromium would allow more standard features as compilers improve. Some find it strange that features like `std::byte` and `char8_t` are banned, saying these are helpful for type safety. Others point out that keeping a big project like Chromium stable means avoiding features that don’t work well everywhere. Overall, most agree that consistency and security are more important than using every new C++ feature.

---

## Route leak incident on January 22, 2026

- 原文链接: [Route leak incident on January 22, 2026](https://blog.cloudflare.com/route-leak-incident-january-22-2026/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46735489)

Cloudflare had a route leak on January 22, 2026, because of a mistake in their automated router settings in Miami. This problem made some Internet traffic, especially IPv6, go through Cloudflare’s Miami data center when it should not have, causing network issues for their customers and others.

The route leak lasted about 25 minutes. During that time, the Miami router was overloaded, which led to slow traffic, dropped connections, and higher delays for some users. The root cause was a change made to routing policies—Cloudflare wanted to stop sending some traffic to Bogotá, Colombia, but the change made their router too open. This allowed many internal network routes to be shared outside Cloudflare’s network by mistake.

In technical terms, the router shared routes from its peers with its providers, breaking a rule called “valley-free routing.” This is not allowed, because networks should only pass some routes to their customers, not back up to other providers or peers. The mistake happened when a configuration filter was removed; after that, the router started to accept and send many routes it should have kept private.

Cloudflare’s team noticed the issue quickly. They stopped the automation, fixed the bad setting by hand, and checked that things were safe before turning it back on. They also looked at data logs to see what routes were leaked and how traffic moved.

Because of this leak, extra traffic came into the Miami data center, causing congestion and some data loss. Also, Cloudflare’s firewalls blocked a lot of the unexpected traffic, dropping about 12Gbps at the peak. This affected not only Cloudflare customers but also some networks whose traffic was wrongly sent to Cloudflare.

To stop this from happening again, Cloudflare is patching their automation tools, adding more checks to reject bad routes, improving their testing, and working on better ways to catch mistakes early. They plan to use new Internet standards, like RFC9234 and RPKI ASPA, to prevent leaks, even if a human or software error happens.

In the comments, some people praised Cloudflare for being open and detailed in their report. Others pointed out that BGP, the protocol for routing Internet traffic, is very old and fragile, and maybe it should be replaced or improved. A few users said these kinds of leaks happen too often and show why the Internet needs better safety tools. Some network engineers discussed how easy it is to make a small mistake that leads to a big problem, especially with complex automation. There was debate about whether automation helps or makes things worse—one side said automation speeds up fixes, while others said it can spread errors faster. A couple of commenters asked for more ways to test changes in a safe environment before going live. Some users shared their own stories of similar mistakes and stressed the need for clear rules and more training. In summary, the community agreed that route leaks are a known risk, but appreciated Cloudflare’s fast response and willingness to share details.

---

## TrueVault (YC W14) is hiring a Growth Lead to test different growth channels

- 原文链接: [TrueVault (YC W14) is hiring a Growth Lead to test different growth channels](https://www.ycombinator.com/companies/truevault/jobs/njvSGDj-growth-lead)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46737870)

TrueVault is looking to hire a Growth Lead who will help find and test the best ways for the company to grow. The company makes software that helps businesses follow complex privacy laws easily, without needing a lot of lawyers or complicated systems.

The Growth Lead will be the first person in this role and will not use a set playbook. Instead, they must create new ways to find customers, like testing paid ads, partnerships, SEO, and more. The job is about running many experiments, measuring results, and seeing what works best. The Growth Lead will work directly with the CEO and have a big impact on the company’s future. They will focus first on getting new customers and, later, help improve all parts of the customer journey, such as keeping customers happy and getting referrals.

The company wants someone who has at least five years of experience finding and growing customer channels for B2B SaaS companies. This person should be good at trying new things, making sense of messy data, and turning ideas into action. They should also be able to handle change, own their work, and be creative with growth strategies. Success will be measured by the number of experiments they run, how many new customer channels they create, and how much they help the company grow each quarter.

The role is remote but only for people in the United States. The pay is $165,000–$185,000, plus a bonus and equity. Benefits include full health, dental, and vision insurance, unlimited paid time off, a 401(k), and more.

In the comments, some people liked that the role is hands-on and gives a lot of freedom. Others thought the wide list of tasks might be too much for just one person, especially in a small company. A few said the pay is fair for the level of experience required, while others wanted more details about the company’s stage and customer base. Some commenters wondered how much support the Growth Lead would really get and if the company would listen to their ideas. Others shared that being the first growth hire can be exciting, but also risky if the product or market isn’t ready. A few asked about the company’s current growth and how much room there is for creative experiments. There was also discussion about the value of privacy tools and if businesses are willing to pay for this kind of product. Some people liked that TrueVault focuses on privacy, while others questioned if the market is big enough. Finally, a few mentioned that the job could be a good fit for someone who likes building things from scratch and wants a challenge.

---

## Mental Models (2018)

- 原文链接: [Mental Models (2018)](https://fs.blog/mental-models/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46737957)

This article is about “mental models”—simple ways to understand how the world works and make better decisions. It explains that mental models are like maps: they help us focus on what matters and ignore the rest.

The article lists many mental models from fields like science, economics, and art. For example, “The map is not the territory” means our ideas are not the same as real life. “Circle of competence” teaches us to know what we’re good at and when to ask for help. “First principles thinking” means breaking problems down to basics, not just copying what others do. “Second-order thinking” asks us to think about what happens next, not just the first result. “Probabilistic thinking” helps us deal with uncertainty by guessing chances based on facts. “Inversion” tells us to look for ways to fail so we can avoid them. “Occam’s Razor” suggests the simplest answer is often best. Other models like “reciprocity,” “inertia,” “leverage,” “feedback loops,” and “margin of safety” show up in science, business, and daily life. The article also covers ideas from math like randomness and regression to the mean, and from economics like scarcity, trade-offs, and specialization. In art, models like “genre,” “contrast,” and “framing” help us understand how people create and see things. The article says using many models from different areas makes us smarter and helps us spot mistakes or hidden risks.

Commenters on Hacker News had many thoughts about these mental models. Some loved the article, saying that collecting models from many fields is very useful for solving tough problems. Others warned that mental models are tools, not rules—they can be misused or over-simplify complex things. Some readers said it’s easy to fall in love with one model and use it everywhere, which can cause mistakes. A few pointed out that some models, like Occam’s Razor or Hanlon’s Razor, can be wrong if used in the wrong situation. People liked examples from real life, such as using “feedback loops” in product design or “margin of safety” in investing. Some said the long list can feel overwhelming and that it’s better to learn a few models well than to try to remember them all. Others added that models are shaped by culture and experience, so what works for one person may not work for another. There was debate about which models are most important; some picked “circle of competence,” others liked “probabilistic thinking.” A few commenters wished for more focus on the limits of each model. Some shared their own models or books, like Charlie Munger’s talks or “Thinking, Fast and Slow.” Many agreed that the most important thing is to stay open-minded, keep learning, and use the right model for each problem.

---

## Vargai/SDK – JSX for AI video, declarative programming language for Claude Code

- 原文链接: [Vargai/SDK – JSX for AI video, declarative programming language for Claude Code](https://varg.ai/sdk)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46724675)

This article introduces the Varg SDK, a tool that lets developers use JSX-like code to create videos with AI. The SDK is made for AI agents and helps users describe what they want in a video, not how to make it.

With Varg, you write code using components like <Clip>, <Image>, <Video>, and <Speech>. The SDK turns your code into rendered videos by calling different AI models for images, video, voice, and music. It works with providers like fal.ai, ElevenLabs, OpenAI’s Sora, Replicate, and Higgsfield. You only need API keys for the services you use.

The system is declarative, so you focus on what the video should look like. It’s optimized for high volumes, using caching: if you render the same prompt again, it uses the cache and is very fast. The API is flexible—you can combine different AI models and build complex video pipelines. There are 16 core components to mix and match, from basic images to animation, overlays, subtitles, and talking heads.

The SDK uses a custom JSX runtime that looks like React but isn’t React. It runs best on Bun, but Node.js also works. Rendering happens server-side and needs FFmpeg, so it doesn’t run in the browser. The code examples show how to build a video in steps—creating images with prompts, editing backgrounds, changing outfits, and turning images into video clips.

Pricing for the SDK is free (Apache 2.0 license), but you pay the AI providers for generation. Caching helps cut those costs by skipping repeated API calls. Varg is different from tools like Remotion, which are for motion graphics or precise animation; Varg focuses on AI-generated content, ads, and talking head videos.

Now let’s look at the top Hacker News comments. Some commenters are excited about making AI video more accessible and see the declarative approach as a big improvement over traditional scripting. Others like that the API is easy for both humans and AI agents to use, making it a good fit for automation.

Several users discuss the technical choices, like using Bun for speed and caching, and some ask about support for other runtimes or cloud platforms. There are questions about costs when generating at scale, with a few people noting that AI video can get expensive, but caching should help.

A few developers compare Varg to Remotion and other video tools. Some say Varg’s integration with AI models is unique, but others point out that it may not suit every use case—especially where detailed control over animation is needed.

There are concerns about vendor lock-in, since you rely on third-party AI providers, and some ask whether you can easily export or move to other platforms later. One person notes that the “JSX but not React” approach is clever but could confuse beginners. Another wonders about security and privacy when sending data to external APIs.

Overall, the community is curious and sees potential, but they want more real-world examples and benchmarks. Some suggest trying it for marketing videos or quick prototypes, while others want to see how well it performs in production.

---

