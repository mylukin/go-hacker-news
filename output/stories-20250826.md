# Hacker News 故事摘要 - 2025-08-26

## 今日概述

Today’s top Hacker News stories focus on new AI tools for browsers and images, open source projects for programmers, and science news about research mistakes and drug safety. There are also stories on privacy, open web standards, and questions about trust in medical guidelines. Many discussions are about how new tools can help but also bring risks, and how to balance progress with safety and honesty.

---

## Claude for Chrome

- 原文链接: [Claude for Chrome](https://www.anthropic.com/news/claude-for-chrome)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45030760)

Anthropic is testing a new Chrome extension that lets their AI, Claude, do things directly in your browser, like clicking buttons or filling out forms. They say this could make Claude much more useful for daily work, but it also brings new safety risks.

The article explains that browser-based AIs can help with tasks like managing your calendar, scheduling meetings, answering emails, and testing websites. But, if not careful, these AIs can be tricked by "prompt injection" attacks. For example, someone could hide malicious instructions in a website or email, making the AI do something harmful—like deleting emails or stealing data—without the user knowing. Anthropic tested these risks and found some real problems: in one test, Claude deleted emails after reading a fake message that claimed to be from the security team.

To fix this, they added new safety steps. First, users can choose which sites Claude can access. Claude also double-checks with users before doing anything risky, like buying something or sharing private information. They block Claude from high-risk websites, and they use special tools to spot suspicious instructions or strange data requests. After adding these changes, the number of successful attacks dropped by more than half.

Anthropic is now starting with a small group of trusted users to test Claude for Chrome in real situations. They want to see how Claude handles real-world browsing, because testing in the lab isn't enough. The goal is to find new types of attacks and improve safety before letting everyone use the extension. They ask testers to avoid using Claude for sensitive sites, like banking or health.

In the Hacker News comments, people had mixed feelings. Some are excited about the idea of an AI that can use the browser for you, saying it could save a lot of time and effort. Others worry about giving an AI so much control, especially with the risk of attacks or mistakes. Several users point out that prompt injection is a big, unsolved problem—not just for Claude, but for all browser agents. Some think Anthropic is moving too fast, while others like the cautious, limited rollout.

A few commenters raise privacy concerns, asking if their browsing data will be safe or if Claude might leak private information. Others talk about possible uses for the technology, like automating boring work or helping people with disabilities. Some mention that even with safety features, users need to pay attention and not trust the AI with sensitive sites or data. Lastly, there are questions about whether these safeguards will be enough as attacks get smarter, and whether browser-based AI will be safe for everyone in the long run.

---

## GNU Artanis – A fast web application framework for Scheme

- 原文链接: [GNU Artanis – A fast web application framework for Scheme](https://artanis.dev/index.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45031673)

GNU Artanis is a web framework made for the Scheme programming language, aiming to help people build web apps quickly and easily. The article explains that Artanis is fast, simple to use, and designed for professional web development, using the GNU Guile version of Scheme.

The article gives a short example to show how simple it is to start an Artanis server and add a basic route that replies with "hello world." Artanis supports many features, like JSON, CSV, XML, WebSockets, and several databases (MySQL, SQLite, PostgreSQL). It is open source and uses the GPLv3+ and LGPLv3+ licenses. The framework focuses on being lightweight, so it is good for beginners, but it also handles advanced things like async servers and caching. Artanis also has good support for handling errors, static files, and HTML templates. The project started in 2013 at a GNU Guile event, then became an official GNU project with support from Richard Stallman (RMS). It has won awards and is now maintained by a team, with the main author being Mu Lei. The article invites people to join the project, contribute, or just use it for their own web apps.

In the Hacker News comments, some users were surprised to see a modern web framework built with Scheme, and a few were excited because they love Lisp languages. Some users liked how simple the code example was and thought it looked easy to start with. One commenter said it is nice to see more tools for less common languages, but they wondered if there are enough users to build a big community. Others discussed technical things, like the use of delimited continuations for async code, which is interesting but might be hard for new users. A few people asked about the performance and how it compares to popular frameworks in Python or Node.js. Some were happy that Artanis is open source and fits with GNU ideas. A few worried that using Scheme might make it harder to find help or hire developers. Others thought it’s great for learning about web servers and functional programming, even if you don’t use it for work. Some users wanted better documentation and more real-world examples to help them start. Overall, people respected the work but saw it as a niche tool for now.

---

## Gemini 2.5 Flash Image

- 原文链接: [Gemini 2.5 Flash Image](https://developers.googleblog.com/en/introducing-gemini-2-5-flash-image/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45026719)

Google has released a new tool called Gemini 2.5 Flash Image, which helps people make and edit images with AI. This tool lets you combine different pictures, keep the same character looking the same in different images, and make changes with just a simple text instruction.

The new model is faster and cheaper than before, but now it also makes better images and gives users more ways to control how the images look. Developers can use it through an API, Google AI Studio, or Vertex AI. The price is about four cents per image. Google made it easy to test and build apps with this tool. For example, you can make an app where users upload a photo and add filters, all using simple prompts.

A big new feature is "character consistency." This means if you want the same person or product in many pictures, it will look the same each time, even in different places or poses. This is great for things like ads, games, or brand images. The tool also follows visual templates, so you can make things like ID badges or product cards that all match.

You can edit images by just telling the tool what you want. For example, you can blur the background, remove a stain, or change someone’s pose with a short message. The model also knows a lot about the real world, so it can understand drawings, answer real questions, and follow tricky instructions.

With "multi-image fusion," you can mix two or more pictures together to create new scenes. For example, you can place a sofa into a photo of a living room, and the AI will blend them together.

All images made by Gemini 2.5 Flash Image get a hidden digital mark, so people know they were made by AI. Google shows a Python code example for using the model, and says more improvements are coming, like better long-form text in images and even more reliable character looks.

In the Hacker News comments, some people are excited about how easy this makes image creation for developers and businesses. They like the simple API and the low price. Others are worried about how AI images might be used for fake news or deepfakes, even with the watermark. Some users are interested in how well the tool keeps character consistency, since that has been a problem for AI art before. A few people talk about the quality compared to other tools, like Midjourney or DALL-E, and want to see side-by-side examples.

Some developers mention the licensing and if they can use images commercially. Others ask about privacy, especially if you upload private photos for editing. There are also questions about how the watermark works, and if it can be removed. People are also curious about the speed for big jobs, and if the cost will stay this low. Finally, some users think this could change how people make images for the web, marketing, or even games.

---

## What happens when ambassadors are summoned by the host country?

- 原文链接: [What happens when ambassadors are summoned by the host country?](https://politics.stackexchange.com/questions/93401/what-happens-when-ambassadors-are-summoned-by-the-foreign-ministry-of-their-host)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45031496)

When an ambassador is summoned by the host country's foreign ministry, it usually means the host country is unhappy about something the ambassador’s country has done or said. The article explains that this is a common tool in international relations and is often more about sending a message than starting a real argument.

The process starts with a formal, polite note asking the ambassador to come for a meeting. Sometimes, the host country tells the press about the meeting to show they are taking action. The meeting itself can be very formal or more relaxed, but both sides know the rules and follow diplomatic manners. In some cases, the host country tries to make the ambassador feel the seriousness—like making them wait in a fancy room, or even having a “meeting without coffee” to show displeasure. The ambassador is usually briefed by their own government beforehand on what to say and what not to say during the meeting. The main goal is for the host country to explain their problem, and for the ambassador to listen and then share their own country’s view. Sometimes, the ambassador is not expected to answer every question right away—they can say they need to check with their government. If things get really bad, the host country could declare the ambassador “persona non grata,” which means they must leave the country. However, most meetings end without big actions and are more about making positions clear.

Commenters point out that the details can change depending on the country, the people involved, and the reason for the meeting. Some say the whole thing is like a well-rehearsed play, where everyone knows their part. Others note that what happens in the room is often kept secret, and the real purpose is to show the public or the media that something is being done. One comment highlights that the strength of the message can be shown by who the ambassador meets—if it’s a high official, the problem is probably serious. There’s also discussion about how both sides use the meeting to explain their positions in detail, sometimes more than they would in public. A few people mention that refusing a summons is rare and can harm relations, but it does happen. Some argue that the process is mostly about showing strong feelings, but not usually about solving the problem right away. Others debate whether these meetings are about real communication or just for show, but most agree that it’s an important part of diplomacy.

---

## Why do people keep writing about the imaginary compound Cr2Gr2Te6?

- 原文链接: [Why do people keep writing about the imaginary compound Cr2Gr2Te6?](https://www.righto.com/2025/08/Cr2Ge2Te6-not-Cr2Gr2Te6.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45030144)

A recent science paper talked about a material called Cr2Gr2Te6, but there is no such thing—it's a typo that should be Cr2Ge2Te6, which is chromium germanium telluride. The article shows how this small error has been copied into many other research papers and even books, which makes it look real when it is not. The mistake is easy to make because "Ge" (for germanium) and "Gr" are close on the keyboard, and "Cr2Gr2Te6" looks similar to the real formula. The author checked the original research and confirmed that the real compound is Cr2Ge2Te6, not Cr2Gr2Te6. Many later papers refer to the wrong formula, sometimes side by side with real discoveries about 2D magnetic materials. The blog warns that these kinds of mistakes can spread even more quickly with AI tools, which might copy text from other sources without checking facts. The author hopes that by pointing out the mistake, people will stop repeating it. A photo of the real Cr2Ge2Te6 crystal is shown, and the article lists several references with both the correct and incorrect names.

In the comments, some people blame the academic system, saying that researchers are pressured to publish a lot, which leads to mistakes. Others think that no one is paid to proofread, so errors slip through. One person worries that AI will make these problems worse by spreading mistakes even faster. Someone else says that ChatGPT did not invent details about the fake compound, but only because it had already seen the correction. There is also a debate about whether Cr2Ge2Te6 is really a "compound" or if it is actually an alloy, with some saying the difference matters and others trusting what sellers and original researchers say. Another comment gives an example of a different nonsense phrase that spread in scientific papers because of bad training data for AI. Some people share links to more information about germanium or point out that corruption and laziness might play a part too. Overall, many agree that typos and errors can easily become "facts" if repeated often, and that both humans and AI need to be careful with details in science.

---

## Rv, a new kind of Ruby management tool

- 原文链接: [Rv, a new kind of Ruby management tool](https://andre.arko.net/2025/08/25/rv-a-new-kind-of-ruby-management-tool/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45023730)

This article talks about a new tool called “rv” for Ruby developers. The writer wanted a better way to manage Ruby versions and gem dependencies, inspired by the Python tool “uv”.

Most Ruby tools today need you to install Ruby, then gems, and then more tools, one step after another. “rv” aims to do all this in one step. It can install pre-built versions of Ruby very quickly, so you don’t have to wait for source code to compile. It also lets you run any Ruby script or tool, even if it needs a different Ruby version than your main project.

“rv” is written in Rust, which helps it run fast. It combines features from tools like “cargo”, “npm”, and “uv”. For example, “rv tool run” lets you run any gem as a command-line tool, and automatically handles the right Ruby version and gem dependencies, each in their own space. If you want to install a CLI tool like “gist”, you just use “rv tool install gist”, and it won’t mess with your main app or other gems.

Another feature is script support: you can put the Ruby version and gem info in your script, and “rv” will run it with everything set up correctly. The main idea is to have one tool that manages both Ruby versions and dependencies at once. This saves time and avoids mistakes when switching projects or running scripts.

The team behind “rv” includes people with a lot of Ruby experience. They already have a working version that can install Ruby 3.4.x in one second on macOS and Ubuntu. You can try it now or read about their future plans.

In the comments, many people are excited for a modern Ruby tool inspired by “uv” and “cargo”. Some say managing Ruby versions and gems has always been slow and confusing, so they like the idea of one tool that does both. Others wonder if “rv” will work well with older projects, or on systems besides macOS and Ubuntu.

A few users point out that “uv” helped the Python world a lot, and hope “rv” will do the same for Ruby. Some are worried about security and want to know how “rv” will handle updates and trust issues. There’s also talk about whether “rv” will support Windows, and if it can really replace “rvm”, “rbenv”, and “bundler” all together.

Some commenters ask for more details about how fast “rv” is compared to old tools. Others suggest ideas for the tool’s name, or ways to help the team. A few people mention they tried the alpha version and found it easy to use. Overall, the community seems hopeful and curious to see if “rv” can make Ruby development faster and simpler.

---

## IETF Internet-Draft: AI Content Disclosure Header

- 原文链接: [IETF Internet-Draft: AI Content Disclosure Header](https://www.ietf.org/archive/id/draft-abaris-aicdh-00.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45032360)

This article is about a proposed standard for a new HTTP header called “AI-Disclosure,” which lets websites say if and how AI was used to make their content. The main goal is to help web browsers, bots, and other tools quickly know if content was made or changed by AI, using a simple and machine-readable signal.

The header works like this: it uses a dictionary format with keys such as “mode” (how much AI was used), “model” (what AI model was used), “provider” (who made the AI), “reviewed-by” (who checked the content), and “date” (when it was generated). For example, a site could say the content was “ai-originated” but reviewed by humans, or only “ai-modified” (like grammar checking), or “machine-generated” (made mostly by AI with little or no human help). The header is optional—if it’s missing, you can’t say if AI was used or not. It is not a security feature; it’s just for information, and anyone could add or change it unless extra steps are taken (like using HTTPS).

The article says this header is not as strong as cryptographic systems like C2PA, which can prove where content comes from and if it’s been changed. Instead, “AI-Disclosure” is meant to be easy for most websites and tools to use, giving a quick advisory note about AI use. Sites that need stronger proof about content origin should use C2PA or similar systems. The document also asks the IANA to add this new header to the official HTTP field name registry.

In the comments, some people think this is a good idea because it increases transparency and helps users and search engines trust what they see. Others say it is too easy to lie—bad actors could just leave the header out or fake the data, so it won’t stop misinformation. A few developers like that it is simple and doesn’t slow down web servers, but some worry about “banner blindness,” where people ignore warnings if they become too common. One person points out that similar efforts in the past have failed because not enough sites use them, and there is no legal way to force compliance. Another commenter says this could help with future laws about AI labeling, but only if browsers and big sites support it. Some people suggest that cryptographic signing, like in C2PA, is the only real solution for trust, and this header could at best be a first step. A few are concerned about privacy and tracking, since headers can sometimes be used to gather extra info about users or sites. Finally, some joke that AI-generated content will just start labeling itself as “human” to get around the rules.

---

## LiteLLM (YC W23) is hiring a back end engineer

- 原文链接: [LiteLLM (YC W23) is hiring a back end engineer](https://www.ycombinator.com/companies/litellm/jobs/6uvoBp3-founding-backend-engineer)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45032276)

LiteLLM is a new startup that builds tools to help developers use many different large language models (LLMs) through one simple API, just like OpenAI’s. The company is hiring a founding backend engineer to help make their product better and faster.

LiteLLM is open source and trusted by big names like NASA and Adobe. It lets you call more than 100 different LLM APIs—like OpenAI, Anthropic, Bedrock, and Cohere—using the same code style. This saves developers time and makes switching between models easy. The backend uses Python and FastAPI, and the frontend uses JavaScript and TypeScript. The company also uses tools like Redis, Postgres, and cloud storage.

The job involves making sure all these different LLMs work well together and fixing any issues that come up with different providers. You might help move parts of the system to use faster networking libraries, add new features like cost tracking, and help the system handle more users and logs as the product grows. The team is small (just two founders now), so you will work closely with them and have a big impact.

LiteLLM has raised $1.6 million from investors including Y Combinator, and they offer good pay plus some company ownership. They want people who like open source, want to talk to users, and are ready to take responsibility. The interview is just one day and includes coding, architecture, and a chat with the founders.

In the Hacker News comments, some people like how LiteLLM helps companies avoid being locked into just one LLM provider. Others mention the value of open-source tools in AI, saying they make it easier for everyone to build and try new things. A few users wonder about the technical challenges, such as keeping the API compatible with so many providers and making sure features and bugs are handled for each one. Some developers are interested in the company’s growth, since many big names already use it. Others discuss the pay and equity, saying it looks fair for a startup in San Francisco. There are also questions about remote work and if the company will support people outside the US. Finally, some people share thoughts on the future of LLMs—will most companies want to use many models, or will one provider win? Most agree that tools like LiteLLM make experimenting much easier right now.

---

## One universal antiviral to rule them all?

- 原文链接: [One universal antiviral to rule them all?](https://www.cuimc.columbia.edu/news/one-universal-antiviral-rule-them-all)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45026792)

This article is about new research into a possible “universal antiviral” drug, inspired by people with a rare immune condition. Scientists found that some people with a mutation in the ISG15 gene never seem to get sick from viruses, even though their bodies show signs of fighting many different infections.

The mutation causes a mild, steady inflammation in their bodies. At first, doctors thought this made them more likely to get some bacterial infections. But then they noticed these people did not get sick from viruses like flu or chickenpox. Their immune systems seemed to stop viruses before they could cause any symptoms.

The lead scientist, Dusan Bogunovic, wondered if this effect could be copied in other people. His team created a new therapy that can turn on the same antiviral state for a short time. The therapy uses lipid nanoparticles, like some COVID vaccines, to deliver mRNA for ten special proteins. These ten proteins create a strong antiviral response, but only for a few days and with less inflammation than the rare condition causes.

In tests on mice and hamsters, the therapy protected them from viruses like flu and COVID-19. In lab tests with cells, no virus could get past it. The hope is that this could become a new tool to protect people during a pandemic, even before a vaccine is ready. The therapy might be given to first responders or people at high risk.

However, there are still problems to solve. Delivering enough of the drug to the right place in the body is hard. The protection only lasts about three to four days. The team also needs to make sure it is safe for people. Still, this research shows that studying rare diseases can lead to new ideas for everyone.

In the comments, many people are excited about the idea of a universal antiviral. Some say this could help a lot in the next pandemic, especially for people who cannot get vaccines. Others worry about the risks of turning on inflammation, even for a short time, because it could cause side effects. A few users point out that viruses often find ways to adapt and escape treatments, so nothing may be truly “universal” for long. Some discuss the technical challenge of delivering mRNA drugs deep into the lungs. Others think the focus on rare diseases is smart, because it can teach us new things about the immune system. A few commenters ask about possible patent issues and drug pricing, and if this would be available worldwide. Some are hopeful but say animal results do not always mean it will work in humans. There is also some talk about how this could change the way we handle outbreaks, letting us buy time while vaccines are made. Finally, others say this shows how basic curiosity-driven research can lead to big medical advances.

---

## Undisclosed financial conflicts of interest in DSM-5 (2024)

- 原文链接: [Undisclosed financial conflicts of interest in DSM-5 (2024)](https://www.bmj.com/content/384/bmj-2023-076902)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45029241)

This article looks at financial conflicts of interest among the people who helped write the DSM-5-TR, the big book doctors use to diagnose mental illnesses. The study found that around 60% of the US-based doctors on the DSM-5-TR panels and task force got payments from drug or medical companies between 2016 and 2019.

The total money paid to these doctors was over $14 million. Most payments were for research, but many doctors also got money for things like travel, meals, consulting, and speaking at events. Some doctors received small gifts, but the study says even little things can affect decisions. The article points out that these payments are a problem because the DSM shapes what gets diagnosed and what drugs get prescribed, so industry money could have a big effect on mental health care.

This is not a new problem: earlier versions of the DSM also had many panel members with ties to drug companies. The American Psychiatric Association has tried to make the process more transparent, but still does not share details about how decisions are made or about the members’ financial ties. The authors argue that DSM panels should not include people with industry ties, or at least they should not let them make final decisions.

The study used a US government database that tracks payments to doctors, but could only check US doctors, not those from other countries or non-physicians. It also could not say for sure if the money influenced any specific changes in the DSM.

In the Hacker News comments, some users were not surprised by the findings. One person said that financial conflicts are common in many areas of medicine, not just psychiatry. Another commenter pointed out that sometimes it is hard to find experts who do not have any ties to industry, given how research is funded. A few people argued that it is unrealistic to expect doctors to never work with companies, but others felt the DSM is too important to allow any conflicts at all.

Some users thought the real problem was the lack of transparency in the decision process. Others suggested including more people from outside the medical industry in these panels, or making sure decisions are fully public. A few commenters worried that industry influence could lead to overdiagnosis and too many prescriptions. But some said that as long as ties are disclosed, readers can judge for themselves. Overall, most agreed that the DSM should be as fair and unbiased as possible because it affects so many people.

---

