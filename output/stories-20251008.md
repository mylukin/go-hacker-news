# Hacker News 故事摘要 - 2025-10-08

## 今日概述

Today's top Hacker News stories focus on security, memory, and CPU technology. There is a lesson on how easy it is to fall for phishing attacks, even for experts. Another story shows that memory gets slower as it gets bigger, which is important for programmers to know. The third story explains how special CPU instructions called SIMD can make programs much faster, but only if software uses them. If you care about security, computer speed, or how computers really work, these stories are worth a look.

---

## Kurt Got Got

- 原文链接: [Kurt Got Got](https://fly.io/blog/kurt-got-got/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45520615)

This article is about Fly.io’s Twitter account getting hacked because their CEO, Kurt, fell for a phishing email. The company quickly noticed the hack, and only their Twitter was affected—no serious damage was done.

The phishing attack worked because Kurt got an email that played on his worries about not understanding new internet jokes (memes). Their team had a young contractor posting “dank memes” on Twitter, which performed better than posts from the older staff. The attackers used a fake email that looked like it was about a strange meme post, and Kurt clicked the link and logged in, giving away the account credentials. The Fly.io team realized fast when they saw the account’s contact email had changed. They checked who accessed the account info in their password manager and took quick action, but the attacker had already set up new security and locked them out. It took about 15 hours for Twitter (now called X.com) to help them get the account back.

The article says you can’t stop phishing just by telling people not to click things—everyone makes mistakes sometimes. Instead, the real protection is using phishing-resistant authentication, like U2F, FIDO2, or Passkeys, which check if you are on the real website before sending your login info. Fly.io uses these tools for important systems, but their Twitter was a shared “legacy” account with basic password protection, so it was more vulnerable. The attackers tried to run a scam by posting about fake crypto giveaways, but it didn’t seem to fool anyone. In the end, Fly.io learned a lesson and now protects their Twitter with better security.

In the comments, many people laugh about how easy it is to fall for phishing, saying even tech experts can get caught. Some say blaming users is wrong—companies should build systems that don’t let a single click cause so much trouble. Others point out that social media platforms don’t have strong enough security features, especially for shared business accounts. Some comment that “x.com” is a confusing domain name and makes phishing easier. A few people share their own stories of being almost tricked by similar emails. Others discuss the challenge of teaching staff about new scams, since phishing emails keep getting smarter. Some praise Fly.io’s honest and funny write-up, while others remind everyone to always use two-factor authentication, even for things that seem unimportant. A few users debate whether password managers help or hurt in these cases. Finally, some say the real takeaway is that even small mistakes can happen to anyone, so having good backup and recovery plans is key.

---

## Memory access is O(N^[1/3])

- 原文链接: [Memory access is O(N^[1/3])](https://vitalik.eth.limo/general/2025/10/05/memory13.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45484999)

This article says that memory access is not as fast as we usually think. The author explains that, in theory and in practice, reading or writing to memory takes time that grows with the size of the memory, and not just a fixed amount.

First, the author uses a simple idea: imagine a processor in the center of a cube of memory. If you make the memory 8 times bigger, the farthest point is 2 times farther away, because memory grows in three dimensions (cube root). This means if you double the distance, you get eight times more memory, but also twice the delay to reach the farthest point. So, accessing memory is O(N^1/3), not O(1).

Next, the author looks at real computers. Different kinds of memory—like registers, cache, and RAM—have different sizes and access times. If you check how many nanoseconds it takes to access each, you see a pattern: the time it takes to access memory grows roughly with the cube root of the memory size.

The article also talks about why this matters. In cryptography, and some other fields, you often make big tables of precomputed values to speed up algorithms. If you think memory access is O(1), you might make these tables as big as possible. But, if access is O(N^1/3), it’s better to keep tables smaller so they fit in the faster cache. The author shares an example: a smaller table that fits in cache made his code much faster than a bigger table in RAM, even though the bigger one would need fewer steps.

The last part explains that this idea is important for special hardware, like ASICs and GPUs, too. If you can break a problem into small, local parts, memory access can be closer to O(1). But if not, you get the slower O(N^1/3) effect again.

In the comment section, some people agree with the author. They say that, in real life, memory is slow and cache is very important for performance. Others point out that the simple cube model is not always true, because real computers are more complex. Some mention that software engineers usually ignore these details, but hardware designers think about them all the time. One commenter says that the bandwidth (how much data you can move, not just the delay) often matters more than latency for big applications. A few people argue that the O(N^1/3) idea may be too simple, since memory chips and CPUs are not cubes and have different layouts. Others like the article because it makes people think more carefully about how algorithms really run on hardware. Someone asks if this idea will change how we write code, and a few say yes, especially for high-performance tasks. Another person says that, for most code, you don’t need to worry about this, but it’s very important for things like cryptography, databases, and scientific computing.

---

## Why we need SIMD

- 原文链接: [Why we need SIMD](https://parallelprogrammer.substack.com/p/why-we-need-simd-the-real-reason)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45487310)

This article talks about why SIMD (Single Instruction, Multiple Data) is important for computers today. It explains how CPUs have changed over time and why adding SIMD was a smart move for making computers faster.

At first, computers could only do one thing at a time, but engineers wanted to make them faster by doing many things at once. In the 1990s, CPUs got better at running several instructions at the same time, thanks to new designs called pipelining and superscalar architecture. But there was a limit to how much faster this could make programs run. To go even further, CPU makers like Intel added SIMD instructions, which let one instruction handle lots of data at once. For example, instead of adding two numbers, SIMD can add four or eight numbers at the same time. This works well for tasks like video and image processing, where you have to do the same thing to lots of data.

The first SIMD for x86 CPUs was called MMX, which could do eight small calculations at once. Later versions like SSE, AVX, and AVX512 made these instructions even wider, letting CPUs do even more work in one step. Adding SIMD was not expensive for chip makers, because they could use much of the hardware already inside the CPU.

However, using SIMD is not always easy. Software has to be written or updated to use these new instructions, and that can take a long time. Sometimes, the benefits of new hardware are not seen until developers update their programs. Video encoding and cryptography are areas where SIMD helps a lot, because the software gets updated quickly and everyone can see the improvements. But in other areas, like 3D graphics, hardware changed so fast that CPUs could not keep up, and special graphics chips took over.

The article says that, over time, CPU makers kept making SIMD better, hoping to get more software to use it. AVX512, for example, is very powerful, but only some programs use it right now. Some experts, like Daniel Lemire, show that SIMD can help with things like processing text very quickly. As more software begins to use these new instructions, regular people will see faster programs without even knowing why.

In the comment section, some people agree that SIMD is great for certain tasks, like video and math-heavy work. Others say it can be hard for most programmers to use SIMD because it makes code more complex and harder to maintain. A few mention that compilers do not always use SIMD well, so you have to write special code to get the best speed. Some people wish CPUs would get even better at using SIMD automatically, while others say that only big companies or experts have time to optimize for it. There are also comments pointing out that as CPUs get more cores, not every program can use SIMD and threads at the same time. Some users feel that hardware changes faster than software can keep up, and so only certain fields really benefit from new SIMD instructions. A few point out that making code portable between different CPUs is harder when you use SIMD. Finally, some are hopeful that, as SIMD becomes more common and easier to use, more programs will get faster for everyone.

---

