# Hacker News 故事摘要 - 2026-01-07

## 今日概述

Today’s top Hacker News stories cover new security changes in Tailscale and NPM, a look at science history and industry influence, US housing rules, and the new US food guidelines. Other stories feature a fun LaTeX package, a big data leak, AI leaderboard problems, a map of world shipping, and Rodney Brooks’ review of tech predictions. Main themes are security, trust in science, policy changes, and how new tools shape our world.

---

## Tailscale state file encryption no longer enabled by default

- 原文链接: [Tailscale state file encryption no longer enabled by default](https://tailscale.com/changelog)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46531925)

Tailscale has changed how it handles state file encryption in its latest release. Now, by default, the Tailscale client does not use encryption or hardware attestation keys for its state files on Linux and Windows.

Before, Tailscale could use your computer’s TPM (Trusted Platform Module) to encrypt the state file, which stores important information about your node. This made the state file safer if someone got access to your device. With this update, if Tailscale cannot load the hardware keys—maybe because the TPM was reset, replaced, or missing—the client will still start up instead of failing. This makes it easier for users who switch hardware or have issues with TPM devices. In Kubernetes setups, hardware attestation keys are no longer stored in Kubernetes Secrets, which helps users move Tailscale containers between different nodes without problems. Certificate renewals have also been changed, so they do not fail if account keys are recreated. The update affects both the regular Tailscale client and the Tailscale Kubernetes Operator. For those who want to keep using encrypted state files, it’s still possible, but now you have to enable it yourself.

People on Hacker News had mixed feelings about this change. Some are worried that turning off encryption by default could make Tailscale less secure, especially on shared or cloud servers. Others think this is a practical move because TPMs can be unreliable, and not all users care about this level of security. A few commenters note that users running Tailscale at home or in trusted environments may not need encrypted state files anyway. There are suggestions that Tailscale could make the option clearer or prompt users during setup, so they can choose what they want. Some think the real problem is that TPM hardware and support is still messy on Linux and in the cloud. Others point out that Tailscale’s main traffic is still encrypted, so the risk is mostly about device keys leaking, not the network data itself. A few users are glad that Tailscale is easier to use now, even if there’s a small security trade-off. Finally, some say this is a reminder to always check default settings and not assume software is always configured for maximum security.

---

## Sugar industry influenced researchers and blamed fat for CVD (2016)

- 原文链接: [Sugar industry influenced researchers and blamed fat for CVD (2016)](https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46526740)

The article explains that in the 1960s, the sugar industry worked with scientists to make people think fat was the main cause of heart disease, not sugar. The industry paid Harvard researchers to write a review that blamed fat and cholesterol while saying sugar was safe, and this review was published in a top journal without telling readers about the sugar industry’s involvement. 

The sugar industry knew that if people ate less fat, they would eat more sugar, which would help their business. When research started to show sugar could lead to heart problems, the industry acted quickly. They gave money to researchers, picked which studies to include in the review, and even helped write the drafts. The published review said only fat and cholesterol were problems for the heart, while ignoring signs that sugar might also be dangerous. This shaped not just public opinion but also how scientists studied heart disease risk factors for many years.

The researchers from UCSF who studied these old documents say this is a good example of why science reviews should be done by people with no ties to industry. They also say it’s important for scientists to tell everyone who gave them money for their research. Today, more studies are showing that added sugar is bad for the heart, but health policies are slow to change. The article ends by saying we need honest, open research to make good health decisions.

In the comments, many people say this story shows why we should always check who paid for scientific research. Some are angry that the sugar industry was able to hide its influence for such a long time. Others point out that this is not just a problem with sugar—many big industries use money to shape science and public health policy. A few commenters are surprised at how much trust was put in these old studies, while others say they already suspected something like this happened. Some people wonder if we are still making the same mistakes today with other foods or industries. There is also talk about how hard it is to change public health advice once a strong story takes hold, even if new science says something different. Overall, the comments show worry about bias in science and a desire for more honest research.

---

## NPM to implement staged publishing after turbulent shift off classic tokens

- 原文链接: [NPM to implement staged publishing after turbulent shift off classic tokens](https://socket.dev/blog/npm-to-implement-staged-publishing)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46530448)

NPM, the main place for JavaScript packages, had many attacks in 2025, leading to big changes in how people publish packages. The latest plan is “staged publishing,” which will slow down the process a bit so maintainers can review changes before making them public.

The big attacks, especially the “Shai-Hulud” campaign, showed that hackers can quickly use broken credentials and automated scripts to spread malware fast. NPM’s answer is to add a review step. When someone wants to publish a package, it will not go live right away. Instead, there will be a waiting period where the package owner must approve the release, and they must use multi-factor authentication (MFA) to do so. This pause is meant to help catch bad changes before anyone else gets them.

NPM is also working on better ways to publish packages safely using trusted systems like OIDC, and they are trying to support more Continuous Integration (CI) tools, not just GitHub Actions and GitLab. This is to give developers more control and make things safer, even when releases are automated.

This new plan comes right after NPM removed their old “classic tokens,” which were easy to steal and caused many security problems. Now, only short-lived tokens and more detailed “granular” tokens are allowed. This move made things safer but also much harder for people who manage lots of packages, especially since many tools and setups didn’t support the new way yet.

Some maintainers, like Adam Jones, said the changes were rushed. The tools to manage new tokens were released on the same day as the old tokens were turned off, leaving little time to switch over. Documentation was also confusing and out of date, causing many broken publishing pipelines. People had issues like having to log in again every two hours just to install or publish packages, though NPM later increased the session time to 12 hours to help.

Trusted publishing (using OIDC) is supposed to be the future, but right now it only works for a few platforms and cannot be used for first-time publishes of new packages. It also needs a lot of manual setup, and some open-source groups say it’s not safe enough yet for very important projects.

Some developers think NPM is focusing too much on credentials and not enough on watching for strange behavior after something is published. Nicholas Zakas, the creator of ESLint, said NPM should copy the credit card industry by looking for odd releases—like a package being published from a strange location or with unusual changes. This way, even if someone steals credentials, bad updates can be found quickly.

Staged publishing is different because it adds a pause before anything bad can spread, even if the attacker has good credentials. How well this works will depend on how NPM sets it up—like if it works well with automation, and if it’s optional or required.

In the comments, many people agreed that the old tokens were unsafe, but they also said the new system made life hard for those with lots of packages. Some maintainers said the token change was rushed and poorly explained, and that many open source projects broke overnight. Others liked the idea of staged publishing, thinking it could stop fast-moving attacks, but they worried about how it would affect automated releases for big teams. Some users called for even more checks, like better detection of strange activity, not just better tokens. A few people felt NPM was trying to fix the right problems, but the rollout was too fast and not tested enough with real workflows. Others suggested more communication and better tools would help everyone adjust. Overall, the community wants stronger security, but not at the cost of making publishing so hard that people quit maintaining their projects.

---

## Shipmap.org

- 原文链接: [Shipmap.org](https://www.shipmap.org/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46527161)

Shipmap.org is an interactive website that shows the movement of cargo ships around the world during 2012. The map was made by Kiln, using ship data from the UCL Energy Institute and other sources.

The website lets you see where ships travel, which ports they visit, and what kinds of goods they carry. You can filter by ship type, such as container ships, tankers, dry bulk, gas bulk, or vehicles. The map also shows CO2 emissions and how much freight is moved at different times. The site uses WebGL for smooth animations, and you can zoom in, pan, and change the layers you see, like ship routes or ports. 

The data comes from exactEarth (for ship positions) and Clarksons (for ship details). The team calculated emissions using ship size, engine type, and speed, following a method from an IMO study. The map shows ship activity for each hour, but data from January to April is missing, so you see fewer ships then. Sometimes, ships look like they move over land; this happens because the map connects two points and misses the river or canal in between.

You can embed the map on your own site, and high-res versions can be bought for printing. The project was funded by the European Climate Foundation, and the music on the site is from Bach’s Goldberg Variations.

In the Hacker News comments, many people praised the site’s beauty and how it makes global trade easy to understand. Some loved the smooth animation and how you can see busy sea routes, like between Asia and Europe. Others talked about how seeing the emissions and ship types made them think about the environment and shipping’s impact. A few users were surprised at how many ships are always moving, while some wondered if the data is still up-to-date or if newer data could be added.

A few technical users asked about the WebGL implementation and how the team handled the large amount of data. Someone noted the missing data early in the year and hoped for more complete or recent maps. Others linked similar projects, like flight or internet cable maps, and discussed the value of good data visualization. Some even shared stories about working on ships or in ports, saying the map felt very true-to-life. Overall, people felt Shipmap.org was both educational and fun to explore.

---

## US will ban Wall Street investors from buying single-family homes

- 原文链接: [US will ban Wall Street investors from buying single-family homes](https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46531068)

The US government plans to stop big Wall Street investors from buying single-family homes. This new rule aims to help regular people buy houses, not just large investment companies.

The article explains that big investors have been buying many houses and renting them out. Because of this, it is harder for families to buy a home. The investors often pay more than a normal family can, making house prices go up. Some companies own thousands of homes in many cities. They use computer programs to find and buy houses very fast. This means normal buyers do not have a fair chance. The government says this is not good for communities. People worry that too many rental homes change neighborhoods and make rents higher. The new rule would ban big firms from buying more houses in the future. It may also force them to sell some homes they already own. The government hopes this will make housing more fair and affordable. The article also says there are still many small landlords who own a few houses, and they are not part of the ban.

On Hacker News, some people support the ban. They say homes should be for families, not for big companies to make money. Others think the ban will not fix the real problem. They believe there are not enough houses for everyone, and building more homes is the real solution. A few commenters worry that the ban could have side effects, like making it harder to rent. Some say small landlords are important and should not be punished. Others ask if the rule will really work, or if companies will find ways around it. Some people think the government should also help lower the cost of building new homes. One commenter says that high interest rates and zoning laws are bigger problems than Wall Street. Another says that rental homes are needed for people who cannot buy. Overall, people have many ideas about how to make housing better.

---

## Eat Real Food

- 原文链接: [Eat Real Food](https://realfood.gov)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46529237)

The article talks about new official dietary guidelines in the United States, focusing on eating “real food” to improve health. It says many Americans are sick, with high rates of diabetes and chronic illness, and blames much of this on eating highly processed foods for decades.

The new guidelines say “real food” means whole, nutrient-rich foods—things like vegetables, fruits, whole grains, eggs, seafood, meats, nuts, seeds, and avocados. The advice is to eat more of these foods and less processed food, sugar, and refined grains. Each meal should have high-quality protein and healthy fats, not just carbs. They suggest a protein target of 1.2–1.6 grams per kilogram of body weight per day. People should eat three servings of vegetables and two servings of fruit daily. Whole grains are okay, but only in their natural forms—not refined or overly processed. The guidelines also say to avoid added sugars and artificial flavors, and to drink water or unsweetened drinks.

The new food pyramid is described as a simple and flexible guide, not a strict diet plan. It can fit different cultures and lifestyles, but the main message is to choose real, whole foods most of the time. The article says this approach is backed by science and common sense, and it hopes to help people move past old, unhealthy eating habits.

In the Hacker News comments, some readers welcome the focus on real food and less processed junk. They say these guidelines feel like common sense and wonder why it took so long for official advice to change. Others are cautious, pointing out that food guidelines have changed many times over the years, and they worry about new mistakes or fads. Some people like that protein and healthy fats are no longer being demonized, while others note that the advice still leaves out more specific help for people with different needs, like vegetarians or those with allergies. A few mention that cost and food access are big problems—real food can be expensive or hard to find for some families. There are also comments about how food companies may still find ways to market processed foods as “real,” so people must stay alert. Overall, most commenters agree that eating more real, whole foods is a good idea, but they want to see how these ideas work in practice for all Americans.

---

## LMArena is a cancer on AI

- 原文链接: [LMArena is a cancer on AI](https://surgehq.ai/blog/lmarena-is-a-plague-on-ai)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46522632)

This article argues that LMArena, a popular AI model leaderboard, is hurting the AI field. The author says people trust LMArena’s rankings too much, even though the system is deeply flawed.

LMArena lets anyone online vote on which AI model gives better answers. But most voters only skim the responses and pick the flashiest one. They don’t check facts or read carefully. Because of this, models that look impressive—using lots of text, bold formatting, and even emojis—win, even if their answers are wrong. The leaderboard does not reward truth or accuracy. Instead, it rewards whatever catches a quick glance.

The article gives examples: In one case, an AI that made up a fake quote from The Wizard of Oz won, while the model that gave the right answer lost. In another, a model gave a mathematically wrong answer about cake pan sizes but still won because its answer looked more confident.

The problem is structural. LMArena is open to everyone, and volunteers are unpaid and unchecked. There is no quality control. The people running LMArena know the votes are low quality, so they try to “fix” the results with tricks, but the author says this is just “alchemy”—you can’t get good data from bad votes.

Because companies and researchers care about leaderboard rankings, they tune their models to win at LMArena, not to be accurate or helpful. This pushes the whole industry toward models that are flashy but unreliable.

The author says the AI field needs better ways to measure quality—ways that can’t be gamed by making answers longer or bolder. They urge model builders to choose between chasing leaderboard hype or focusing on making truly useful, reliable AI.

In the comments, many people agree that LMArena’s voting system is flawed and rewards style over substance. Some share their own examples of bad answers winning on LMArena. Others point out that open, crowd-based voting is always easy to game and is not unique to AI.

A few commenters defend LMArena, saying it’s still useful for seeing which models are engaging or user-friendly, even if it’s not perfect. Some suggest the real problem is that companies treat LMArena as the only important metric, instead of using a mix of tests, including expert review and automated benchmarks.

One user notes that it’s hard to get enough expert voters, so crowdsourcing is the only practical option for now. Others think the AI field should invest more in better, more careful evaluation, even if it takes more time and money.

Some people say that leaderboard chasing is a problem across tech, not just in AI, and that the real solution is for leaders to value quality and truth, not just popularity. Yet, a few worry that the market will always favor shiny, popular products over careful, correct ones. Overall, most agree LMArena is fun, but it should not be treated as the gold standard for AI quality.

---

## LaTeX Coffee Stains (2021) [pdf]

- 原文链接: [LaTeX Coffee Stains (2021) [pdf]](https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46526933)

This article is about a LaTeX package that lets you add coffee stains to your documents. The package is called "coffeestains" and is meant to make papers look like they have real coffee marks on them.

The package gives you four types of stains: a full circle with splashes, a small circle, some light splashes, and a colorful twin splash. You use simple commands in your LaTeX file to place these stains anywhere you want on the page. Each command lets you set how see-through the stain is, its size, its angle, and its position. The stains are images that the author made by hand, photographed, and traced digitally. You can download the package and use it for free—there is no copyright, and the author only asks for coffee as thanks.

The package has a fun history. It started in 2009 with a basic coffee stain, then later versions added support for different LaTeX tools, and more features like scaling and moving the stains. Other people helped improve it, making the stains easier to use and place anywhere on the page. The newest version is on Git, and now stains rotate around their own center instead of the page center. The article jokes that more stains are needed, like tea or grease stains, and encourages people to help make them.

In the comments, many people find this package funny and clever. Some say it is a perfect prank for academic papers or a way to make documents look less formal. Others remember times when real stains made their papers look "well-used" or "authentic." A few users ask if there will be more types of stains, like wine or pizza. Some joke about adding features for "aged" or "burned" paper effects. On the technical side, some users discuss how easy the package is to use and how well it works with different LaTeX setups. Others point out that, while fun, it is not very useful for serious documents. But many agree it brings some humor and creativity to the usually dry world of LaTeX. There are even tips in the comments about how to combine this with other LaTeX packages for unique styles.

---

## Health care data breach affects over 600k patients, Illinois agency says

- 原文链接: [Health care data breach affects over 600k patients, Illinois agency says](https://www.nprillinois.org/illinois/2026-01-06/health-care-data-breach-affects-600-000-patients-illinois-agency-says)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46528353)

A data breach at the Illinois Department of Human Services made private health information for over 600,000 patients publicly viewable. The leak happened because map tools used by the agency had the wrong privacy settings from 2021 to 2025.

For about 32,000 people who used rehabilitation services, details like names, addresses, case numbers, and status were shown online. For another 670,000 Medicaid and Medicare Savings Program users, addresses, case numbers, demographic data, and plan names were open to the public. The maps were meant to help the agency decide where to put new offices and resources, but too much patient information was included and made public. The agency says it does not know who, if anyone, actually saw or misused this information. When the problem was found in September 2025, the agency locked down the maps and changed its policy so that customer data can’t be put on public mapping sites anymore. Everyone whose data was exposed will get a letter and a phone number for more help.

People in the Hacker News comments are upset and worried. Many say this breach could have serious effects for patients, even if no misuse is known yet. Some users stress that health data is very sensitive and should always be protected with strong security. Others are frustrated that the mistake lasted for years before it was fixed. A few note that government groups often have weak data practices and slow response times. Some think the agency did the right thing by telling the public and changing their rules, but others feel more needs to be done to stop this from happening again. There’s also talk about how easy it is to misconfigure cloud tools and mapping software, and that these errors often go unnoticed for too long. Some commenters suggest regular audits and better training for staff. Others say people affected should get free credit monitoring. A few point out that, sadly, the U.S. sees big data leaks like this too often, especially in health care. Overall, many people agree that more care and better checks are needed when working with personal data.

---

## 2026 Predictions Scorecard

- 原文链接: [2026 Predictions Scorecard](https://rodneybrooks.com/predictions-scorecard-2026-january-01/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46533343)

This article is Rodney Brooks’ yearly check on predictions he made in 2018 about technology like self-driving cars, AI, robotics, and space travel. He reviews how close his old predictions were, explains new ones for the next decade, and gives updates on what has really happened.

Brooks says his predictions held up pretty well, but he was sometimes too optimistic, even though people once thought he was a pessimist. For example, he did not see the rise of Large Language Models (LLMs) like ChatGPT coming, but he did predict a big new AI idea would arrive around 2023, which happened. He now understands that it’s hard to guess the speed of research, hype, and real-world adoption—they all move differently.

He shares five new predictions for the next ten years: 1) Quantum computers will be useful mostly for simulating physical systems, not for breaking codes. 2) Only Waymo and Zoox matter for self-driving cars in the US; success depends on how little they need humans to intervene. 3) Humanoid robots will still be much less dexterous than humans and too unsafe to work closely with people. 4) There will be fresh, small advances in neural network methods, but no clear winner by 2036. 5) Useful LLMs will need to explain their answers and be surrounded by new mechanisms to control what they do.

He reviews the current state of self-driving cars, noting that “driverless” taxis like Waymo still depend on remote human help more often than people think. He describes a big failure when Waymo’s cars got stuck during a San Francisco power outage, highlighting the need for lots of hidden human support. Cruise, another self-driving company, has shut down, and Tesla’s supposed robotaxis are not really self-driving and require a safety driver. Only Waymo and Zoox are expanding, but slowly, and both still face big technical and business challenges.

For humanoid robots, Brooks is skeptical. He says current robots are far from matching human hand skills or safely moving around people. Companies over-promise on what their robots can do and how fast they can scale. He thinks reality will shift the definition of “humanoid” robots over time, and most current efforts will quietly close or change direction.

Brooks is also doubtful about the pace of electric cars and flying cars in the US. Electric vehicles are not growing as fast as many predicted, and “flying cars” as commonly imagined do not exist in any real way yet. On AI and robotics, Brooks points out that most progress comes slower than the hype suggests, especially for useful robots in homes or elder care. He notes that children can do things with ease that robots still cannot, like navigating a messy house or carrying packages up stairs.

In the comments, some readers praise Brooks for being honest and willing to admit where he was wrong. People appreciate that he sticks to making predictions only in areas where he has experience. Some users agree with his view that tech hype usually outpaces real progress, especially for self-driving cars and robots. Others argue that even slow progress is still impressive, and that setbacks are normal in big new fields. A few readers question whether Brooks is too negative and think that breakthroughs could come faster than he expects, especially with recent AI advances. Some discuss how definitions of “driverless” or “humanoid” keep changing, making it hard to judge predictions. There is debate about LLMs—some think new guardrails will help, while others believe their risks mean they shouldn’t be widely used yet. Overall, most commenters respect Brooks’ careful, detailed approach and his focus on long-term, realistic thinking.

---

