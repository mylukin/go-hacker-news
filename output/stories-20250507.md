# Hacker News 故事摘要 - 2025-05-07

## 今日概述

Today on Hacker News, Unity bans VLC for Unity from their store, stirring debate about licensing and open-source alternatives like Godot. A leaked system prompt for Anthropic's AI, Claude, reveals detailed guidelines for its operation, sparking discussions on AI ethics. The indie game VVVVVV releases its source code, delighting fans. An article highlights software bloat as a major security issue. Docker2exe, a tool for converting Docker images to executable files, receives mixed feedback. Lastly, a study finds cuttlefish use their arms to communicate, intriguing readers about animal intelligence.

---

## Unity’s Open-Source Double Standard: the ban of VLC

- 原文链接: [Unity’s Open-Source Double Standard: the ban of VLC](https://mfkl.github.io/2024/01/10/unity-double-oss-standards.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43914832)

The article covers Unity's decision to ban VLC for Unity from their store, despite using LGPL code themselves. Unity's ban seems inconsistent since many other Unity assets also use LGPL code, like FFmpeg. The LGPL license allows proprietary apps to link to open-source libraries without having to release their source code, but Unity seemed to have issues with assets using LGPL. After the removal, the VLC team launched their own store to distribute VLC for Unity.

In the comments, users are puzzled by Unity's double standard. Many point out that Unity itself uses LGPL libraries, suggesting the ban might be due to a misunderstanding or legal oversight. Some believe Unity's legal team might be avoiding potential complexities with LGPL, as it can still be restrictive, especially for apps distributed on platforms requiring signed components. Others argue that Unity's actions may be to limit legal risk, as ensuring all assets meet LGPL requirements could be difficult. Some commenters criticize the complexity of LGPL and other licenses, saying they lead to confusion and are open to interpretation, suggesting there's room for simpler alternatives. Additionally, many users express skepticism towards Unity's broader business practices, hinting at a growing preference for open-source alternatives like Godot.

---

## Claude's system prompt is over 24k tokens with tools

- 原文链接: [Claude's system prompt is over 24k tokens with tools](https://github.com/asgeirtj/system_prompts_leaks/blob/main/claude.txt)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43909409)

The article discusses a leaked system prompt for Anthropic's AI, Claude, which is over 24,000 tokens long. This prompt includes detailed instructions and rules about how Claude should respond to various queries, including not repeating song lyrics and avoiding politically biased responses. The prompt is intended to guide Claude's behavior and ensure it adheres to ethical guidelines and copyright laws.

Main points from the article highlight how the system prompt serves as a set of pre-defined instructions that guide the AI's responses, almost like a rulebook. The prompt is comprehensive, covering many scenarios, including specific instructions for dealing with copyrighted material, political neutrality, and avoiding hallucinations—where the AI might generate inaccurate information. It's designed to ensure that Claude behaves consistently and ethically, even in complex or sensitive situations.

In the Hacker News comments, there are varied opinions. Some users discuss how the AI can be tricked into bypassing its restrictions through cleverly crafted prompts, likening it to a "jailbreak". Others debate the effectiveness and necessity of such a long system prompt, suggesting that it might be more efficient to integrate these guidelines directly into the model through training. There are also discussions about the legality and ethics of how AI models handle copyrighted content, with some users pointing out that while the system has protections against this, they can sometimes be circumvented. Overall, the comments reflect a mix of technical curiosity and ethical concerns about AI behavior and control.

---

## VVVVVV Source Code

- 原文链接: [VVVVVV Source Code](https://github.com/TerryCavanagh/VVVVVV)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43910681)

The source code for the indie game VVVVVV by Terry Cavanagh has been released on GitHub. The game, known for its simple graphics and challenging mechanics, was originally made in Flash before being ported to C++. The code release allows fans to explore and even modify the game for personal use.

VVVVVV was developed in 2010 and became popular for its unique gravity-flipping mechanic. The code includes both the desktop and mobile versions, with the desktop version being more polished. Despite its simple look, the game is known for its tough levels and catchy music by Magnus Pålsson. The open-source release lets developers see the game's inner workings, although some parts of the code may seem messy, especially due to its Flash origins.

In the comments, one user shared a personal story about meeting Terry Cavanagh, who inspired them with his approach of experimenting with game mechanics. Opinions on the code's quality vary: some find its roughness typical for indie games of its time, while others see it as an example of creativity over perfect coding practices. The game’s simplicity and fun were appreciated, with some reminiscing about playing it during its early release. Overall, the release is celebrated as a great opportunity for learning and nostalgia.

---

## Bloat is still software's biggest vulnerability (2024)

- 原文链接: [Bloat is still software's biggest vulnerability (2024)](https://spectrum.ieee.org/lean-software-development)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43910745)

The article "Why Bloat Is Still Software’s Biggest Vulnerability" discusses the ongoing problem of software bloat, which is excessive and unnecessary complexity in software. The author argues that modern software often contains millions of lines of code, much of which is unnecessary for the task at hand, like opening a garage door. This bloated software becomes a security risk as it creates a larger attack surface for hackers. Additionally, many software systems rely on numerous external libraries, increasing the potential for vulnerabilities. The European Union has started to address these security issues with new legislation, but the article suggests that the software industry needs to prioritize leaner and more secure coding practices.

The author provides an example of lean software called Trifecta, an image-sharing solution with minimal code and dependencies, contrasting it with other bloated software solutions. The article also touches on how incentives in the software industry often favor speed and new features over security and quality. It mentions memory-safe languages like Rust and tools like fuzzers that can help improve code quality but highlights that logical errors still pose significant security risks.

In the Hacker News comments, opinions vary widely. Some users argue that the ease of adding dependencies with package managers like npm leads to unnecessary bloat. Others believe that dependencies should be carefully managed, especially for core business functions, while security should be outsourced to experts. Some commenters express concerns about the quality and security of external libraries, while others argue that reimplementing everything from scratch isn't always practical. There's also discussion about the importance of considering security from the start of development and the need for better dependency management practices. Overall, the comments reflect a mix of skepticism, practical advice, and acknowledgment of the challenges in balancing software complexity with security and usability.

---

## Alignment is not free: How model upgrades can silence your confidence signals

- 原文链接: [Alignment is not free: How model upgrades can silence your confidence signals](https://www.variance.co/post/alignment-is-not-free-how-a-model-silenced-our-confidence-signals)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43910685)

The article discusses how upgrading language models (LLMs) can affect their confidence signals, particularly when moving from a larger model like GPT-4o to a smaller version such as GPT-4.1-mini. The main issue is that these upgrades can eliminate useful confidence signals, making it harder to detect when a model is uncertain or hallucinating. This is a problem for systems that rely on these models for tasks like content moderation, where it's important to identify and review potentially problematic outputs.

One key point is about model calibration, which refers to how well a model's confidence in its predictions matches the actual likelihood of those predictions being correct. The article explains that post-training processes like Reinforcement Learning with Human Feedback (RLHF) can distort this calibration, making models overly confident even when they're wrong. The authors tried several methods to regain lost confidence signals with the new model but were unsuccessful.

The article also highlights that alignment, which aims to make models safer and more steerable, can come at the cost of losing important uncertainty measures. This alignment might make models safer for users but can mask their uncertainty, posing challenges for engineers trying to maintain high-precision systems.

In the comments, users discuss various aspects of the article. Some question how the authors concluded that alignment was the reason for losing the confidence signal, while others wonder about the implications of alignment on model creativity. Users also debate whether reducing model creativity is similar to how humans become more conservative under restrictions. Some suggest that the trade-off between alignment and creativity is akin to risk-aversion in humans.

Others explore the idea of having dual-mode models, with one mode focused on safety and another for creativity. A few commenters express concerns about using LLMs in high-precision systems, noting potential risks. There are also discussions about the role of AI in filtering content and whether it should be more open or restricted. Overall, the comments reflect a mix of curiosity, skepticism, and interest in the balance between model safety and functionality.

---

## docker2exe: Convert a Docker image to an executable

- 原文链接: [docker2exe: Convert a Docker image to an executable](https://github.com/rzane/docker2exe)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43899288)

Docker2exe is a tool that converts a Docker image into a standalone executable. This allows users to share Docker images as files that can be run directly, making it easier to distribute applications to others who might not be familiar with Docker. To use docker2exe, you need Docker and GoLang on your machine. The tool creates executables for different operating systems, and these executables check for the needed Docker image on a user's system, downloading it if necessary.

In the comment section, people had mixed feelings. Some found it confusing because the executable still requires Docker to run, which seems to counter the idea of making it simpler for non-technical users. Others liked the potential for simplifying the shipping of applications, especially to those unfamiliar with Docker. A few commenters suggested using similar tools like dockerc, which might not have the same limitations. Some felt that Docker adds unnecessary complexity and wished for simpler software distribution methods. Yet, others appreciated the project as a step towards making software distribution easier, especially for applications that need certain environments to run smoothly.

---

## Cuttlefish 'talk' with their arms, study reveals

- 原文链接: [Cuttlefish 'talk' with their arms, study reveals](https://scienceblog.com/wildscience/2025/05/06/cuttlefish-talk-with-their-arms-study-reveals/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43904478)

Scientists have found that cuttlefish use their arms to communicate, adding a new layer to our understanding of these fascinating sea creatures. Researchers from Paris and Italy discovered that cuttlefish perform specific arm gestures, called "arm wave signs," to communicate with one another. These gestures create a multi-sensory conversation, as they can be seen visually and felt through water vibrations. The study focused on two species of cuttlefish, observing arm movements like "up," "side," "roll," and "crown" that often include color changes in their skin. Interestingly, cuttlefish responded more to videos of these gestures when shown right-side up, hinting at their complex understanding. The research suggests that this communication is perceived through cuttlefish's sensory organs, showing parallels between cephalopod and vertebrate communication systems. While the exact meanings of these signs aren't fully known, they appear in various contexts, from mating to defense, hinting at multiple purposes.

In the comments, users shared mixed reactions. One commenter humorously linked to an old Reddit story about someone communicating with a cuttlefish using hand gestures. Others drew comparisons to the way humans use gestures, like sign language, to communicate, highlighting the rich, non-verbal ways of interaction. Some debated whether the study truly proved communication, questioning if reactions alone signify a communication system. Others reflected on the ethical aspects of eating intelligent animals like cuttlefish and octopi, citing their intelligence as a reason to reconsider culinary choices. A few commenters joked about the cultural similarities between cuttlefish gestures and Italian hand gestures. Overall, the discussion highlighted curiosity about animal intelligence and empathy, with varying opinions on the implications of these findings.

---

