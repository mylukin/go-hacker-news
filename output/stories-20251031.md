# Hacker News 故事摘要 - 2025-10-31

## 今日概述

Today’s top Hacker News stories cover tricky bugs in async Rust, Ubuntu’s new high-speed CPU support, and Google’s stricter rules for Android app developers. There are also stories about big security mistakes at Tata Motors, a new way to search huge data archives in your browser, and tools for tracing Linux performance. Other highlights include a deep dive into x86 opcodes and a look at how Fly.io keeps servers in sync worldwide. If you like programming, security, or system design, you’ll find something interesting today.

---

## Futurelock: A subtle risk in async Rust

- 原文链接: [Futurelock: A subtle risk in async Rust](https://rfd.shared.oxide.computer/rfd/0609)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45774086)

This article explains a tricky bug in async Rust called “futurelock.” It happens when a future is waiting for a resource, like a Mutex, but the task stops polling that future. Another future, from the same task, also wants the same resource, but can’t get it. The code looks like it should work, but it hangs forever.

The article shows an example using Tokio and Mutex. A background task grabs a lock, waits, then releases it. Meanwhile, the main code uses `tokio::select!` to run two futures: one waits for the lock, the other waits for a timer. When the timer is done, it runs another future that also needs the lock. Now, both futures are waiting for the lock, but the main task only polls one of them. When the lock is finally free, the first future in line should get it, but no one is polling it anymore. The program is stuck.

The article explains why the Mutex cannot fix this. It wakes the right future, but if the main task isn’t polling that future, nothing happens. This is not a problem with unfair or fair Mutex—both can have this issue. The core problem is a single task owning several futures that block on each other, but the task only polls one at a time.

The article says using `tokio::spawn` to run each future in its own task avoids this problem, because each task only holds one future. If you use a loop with `tokio::select!`, wrapping each future in a task is safer. Using `join_all` is also fine, because it polls all futures until they finish.

The article warns that debugging futurelock is very hard. When it happens, your program just hangs. You may see tasks waiting forever, not knowing why. There are some tips for avoiding futurelock, like not using `&mut future` with `tokio::select!` and not putting `await` in select branches unless you know what you are doing.

In the comments, some people say this is a great explanation of a subtle bug in async Rust. They like the clear examples and advice. Others say they have hit similar problems, and debugging was very hard—they only found the cause after hours or days. A few suggest that futurelock is an example of how async code is hard to reason about, especially with shared resources.

Some commenters wonder if Rust could add better tools or lints to catch this pattern. Others reply that it might be possible, but many false positives could happen. Some users say they now prefer to always spawn tasks for real concurrency, rather than rely on `select!` and manual polling.

A few people compare this to deadlocks in classic multithreaded code, but agree futurelock is sneakier, because even simple code can get stuck if you’re not careful. Some point out that documentation for async primitives could be clearer about these risks. Others say this is a trade-off for Rust’s powerful async system, and developers just need to be careful and understand these patterns. A couple of readers share similar stories from other languages, showing that async deadlocks are a common problem, not just in Rust.

---

## Introducing architecture variants

- 原文链接: [Introducing architecture variants](https://discourse.ubuntu.com/t/introducing-architecture-variants-amd64v3-now-available-in-ubuntu-25-10/71312)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45758392)

Ubuntu 25.10 is adding support for “architecture variants,” starting with special packages built for amd64v3 (a newer type of CPU). This means some Ubuntu packages can now run faster on modern computers, without dropping support for older machines.

The article explains that Ubuntu has always tried to work on as many computers as possible, but this can limit performance. Processors using amd64v3 have new features, so programs made just for them can work a bit faster (about 1% faster in tests, sometimes more for special programs). Ubuntu built the tools and systems to let some packages be rebuilt for amd64v3, but only in the “main” part for now. These new packages are not fully tested yet, so early users might find bugs.

If you want to try amd64v3 packages, you need a modern computer (most from the last 10 years are fine). The article shows how to check your CPU and how to update your system to use the new packages. There is a warning: if you install these new packages, you can’t just move your hard drive to an older computer that doesn’t support amd64v3, because it won’t work. By the next Ubuntu LTS version (26.04), the team wants to rebuild all packages for amd64v3 and test everything well.

The article also describes some small problems, like package managers sometimes showing confusing messages about “downgrading” packages or third-party repositories not having amd64v3 versions yet. These are being fixed in future updates.

In the comments, some users are excited about the speed boost and happy to see Ubuntu moving forward. Several people tried updating and ran into errors during the upgrade, mostly with certain files (like changelogs) causing conflicts. Users shared workarounds, such as deleting or renaming the problem files, which often fixed the upgrade.

A few users were confused by warning messages from apt, but others explained these are normal and will be improved soon. Some noticed that driver tools stopped working or showed empty results, and they wondered if this was related to the new packages. There were also reports of strange icons after rebooting, but running the upgrade again or removing some old packages fixed things.

One user asked about building packages for amd64v3 on their own, showing interest in customizing their system. Most people agreed that the process is still rough, but the Ubuntu team is responsive and helpful, asking for bug reports and giving advice. Overall, the community is testing the new feature, sharing fixes, and looking forward to better support in the next release.

---

## A theoretical way to circumvent Android developer verification

- 原文链接: [A theoretical way to circumvent Android developer verification](https://enaix.github.io/2025/10/30/developer-verification.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45776269)

This article talks about Google’s new developer verification rules for Android apps. Google now requires most apps to be linked to a verified developer, even for apps loaded outside the Play Store.

The author explains that this change makes it harder for small developers to share their apps. There is a paid license (costs $25) and a free “hobbyist” license, but the free one may have strict limits, such as how many people can install your app. No one knows the full rules for hobbyists yet. Google says you can still install apps directly with ADB (a developer tool), but these details are unclear, and Google could change the rules in the future. Also, the new system is not open, so it’s hard to see what Google is really doing.

The author worries that these changes will stop many independent developers from sharing their apps. For example, older apps using old Android features may not pass Google’s security checks. The author also points out that installing with ADB is not easy for normal users, so it’s not a good solution.

To work around these rules, the author suggests a new idea: make a special “loader” app. This app would be verified once, then it could load and run other apps without needing to install them the normal way. This works because Android’s Java system lets one app load code from another file. The loader app would need to handle starting the other app’s screens and manage file conflicts. This is technically hard because Android changes a lot between versions.

The author tried to make a simple version of this loader but ran into many problems. Some people suggested using risky, “unsafe” Android features to trick the system, but the author couldn’t get this to work. The idea is only a concept for now, not a working tool.

Another problem is that even this loader app needs to be verified. If you use the free hobbyist license, you might need many people to help sign and share the app, which is hard to organize. The loader app might also be blocked by Google for being suspicious, so it would need to hide what it does, maybe by pretending to be a normal app.

The author ends by saying this is just an idea, not a real solution, and asks for feedback.

In the Hacker News comments, many people are unhappy with Google’s new rules. Some worry this will make Android as closed as Apple’s iOS, hurting indie developers and open-source apps. Others say the paid license is not the real problem—the problem is the confusing and changing rules and the lack of clear information from Google.

A few commenters think the loader idea is clever but probably won’t work for long, because Google could block it with future updates. Some point out that similar tricks have been blocked before, and Google is always closing these loopholes.

Some developers share stories about their apps being removed from the Play Store for small rule changes, making them worry about trusting Google. One person says this will push people to use alternative app stores, but others reply that these are also getting harder to use.

Several people discuss how this will hurt users who want to run old apps or apps not in the Play Store. Others think the new rules will help stop malware, but most agree the balance is wrong.

Finally, a few suggest moving to open platforms like Linux phones, but most agree this is not realistic for most people. Most commenters think these changes are bad for Android’s openness and make life harder for small developers and power users.

---

## Hacking India's largest automaker: Tata Motors

- 原文链接: [Hacking India's largest automaker: Tata Motors](https://eaton-works.com/2025/10/28/tata-motors-hack/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45741569)

This article tells the story of a security researcher who found big problems with Tata Motors’ websites. Tata Motors is India’s largest car company, and the researcher discovered that sensitive information was not well protected.

First, the researcher found two sets of Amazon Web Services (AWS) keys on Tata Motors’ websites. These keys gave access to over 70 terabytes of important company data, including customer details, invoices, and business reports. One set of keys was even hidden in plain sight, used just to download a small tax code file. The second set was “encrypted,” but the encryption was easy to break because the decryption code was also on the website. With these keys, anyone could read, download, or even upload files, which could lead to serious damage.

Next, the researcher found a backdoor in a tool called Tableau, which Tata Motors used for business reports. With just a username, and without needing any password, anyone could log in and access private dashboards, financial data, and other internal documents.

Another problem was on a fleet management site, where the key for Azuga (a car tracking service) was left in the public website code. This key could be used to access test drive data for Tata’s cars.

The researcher reported all these problems to Tata Motors through India’s cybersecurity team. However, the company was slow to fix the issues, especially the AWS keys. It took many reminders and several months before all the problems were finally solved.

People in the Hacker News comments had lots to say. Some were shocked that such a big company could leave their secrets out in the open. They pointed out that AWS keys should never be in public code—this is a basic security rule. Others said this shows how often companies ignore simple security practices, especially when teams are under pressure or not trained well.

A few commenters argued that using client-side encryption (where decryption happens in the browser) does not work, because if someone can see the code, they can always find the key. Some people said Tata Motors was lucky the researcher was ethical, because a bad actor could have done much worse.

Another group noted that many companies, not just Tata, make these mistakes, especially big firms that move fast and have many different teams. Some discussed how slow companies can be to react to security reports, which puts customers at risk for longer. A few shared similar stories from their own work, showing this is a common problem.

Many agreed that companies should have better bug bounty programs and respond faster to security warnings. Several people praised the researcher for being responsible and patient, saying it takes real effort to get big companies to fix things. Finally, some wished for stronger rules and more training, so companies protect user data better in the future.

---

## Tim Bray on Grokipedia

- 原文链接: [Tim Bray on Grokipedia](https://www.tbray.org/ongoing/When/202x/2025/10/28/Grokipedia)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45777015)

Tim Bray shares his thoughts after reading his own entry on a new website called Grokipedia, which aims to be an alternative to Wikipedia. Grokipedia uses AI to auto-generate long articles, and Bray’s entry there is over 7,000 words—much longer than his Wikipedia page.

He says the article covers every part of his life, but it is too long and boring, even for him. More importantly, he finds mistakes in every paragraph. Some errors are obvious, others are only clear to him as the subject. The writing style is flat and generic, typical of text made by large language models (LLMs). He also points out that many references are just links that do not really support what the article says.

Bray checks how Grokipedia handles debates about “woke” and “anti-woke” topics. He sees that Grokipedia often tries to balance or push back against what it calls “woke” arguments. For example, it uses many right-leaning sources to defend big tech companies or challenge climate activists like Greta Thunberg. Sometimes, the citations do not match the claim or come from questionable sources. Bray does not find these attempts very convincing or helpful.

He concludes that, unlike Wikipedia, Grokipedia does not serve well as a quick reference or a place for deep research. Right now, it is not very useful, but maybe it will improve in the future.

In the Hacker News comments, some users agree with Bray that AI-written articles often sound flat and miss important context. Others think Grokipedia could be interesting if it improves quality and checks sources better. Some users are worried about possible bias, saying Grokipedia just swaps one kind of bias for another. A few people argue that Wikipedia also has problems with accuracy and bias, so competition is good. Some commenters point out that AI-generated content can quickly spread mistakes if not watched carefully. Others are curious if Grokipedia can ever really replace Wikipedia, or if it will just become another echo chamber. A couple of users joke about the huge length of the articles and wonder who would ever read them. Lastly, some hope that both sites will push each other to get better over time.

---

## Leaker reveals which Pixels are vulnerable to Cellebrite phone hacking

- 原文链接: [Leaker reveals which Pixels are vulnerable to Cellebrite phone hacking](https://arstechnica.com/gadgets/2025/10/leaker-reveals-which-pixels-are-vulnerable-to-cellebrite-phone-hacking/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45766501)

A new leak shows that some Google Pixel phones can be hacked by Cellebrite, a company that makes tools to unlock phones for police. The article shares a list of Pixel phones that Cellebrite can break into, and which ones are safe.

Cellebrite is known for helping law enforcement get data from locked phones. They use special tools to unlock and copy information. The leak says that older Pixel models, like Pixel 2, 3, and 4, are vulnerable. This means Cellebrite’s tools can get into them and take data like messages, photos, and contacts. Newer models, like the Pixel 7 and 8, are not listed as vulnerable. The article points out this is likely because newer phones have better security features. Google updates their phones’ software to fix problems, but hardware also matters. If you have an old Pixel, just updating the software may not be enough to keep it safe. Some experts say this shows why it’s important to use new phones with up-to-date security. The article also says that other brands, like Apple’s iPhones, may have similar risks, but this leak is just about Pixel phones.

In the comments, some people are surprised that even newer Pixels were once at risk. Others say it’s expected that law enforcement has ways to unlock phones, and users should not assume their data is safe if police have their device. A few readers point out that security is always a race—hackers and companies keep trying to outsmart each other. Some users share tips, like using strong passwords, turning off your phone if you’re worried, or buying phones that get regular security updates. A few are worried about privacy and wonder if it’s possible to have a truly secure phone. Others say this leak helps everyone understand phone security better, and that it’s good Google is making progress on new models. Some commenters think the leak might make companies work harder on security. There are also jokes about how hard it is to keep up with phone security news. Finally, some readers remind everyone that no device is ever 100% safe.

---

## Use DuckDB-WASM to query TB of data in browser

- 原文链接: [Use DuckDB-WASM to query TB of data in browser](https://lil.law.harvard.edu/blog/2025/10/24/rethinking-data-discovery-for-libraries-and-digital-humanities/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45774571)

This article is about how Harvard’s Library Innovation Lab made it possible to search huge public data archives right in your browser, without needing big servers. Their team wanted to help libraries and digital projects make their collections easy to find, but with less cost and less work.

Before, if you wanted users to search and filter big collections, you needed servers and databases. These are expensive to run and need people to keep them safe and working. Over time, projects can lose funding or staff, and then their websites break or disappear. The cheaper way is to just store files online as static files, which is much cheaper and easier. But then users can’t really search or filter—they have to look through folders or use bad search tools.

Harvard’s team wanted something better for their Data.gov Archive, which is 18 terabytes of data. They wanted a way to let people search and filter the data, but without needing to run servers. They tried new tools that let you run database queries in your browser, using WebAssembly and HTTP range requests. DuckDB-Wasm is one of these tools. It lets the browser only download the small parts of the data needed for each search, not the whole file.

They built a website where the data is stored as compressed Parquet files (a special format for big datasets) on cheap static storage. When someone searches, the browser loads DuckDB-Wasm and runs the search locally, pulling only the data needed. This means there’s no server doing the work—it all happens in your browser.

They did run into some problems, like the DuckDB-Wasm file being large and slow to load, and needing to organize the data carefully for speed. They are still looking at other tools to make it faster. But overall, they are happy: it’s cheap, easy to keep running, and people can search and browse a huge archive right from their browser.

This approach is important for libraries and digital projects. It means lower costs, less work to keep things running, and better chances that archives stay online even if budgets or staff change. The team thinks others can use this idea too—especially if they have big, mostly unchanged datasets. They invite others to try it, share their results, and work together to improve these tools.

In the Hacker News comments, people were excited about the idea of using DuckDB-Wasm and similar tools to avoid running expensive servers. Some mentioned how this could help universities and small projects that don’t have big IT budgets. Others pointed out that this approach works best for data that doesn’t change often, since updating static files can be tricky.

Some users worried about browser memory limits, saying this might not work for very large or complex queries. A few commenters asked about privacy, since all the data comes to the user’s browser, and wondered if this could be misused. There was also a discussion about the size of the DuckDB-Wasm binary and startup time, with suggestions for making it smaller or faster.

Several people shared their own projects using similar techniques, like running SQLite in the browser, or building static sites for open data. Others talked about the power of HTTP range requests, and how they let users grab just the pieces of data they need. Some saw this as a big win for the open web, making it easier to share public data without huge costs or technical hurdles.

Overall, the community liked the idea, saw lots of possible uses, and offered ideas for making it even better.

---

## Perfetto: Swiss army knife for Linux client tracing

- 原文链接: [Perfetto: Swiss army knife for Linux client tracing](https://lalitm.com/perfetto-swiss-army-knife/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45771019)

This article talks about Perfetto, a set of tools to help Linux and embedded developers trace and fix performance problems. The author explains how Perfetto works, gives a real example with a Rust demo app, and shows how to use Perfetto’s UI for deep debugging.

Perfetto is made of several parts: recording tools to collect data, a special file format (protobuf), and a trace processor that lets you run SQL queries on your trace data. You can collect traces from many sources—like perf, ftrace, and even custom programs—and Perfetto can read many common formats. The Perfetto UI is a web tool that shows timelines, lets you select regions, and gives instant flame graphs and tables for analysis. Importantly, it runs in your browser and does not send data anywhere.

The demo program is a Rust app that renders fractals using the GPU and CPU. The app sometimes drops frames, and the author uses Perfetto to find out why. First, using perf, they create a flame graph, but it only shows the average over the whole run, hiding short-term problems. Perfetto’s UI, however, can show what happens at specific times, letting them zoom in to see the “staircase” pattern where only one thread runs at a time.

To dig deeper, the author collects a scheduler trace, which shows how threads are sleeping and running. The trace reveals that threads often sleep, causing performance drops. To find the real reason, the author uses application-level tracing (with Rust’s tracing library), which shows that a function to update quality is blocking everything. By merging all the traces in Perfetto’s UI, they see the problem clearly: adaptive quality updates are causing frame drops.

The article also highlights powerful features of Perfetto UI: area selection, custom filters, dynamic tracks, and SQL queries to find slow events or patterns. The author fixes the problem by moving the quality update to a background thread, improving performance.

Perfetto is used in many places, not just Android and Chrome. Mesa, VizTracer, pthread_trace, magic-trace, and other tools all use Perfetto or its format for tracing. There are scripts and projects to make Perfetto easier to use, and it works with BPF-based tracing too.

In the Hacker News comments, some users praise Perfetto as a very useful tool, especially for complex debugging when performance issues are hard to catch. They like that it supports many trace formats and can merge them together. Others say Perfetto is easier to use than older tools like trace-cmd or perf alone, and that the web-based UI is a big improvement.

A few commenters mention that the setup can be confusing at first, especially for new users who do not know which events to record or how to start. Some wish for better guides and more built-in help. Others point out that Perfetto’s focus on browser-based tools is good for privacy, but they want stronger guarantees that trace data never leaves their machines.

Several developers discuss using Perfetto with Rust, Python, and even kernel development; they like that the tool is flexible and works with many languages. Some share tips for writing converters for their own trace data. Others mention that merging traces from different sources is still tricky, but getting better.

A few people compare Perfetto with tools like pprof or Speedscope. They like Perfetto’s timeline and area selection but note that each tool has its own strengths. Some suggest that integrating GPU traces or supporting more file formats would make Perfetto even more powerful.

Finally, some users thank the Perfetto team for being open to contributions and for merging pull requests quickly. Others are happy that the tool is open source and find the code and docs easy to follow. Overall, the comments show that Perfetto is popular, flexible, and getting better, but still has some learning curve for new users.

---

## x86 architecture 1 byte opcodes

- 原文链接: [x86 architecture 1 byte opcodes](https://www.sandpile.org/x86/opc_1.htm)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45774724)

This article lists all 1-byte opcodes in the x86 architecture, showing which instructions each code represents and how they change with different CPUs and modes. The page is like a big table for anyone who needs to know what each byte value does on x86 processors.

It covers basic instructions like ADD, SUB, MOV, and XOR, and shows how these work with registers, memory, and immediate values. It also lists special and legacy instructions, like PUSH, POP, and XCHG, and explains how some opcodes have different meanings in 16-bit, 32-bit, and 64-bit modes. The article describes how new CPU features, like REX and VEX prefixes, let x86 handle more registers or new instructions. There are notes about NOP (no operation) instructions, showing that “XCHG AX,AX” is the classic 16-bit NOP, while “XCHG rAX,rAX” is used in 64-bit. The author mentions that some NOPs behave differently in 64-bit mode, especially with REP prefixes or certain register sizes. There are also comments on less-used or reserved opcodes, and on how some byte values are now used by companies outside Intel or AMD. Links point to extra tables for grouped or two-byte instructions, and to technical notes for more details. The page helps programmers, emulator writers, and reverse engineers see exactly what each opcode byte does, and how it has changed over time.

In the Hacker News comments, some people say this table is a “must-have” for anyone working close to the hardware, like OS developers or those making emulators. Others point out how complex and messy x86 has become, comparing it to simpler architectures like RISC-V. A few users share stories about debugging hard-to-find bugs because documentation like this was missing or unclear. Some commenters are amazed at how many instructions are packed into just one byte, while others are surprised by all the old, strange, or obsolete instructions that are still supported. A few people mention using this table when writing JIT compilers or disassemblers. There’s also talk about how certain opcodes, like NOPs, have special uses or tricks for timing and optimization. One person jokes about “arcane magic” needed to fully understand x86. Some wish that x86 was designed more cleanly, but others respect its history and the need for backward compatibility. A couple of users highlight the author’s deep knowledge and thank him for keeping this resource up to date.

---

## Corrosion

- 原文链接: [Corrosion](https://fly.io/blog/corrosion/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45710875)

This article is about Corrosion, a new system Fly.io built to handle global state synchronization for their platform. They needed a better way to keep track of all their servers and apps around the world, after running into problems with existing tools.

The main problem was keeping their network up-to-date as apps started and stopped on servers everywhere. Old systems like Consul and SQLite weren’t enough—they were too slow, complex, or broke under global scale. Corrosion takes ideas from network routing protocols like OSPF. Each server keeps its own data and shares changes with the others using a gossip protocol, similar to how routers tell each other about new links. The system uses a SQLite database that is synced between servers, but without central control or heavy consensus rules. It uses CRDTs (conflict-free data types) to handle updates, making sure changes don’t clash. The team learned hard lessons from outages, like a deadlock bug that spread fast and brought down all their proxies, and problems from database changes that caused huge amounts of network traffic. They also found issues running both Corrosion and Consul at the same time, which led to network overload when Consul failed.

To make things better, they added watchdogs to restart services if they got stuck, improved their testing, and now keep backups to recover from failures. They also changed how updates are sent, moving from small partial updates to full data sends for each server, which helped avoid bugs. Recently, they split Corrosion into regional clusters instead of one global system, so problems affect fewer places if they happen.

In the comments, many people were impressed by the honesty in sharing failures and lessons learned. Some liked the switch from consensus-based systems to a gossip protocol, saying it fits Fly.io’s needs better. Others pointed out that CRDTs and gossip protocols can be tricky and need careful handling to avoid new problems. A few users asked if Corrosion could be useful for other companies, or if it was too specialized for Fly.io’s setup. Some commenters worried about the risks of not having a central source of truth, but others argued that regionalization and backups help manage those risks. There were questions about how Corrosion compares to tools like Etcd or Zookeeper, with answers explaining the difference in design and trade-offs. Several users praised the technical depth of the post, while a few said the “war stories” showed how complex real distributed systems are, no matter how carefully you plan. Many agreed that sharing these details helps everyone in the industry learn.

---

