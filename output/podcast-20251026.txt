Hello everyone, this is the 2025-10-26 episode of Hacker News Daily Podcast. Today, we have a full lineup of stories from technology, science, and the world of programming. Let’s get started.

First, we look back at the Cold War with a story about the NORAD Cheyenne Mountain Combat Center, built in the 1960s as a secret bunker for North American air defense. The article explains why a new center was needed: the first operations center was just a small office, even using an old bathroom, and was not safe enough. As the Soviet missile threat grew, the U.S. Air Force decided to build a bigger, safer place underground. Work began in 1961 and finished in 1964. The bunker was designed to survive a nuclear bomb hit and could run for 30 days without any outside help, with its own power, water, and food.

Inside, there were 11 buildings placed on huge steel springs to protect against shockwaves. Staff used computers like the Philco 212 and large screens to watch for missiles, planes, and satellites. Special radio antennas, thick concrete walls, and lots of steel gave extra safety. There was even a special camera, the Baker-Nunn, to track satellites in space. Both U.S. and Canadian officers worked together in the center. The article shows how the center was built to be almost impossible to destroy.

Hacker News users were impressed by the technology and engineering of the time. Some compared the bunker’s self-sufficient design to modern data centers or science fiction movies. Others noted how much computers have changed—those old Philco 212 machines were huge and slow compared to today’s phones. There were memories of the fear and tension during the Cold War, and personal stories from people whose family members worked at Cheyenne Mountain. Some discussed the famous role of the bunker in movies like “WarGames” and “Stargate.” Others respected the engineers who planned for every danger, and a few joked that the bunker was more comfortable than their own offices. Overall, people were both impressed and a bit nostalgic.

Next, we have MyraOS, a new operating system made from scratch in C and assembly for x86 computers. It’s not just an experiment—the goal is to be a real Unix-like OS that runs on real hardware. MyraOS has many features: it works in protected mode, has memory management and paging, uses both user and kernel modes, and supports drivers for keyboard, mouse, disk, and display. It uses the ext2 filesystem and has a window-based user interface. The OS can run real programs with an ELF loader, and even comes with the classic game Doom built in. MyraOS can be tried using QEMU on Windows, Linux, or macOS. The creator is looking for feedback and spent a lot of time on the project.

Comments on Hacker News are full of praise. Many people said it’s impressive to write an OS from scratch with so many features. Some asked about performance and if it runs on real hardware. Others were curious about the Doom port and graphics support. There was discussion about the learning value of building an OS and which parts were hardest, like memory or drivers. Some suggested better documentation or simpler build steps. A few shared links to their own OS projects and encouraged the creator to continue. There were warnings that OS development is hard to maintain alone, and some suggested finding more contributors. Many liked that MyraOS is not just a “toy” OS and hope the creator will share blog posts about the process to help others learn. Overall, people are excited and supportive.

Moving on, we have a look at Wren, a small scripting language designed to be simple, fast, and easy to use in other programs. Wren is inspired by Smalltalk, Lua, and Erlang, but uses a modern syntax. The language is small, with clear and readable code. It’s fast, compiling in one step and running tight bytecode. Wren is class-based, supports concurrency with fibers, and is easy to embed in C and C++ apps. The website has guides and an online playground, with simple code examples.

Hacker News users liked Wren’s small size and clear code. Many thought fibers were a cool way to handle many tasks at once, and embedding Wren is easy. Some compared Wren to Lua, saying Lua is still more popular, but Wren’s class-based style is more modern. A few wanted a bigger standard library, but others liked the minimalism. Some shared their own use cases, especially in games or small apps, and said the syntax is good for beginners. There were hopes for a bigger community and praise for the good documentation. Some wondered about performance, but most agreed Wren is fast enough for scripting. Overall, people are interested, with a mix of hope and questions about Wren’s future.

Now, let’s talk about a tricky PyTorch bug that stopped a neural network from learning on an Apple Silicon Mac. The writer tried changing hyperparameters and checked their code, but nothing worked. The bug was deep inside PyTorch’s backend. The loss during training stopped improving because the encoder weights never changed, even though gradients seemed to flow and the optimizer worked for the decoder. With Adam optimizer, encoder weights did not update, but with SGD, they did. The cause was a bug in PyTorch’s MPS backend—when working with non-contiguous tensors, certain operations silently failed. The encoder weights were non-contiguous due to being transposed, and Adam’s state tensors copied this layout, so updates failed.

The fix was to make tensors contiguous at initialization or upgrade to a new PyTorch or macOS version. The article explains how PyTorch chooses different kernels for devices and why memory layout matters. Many commenters shared similar stories. Some said they always blame their own code first, and others noted that device-specific bugs are hard to find. Many agreed that PyTorch should give errors instead of silent failures. Some praised the deep dive into debugging and said real-world stories like this help more than documentation. Others warned that making tensors contiguous can slow things down if not used carefully. Several people felt encouraged by the clear debugging steps, and some said this shows the value of good bug reports and open-source software. Most agreed the article was both useful and encouraging.

Next, we have news about Advent of Code 2025. For the first time, the event will have only 12 puzzles, not 25. The event is still run by Eric Wastl and keeps most old traditions, but this is the biggest change in ten years. Advent of Code is an online programming contest with small puzzles for all skill levels. You can solve puzzles in any language, and many use it for learning or practice. No deep computer science background is needed. Problems are designed to run on old computers, and you can support the event by sharing or donating.

Some changes: the global leaderboard is gone, replaced by private leaderboards with friends, because the old system caused stress and even DDoS attacks. Hacker News comments are mixed. Some are sad to lose the full 25-day tradition, but others think 12 puzzles is better—it’s easier to finish, and more people will complete the event. Some say making 25 puzzles each year is very tiring for Eric, and they support the change. Others worry about losing the “advent calendar” feel, but many agree that quality is more important than quantity. Some wish the global leaderboard would come back, but agree that cheating made it hard to keep. Most people are just happy the event continues.

Switching gears, we have an article about System.LongBool in RAD Studio, the set of tools for building apps in Delphi and C++. System.LongBool is a special type used to show true or false values. Unlike a normal Boolean, which may use 1 byte, LongBool uses 4 bytes. Zero means false, anything else is true. This helps when working with other languages or operating systems that expect a 4-byte Boolean. There are also ByteBool and WordBool, which use 1 and 2 bytes. The main reason for these types is to make code work well with different libraries and systems.

In the comments, people discuss why these Boolean types exist. Some say LongBool avoids bugs with Windows APIs, which expect 4-byte values. Others warn that mixing types can cause confusing bugs, since only zero is false and any other number is true. Some share stories of old bugs caused by mixing Boolean types, or code expecting only 0 and 1. A few point out that these differences are mostly for making C++ and Delphi work together smoothly. One comment says to use standard Boolean types unless you need compatibility. Another reminds people to check the documentation when calling system libraries. Some find it strange that small details like this can break programs, but others say it’s normal when working close to hardware. The comments show that while LongBool is simple, using the right type is important in real projects.

Next up is new research on Alzheimer’s disease and the brain’s daily rhythms. The study used mice to see how Alzheimer’s affects genes in microglia and astrocytes—cells that help clean the brain. Normally, our body runs on a circadian rhythm, like an internal clock. Alzheimer’s patients often have trouble sleeping and get confused at certain times. The study found that in mice with Alzheimer’s, the timing of hundreds of genes in cleaning cells gets mixed up—different from normal aging. Some of these genes are linked to Alzheimer’s risk, and about half are controlled by the internal clock. When the rhythm is disrupted, the cells do not clear away harmful amyloid protein as well, making the disease worse. The presence of amyloid also causes some genes to start following new, abnormal rhythms, especially those involved in brain inflammation.

Scientists hope that by fixing or controlling these rhythms, they might slow or stop Alzheimer’s. In the comments, some users were excited about the idea of treating Alzheimer’s by fixing circadian rhythms. Others noted that sleep problems often come before memory loss, so this research makes sense. Some wondered if better sleep habits could help protect against Alzheimer’s, while others said results in mice do not always apply to humans. There was discussion about the importance of microglia and astrocytes, and some users shared personal stories of caring for family members with Alzheimer’s. Some hoped for more research into how light, diet, and exercise might help the brain’s clock. The mood was hopeful, but careful.

For our next story, we return to machine learning and hardware, with a look at real-world ML work on the NVIDIA DGX Spark system. The writer spent six days training and fine-tuning large models, comparing NVIDIA’s official benchmark numbers to real production use. NVIDIA’s benchmarks say DGX Spark is very fast, both for training and inference. In practice, the writer found that training was fast and matched the benchmarks, and inference was fast on smaller models. But there were big problems: GPU inference was broken—models trained fine, but GPU outputs were empty or had errors. Only CPU inference worked, but it was slow. Training on GPU also had problems, like memory fragmentation, causing the system to freeze after a few hours, unless jobs were split up and manual memory clearing was done. Some software did not work at all on this new hardware.

The root of these problems is the new combination of ARM64, the Blackwell GB10 GPU, and the latest CUDA version. Many ML tools are not ready for this setup, and drivers are still new. The writer points out that NVIDIA’s public benchmarks do not mention these real-world issues. In short, training is fast with workarounds, CPU inference is reliable but slow, and GPU inference is not ready for production. Anyone using DGX Spark now must be an expert and expect things not to “just work.”

Hacker News users agreed that hardware benchmarks can hide real pain points. Some shared similar problems with new GPUs and drivers and suggested using older, more stable hardware for important work. Others noted that production ML always needs more testing and manual work than people expect, and that edge-case problems are common with new tech. Some wondered if updates would fix these bugs soon, while others thought alternative hardware or cloud solutions might be safer for now. Many praised the honest write-up and wished more people shared “what went wrong” stories. Overall, the view is that “bleeding edge” hardware is powerful but risky, and honest reviews help the whole community.

Now, let’s go back in time again with the history of the electron microscope. This tool lets scientists see much smaller things than normal light microscopes—down to viruses and proteins. Light microscopes cannot show details smaller than about 200 nanometers. Scientists tried X-rays, but those were hard to focus. The breakthrough came in the 1930s, when Ernst Ruska and Max Knoll built the first electron microscope using magnetic fields to focus electron beams. Electrons have much shorter wavelengths than light, so much higher magnification is possible.

The first electron microscopes were not very powerful, but improvements came quickly. Scientists learned to make thin samples, use special stains, and even freeze samples, leading to cryo-electron microscopy, which can now show single atoms in proteins. The electron microscope has helped study viruses like polio and COVID-19 and played a role in vaccines. The story also mentions inventors like Reinhold Rüdenberg and Ladislaus Marton. The article shows that the invention took teamwork, new physics, and clever engineering, but also had problems—electron microscopes are big, expensive, and can only see dead or frozen samples, not living cells in motion.

Comments praise the article’s clear storytelling and note how many people, not just one “genius,” worked on the invention. Some discuss how advances in physics changed biology. There are memories of using these microscopes, and comments that electron microscopy cannot replace light microscopes for live observations. Some wonder if AI or new technology could make electron microscopes more common in the future. Others discuss the human side—how war and politics affected who got credit and who could work. Most agree invention is often messy and shared, and the story inspired many to learn more about the people behind the science.

For our last story, researchers used a regular satellite dish to listen to signals from geostationary satellites and found a lot of sensitive data being sent without encryption. This included phone calls, SMS, internet traffic, military and government information, in-flight Wi-Fi, and even banking records. They picked up unencrypted mobile phone traffic, some military and police data, in-flight Wi-Fi leaks, VoIP call audio, and even control messages for power grids and pipelines.

The main reason for no encryption is cost—encryption uses more bandwidth and needs better hardware. Some companies think the risk is low or do not realize the data is exposed. When researchers reported leaks, some companies like T-Mobile and Walmart fixed the problem, but many others have not responded. Most web traffic is safe because of HTTPS, but calls and texts can be at risk. Users are advised to use VPNs or encrypted apps like Signal. Companies using satellites are told to always encrypt, as if the link was public Wi-Fi.

Hacker News users were shocked that so much important data is still sent unencrypted in 2025. Some wondered why organizations use satellites for sensitive data at all, and why they do not treat satellite links as risky. Others pointed out that encryption can be hard for old systems, but should not be skipped. Some shared their own stories of working with satellite networks and seeing similar problems. There were debates about whether newer systems like Starlink have the same risks, but most agreed all sensitive data should be encrypted. Some worried about privacy and legality, while others were amazed by how simple the equipment was. Overall, the community agreed that satellite security needs much more attention.

That’s all for today’s episode. Thank you for listening to Hacker News Daily Podcast. We’ll see you next time with more stories from the world of technology, science, and programming.