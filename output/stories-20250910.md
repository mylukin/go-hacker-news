# Hacker News 故事摘要 - 2025-09-10

## 今日概述

Today’s top stories are about new tools and updates for developers. There is a new ChatGPT Developer Mode for more control, a tool to run GUI apps in the terminal, and fixes for AI model output differences. Other highlights include a Raspberry Pi handheld computer, an API for meeting transcripts, a faster Postgres storage engine, and new tools for code review and dotfile management. Many stories focus on open source, better debugging, and making developer work easier.

---

## ChatGPT Developer Mode: Full MCP client access

- 原文链接: [ChatGPT Developer Mode: Full MCP client access](https://platform.openai.com/docs/guides/developer-mode)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45199713)

This post talks about a new “Developer Mode” for ChatGPT that gives developers full access to the Model Control Panel (MCP) client. The article explains how this mode lets people change advanced settings and see how ChatGPT works under the hood. It lets users look at model logs, adjust system prompts, and test different model versions more easily. With Developer Mode, you can try out new features before they come out for everyone. The article also says you can use special API endpoints and see more details about each API call. There are tools in the panel to help catch problems and see what messages the model got and sent. Some parts of Developer Mode are for power users, so you should be careful if you don’t know what you’re doing. The documentation gives step-by-step instructions on how to turn on Developer Mode and how to use the new settings. It also explains how developers can use this mode to build better apps and test them faster.

In the comments, some people are excited about having more control over ChatGPT and think it will help them understand and use the model better. Others worry that giving access to advanced features might make it easier to break things or accidentally leak private data. A few users ask if this mode is only for paid users or if everyone can try it. Some developers like the idea of seeing logs and system prompts because it helps with debugging. Others say the new settings might be too complex for beginners. One commenter points out that this could help catch bugs in apps faster, while another wonders if OpenAI will limit or shut down Developer Mode if people misuse it. Overall, most people agree this is a helpful update, but they want OpenAI to be careful with security and privacy.

---

## Show HN: Term.everything – Run any GUI app in the terminal

- 原文链接: [Show HN: Term.everything – Run any GUI app in the terminal](https://github.com/mmulet/term.everything)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45181535)

This project, called term.everything, lets you run any GUI app inside your terminal window, even over SSH. It works on Linux, supports both X11 and Wayland systems, and is written in TypeScript with some C++. 

The tool acts as a special Wayland compositor that sends GUI window graphics straight to your terminal instead of your normal computer screen. You can use it to open apps like file managers, video players, or even games, all inside a terminal. If your terminal supports images (like kitty or iTerm2), the quality can be very high, but performance may slow down. If not, the graphics are made from text, and the look depends on your terminal’s size and resolution. There are fun examples shown, such as watching movies, playing Doom, or running a terminal inside another terminal. The software is still in early stages, so some apps may not work well yet. You can try it by downloading the beta from GitHub. There is detailed info on how it works and how to use it in the project’s README and help files. The project is open source under the AGPL-3.0 license.

People in the comments are both excited and skeptical. Some think it is a cool hack and fun to play with, praising the creativity behind running GUI apps in a text terminal. Others wonder about the real use cases, saying it is more of a technical demo than a daily tool. A few mention it could help when working on remote servers over SSH, where normal GUI forwarding is slow or hard to set up. Some worry about performance, since sending images over the terminal can be slow, especially for complex apps. Others point out that the idea is not new and compare it to older projects that tried showing images or videos in terminals. There are also questions about support for different apps, with some users asking if it can run their favorite programs. A few give technical advice or report bugs they found when testing. Some just enjoy seeing Doom run anywhere, as it is a classic test for these kinds of projects. Overall, the response is a mix of fun, curiosity, and practical questions.

---

## Defeating Nondeterminism in LLM Inference

- 原文链接: [Defeating Nondeterminism in LLM Inference](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45200925)

This article explains why large language model (LLM) outputs are not always the same, even when you give the same input and set the temperature to zero. Many people think this is because computers use floating-point numbers and do math out of order when running in parallel, which can cause small differences. But the real reason is more about how LLM servers group user requests together in batches, and how the size of those batches changes depending on the server’s load.

The article first shows that floating-point math is not always exact, especially when adding numbers with very different sizes. This is called non-associativity: changing the order of addition changes the result. But on its own, this does not make LLM inference random; it just means small differences can happen if the order changes.

Many people think the main problem is parallel computing using atomic adds, where the order can change each time. However, in practice, LLM inference does not use atomic adds in the forward pass, so this is not the main source of nondeterminism. Most of the time, the code is run-to-run deterministic if you give it the same batch of inputs.

The real issue comes from “batch invariance.” When an LLM server gets requests from many users, it groups them into batches. The results you get can depend on the batch size and which other users’ requests are grouped with yours. For example, matrix multiplication and attention operations in neural networks can give slightly different results depending on batch size, even if your input is the same. This is because the GPU runs different reduction strategies depending on the batch size to get better speed.

To fix this, the article suggests making these kernels “batch-invariant.” This means always using the same math order for each individual input, no matter how big the batch is. The article explains how to do this for different LLM operations: RMSNorm, matrix multiplication, and attention. Sometimes, this means giving up a little speed, but it makes the output fully reproducible.

The authors tested their new batch-invariant kernels. They found that with standard (non-invariant) code, generating the same text 1,000 times still gave 80 different results, even at temperature zero. With the new kernels, all 1,000 results were the same. Performance was a bit slower, but not by much.

In the Hacker News comments, some readers liked the deep dive into floating-point math and batching. They agreed that determinism is important for debugging and research. Others pointed out that in many real-world cases, small differences in LLM outputs don’t matter much, since the models are probabilistic anyway. A few said they had run into bugs or failed tests due to this issue, and were glad to see a solution. Some were surprised that atomic adds are not the main problem, and thanked the author for clarifying. There were questions about how well this solution works with very large models and real production systems, and if the speed loss would be a problem. Some readers wondered if libraries like PyTorch and TensorFlow should make batch-invariant kernels the default, or if this should be something users turn on only when needed. A few people shared their own tricks for getting more reproducible results, like always using the same hardware and software versions. Others discussed the trade-off between perfect reproducibility and running models as fast as possible. Some praised the article for not giving up on the problem, and for explaining the details clearly.

---

## The HackberryPi CM5 handheld computer

- 原文链接: [The HackberryPi CM5 handheld computer](https://github.com/ZitaoTech/HackberryPiCM5)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45172058)

This project is about the HackberryPi CM5, a small, handheld Linux computer that uses the Raspberry Pi Compute Module 5 as its heart and a real BlackBerry keyboard. The goal is to make a portable device where you can learn Linux and understand how hardware and software work together.

The HackberryPi CM5 has a 4-inch, 720x720 touch screen, a metal case, and a battery that lasts about 3–5 hours. It supports two USB 3.0 ports, a full-size HDMI port, and an NVME slot for fast storage or AI cards. The keyboard comes from old BlackBerry phones and can be changed to fit your needs. The main chip is a fast quad-core ARM processor. There are two Bluetooth speakers, and you can add an external antenna to help with wireless range, since the case is metal. The battery is 5000mAh, and you can check its voltage using I2C. The device is partly made from 3D-printed parts, and you can find 3D models online to print or change.

You can buy the HackberryPi CM5 as a kit and put it together yourself. The creator is a university student from Germany, and there is a Discord group for help. There are detailed guides for hardware, assembly, and troubleshooting. You can also see many photos and short videos of the device. The project is open-source with files and code on GitHub, and uses the MIT license.

In the comments, some people love the idea and say it brings back the feel of older BlackBerry phones but with modern power. Others are happy to see a real keyboard, saying it’s better for typing than a touchscreen. Some want to use this as a small Linux computer for programming or server work. A few hope for more battery life or a lighter case. There are questions about how well the keyboard works with Linux and if all the keys can be used. Some people think the price might be high, but still see it as a cool project for learning and hacking. Others wonder if it can replace a phone or if it’s more for fun and experiments. A few suggest adding more wireless features or making the design even smaller. Overall, most agree it’s a fun project that shows what you can do with open hardware and Linux.

---

## Launch HN: Recall.ai (YC W20) – API for meeting recordings and transcripts

- 原文链接: [Launch HN: Recall.ai (YC W20) – API for meeting recordings and transcripts](item?id=45199648)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45199648)

Recall.ai is a tool that lets people record and get transcripts from video meetings using an easy API. The article explains how Recall.ai works and why it is helpful for developers and companies.

Recall.ai connects to popular video meeting platforms like Zoom, Google Meet, and Microsoft Teams. It joins meetings as a bot, records the audio and video, and then creates a transcript using speech-to-text. Developers can use a simple API to add these features to their own apps. This means companies do not need to build their own recording and transcription systems from scratch. The API is designed to save time and make it easy to handle meeting data. Recall.ai also helps with privacy and security by letting users control who can access the recordings and transcripts. The product supports different languages and offers real-time or post-meeting transcription. The team shares that setting up Recall.ai takes only a few minutes, and it works well for many use cases, such as sales calls, customer support, and note-taking tools. The company is part of Y Combinator and wants to help more teams use meeting data in smart ways.

In the comments, many people are excited about the idea and see it as very useful. Some developers say it will save them a lot of work. Others are happy they do not need to worry about the hard parts of recording and transcription. A few users ask about privacy and wonder how safe it is to let a bot join their meetings. Some people share that company rules do not allow recording meetings, so they cannot use this service. Others think the service could be too expensive if used for many calls. There are suggestions for extra features, like better speaker identification or summaries of meetings. A few commenters raise concerns about the quality of transcripts, especially for people with strong accents or poor audio. Some discuss legal issues, such as telling everyone in the meeting they are being recorded. Overall, most people think Recall.ai is a good idea and want to try it in their own projects.

---

## OrioleDB Patent: now freely available to the Postgres community

- 原文链接: [OrioleDB Patent: now freely available to the Postgres community](https://supabase.com/blog/orioledb-patent-free)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45196173)

Supabase now owns the OrioleDB storage engine patent and is giving everyone a free license to use it. OrioleDB is a new, faster storage engine for Postgres, meant to replace the old one without any big changes for users.

OrioleDB uses PostgreSQL's pluggable storage system, so you can just swap it in. It is made to work better with modern hardware and the cloud. Benchmarks show OrioleDB can be about five times faster than the usual Postgres storage engine for some workloads. Supabase and the OrioleDB team want to make Postgres stronger and faster, not compete with it.

The project is open source and welcomes help from anyone—whether you use Postgres at work, build tools, or are just interested. The main goals are to make OrioleDB work as a simple replacement for Postgres storage and to change as little as possible in Postgres itself. They want to get OrioleDB’s changes accepted into the main Postgres project, so it can be part of the official Postgres code in the future.

OrioleDB’s license is based on the same open license as Postgres. By sharing the patent freely, Supabase wants to protect the open-source community and stop anyone from using the patent to attack open projects. Their aim is to make OrioleDB better for everyone, with clear guides, strong benchmarks, and open discussions. Anyone can try OrioleDB or contribute by reporting issues, writing code, or giving feedback.

In the Hacker News comments, many people are happy about the open patent and think it will help the Postgres community. Some users warn that patents can still be tricky for open source, even with a free license. Others point out that real integration into Postgres may take a long time, since the main project moves slowly and carefully. Some are excited about the big performance gains, but want to see more tests in real-world situations. A few are worried about changes needed in Postgres to support OrioleDB and hope it won’t cause problems for other extensions. There are questions about how much support OrioleDB will get in the long term. Several users praise Supabase for being open and for sharing the patent instead of keeping it private. Overall, most agree this is good news for Postgres and open source, but want to watch how it develops over time.

---

## Mux (YC W16) Is Hiring Engineering ICs and Managers

- 原文链接: [Mux (YC W16) Is Hiring Engineering ICs and Managers](https://mux.com/jobs)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45203643)

Mux is a company that helps developers add video to their apps and websites. They are looking to hire engineers and managers to join their team, which is known for working on hard video technology problems like encoding, streaming, and monitoring.

The article explains that Mux wants to make video easier for everyone, not just big companies. Their team has experience from top tech companies like Google and YouTube, and they have built popular tools like Video.js. Mux works with many well-known clients such as Reddit, Vimeo, and TED. The company has strong investors, including Y Combinator and Andreessen Horowitz. Mux values a healthy and diverse workplace, encourages growth, and aims to make customers into fans. They offer jobs for distributed systems engineers, video software engineers, and engineering managers, both in San Francisco and for remote work. Benefits include good health insurance, flexible time off, family support, and professional development. Mux also supports remote and in-office work, no-meeting Thursdays, and fun perks like lunch and equipment reimbursement. They say they are an equal opportunity employer and welcome people from all backgrounds.

In the Hacker News comments, some people say they respect Mux for their open-source work and for organizing Demuxed, a video engineering event. Others mention that the video stack is hard and Mux’s tools are helpful for smaller teams that can’t build video systems themselves. A few commenters talk about the challenges of scaling video streaming and the technical skills needed for these jobs. There are questions about the company’s remote policy and whether international candidates can apply, with some wishing there were more options outside the US. One person shares a positive experience working with Mux’s support team, saying they are very responsive. Others discuss the job listings, noting they focus on senior roles, and wonder if there are entry-level openings. Some users praise Mux’s benefits, especially the professional development support and family-friendly policies. There are also a few discussions about the competition in the video space, and how Mux compares to larger providers like AWS or Cloudflare. Lastly, some people note the importance of diversity and good communication, which Mux highlights in their company values.

---

## Show HN: Haystack – Review pull requests like you wrote them yourself

- 原文链接: [Show HN: Haystack – Review pull requests like you wrote them yourself](https://haystackeditor.com)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45201703)

Haystack is a new tool that helps software developers review pull requests more easily and quickly. It aims to make the code review process faster and less painful for everyone on a development team.

The main idea behind Haystack is to give reviewers all the information they need in one place. Instead of jumping between files, issues, and documentation, Haystack shows you the context right inside your code review. This means you can see how the code changed, why it changed, and even get background information without leaving the review page. Haystack tries to make reviews feel like you wrote the code yourself, so you understand it better and can spot problems faster. It also promises to reduce the back-and-forth between reviewers and authors by making it easier to leave clear comments and suggestions. The tool is made to fit into your current workflow, so you don't have to change how you use GitHub or other platforms. The makers say Haystack is fast, lightweight, and easy to set up, so teams can start using it without a lot of hassle. They also claim it helps teams ship better code by catching mistakes early and improving communication.

In the Hacker News comments, some users are excited about anything that makes code review easier. They mention that reviews are often slow and hard, so a tool like Haystack could help teams move faster. Others are curious about how Haystack collects and shows context—some worry it might miss important details or become cluttered with too much information. A few developers say that tools like this work best if the team already has good habits, like writing clear pull requests and good commit messages. Some people ask if Haystack will work with private code or if it’s safe to use with sensitive projects. Others note that similar tools exist but still hope Haystack brings something new. There are also comments about pricing—developers want to know if there’s a free tier or if it’s open source. Some users are skeptical that a new tool can really fix the hardest parts of code review, but many agree it’s worth trying new ideas to make teamwork better.

---

## Dotter: Dotfile manager and templater written in Rust

- 原文链接: [Dotter: Dotfile manager and templater written in Rust](https://github.com/SuperCuber/dotter)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45202252)

Dotter is a tool written in Rust to help you manage your dotfiles, which are files that store your computer’s settings. The project makes it easier to set up these files across many machines and to handle changes for each system.

Normally, people keep dotfiles in a Git repository and use links to point them to the right place, but this can get messy. Dotter fixes this by letting you organize your files with simple config files, so you know what each file does and where it goes. It can automatically create links, replace files, or even change how files look, using templates. If you have different setups for a laptop and a desktop, Dotter can handle that with custom rules. It also helps you set up a new computer fast, since it does the linking and copying for you. You can install Dotter on Mac (using Homebrew), Arch Linux (from the AUR), Windows (with Scoop), or by downloading it directly. The tool has helpful commands: for example, you can use ‘deploy’ to set up your files, ‘undeploy’ to remove them, and ‘watch’ to keep things in sync if you change your configs. It supports hooks, so you can run commands before or after changes. There’s also a ‘dry-run’ mode to see what would happen without making changes, and you can turn on more detailed messages if you want.

In the comments, some people like that Dotter is simple and works across different systems, not just Linux. Others say its setup is easier to understand than older tools like GNU Stow. A few users mention they switched to Dotter because it handles templates and custom setups well, which saves time. Some say they wish for better documentation or examples, but the wiki helps. There’s a discussion about how Dotter compares to other tools, with some saying it’s faster thanks to Rust. Others prefer different styles—like just using scripts or linking by hand—because it feels more flexible. One person points out that Dotter’s config files are easy to read, which is good for teams. Another mentions that it’s nice to see an active open-source project with lots of updates. Some users wonder if Dotter can manage secrets or passwords, while others say you should keep those separate. In summary, people think Dotter is modern, fast, and helpful, but as always, it depends on what you need for your setup.

---

