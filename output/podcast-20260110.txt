Hello everyone, this is the 2026-01-10 episode of Hacker News Daily Podcast. Today, we have stories about self-evolving code, AI linking books, tech parody, debugging memory leaks, open source business under pressure, the loss of a weather app, surprising animal behavior, ASCII UI design, and concerns about privacy in health AI. Let’s get started.

First, let’s talk about OpenChaos.dev, a new open-source project that calls itself “self-evolving.” Anyone can suggest or vote on code changes. The main rule is simple: there’s a countdown timer, and when it ends, the most-voted pull request gets merged, no matter if it’s serious or silly. Some PRs add features, change designs, or just make jokes. For example, people have proposed rewriting the whole project in Rust, making the site look like old GeoCities, or scrambling the page every 10 seconds. Others add fun effects like snow, pointer trails, or a “Hall of Chaos” for past winners. Technical PRs include showing PR age, adding vote bars, or health checks, but there are also meme PRs, like always using light mode in dark mode or adding a “dickbutt” image. Each merge changes the project, sometimes in useful ways, sometimes just for fun, and new changes can build on or undo old ones.

In the comments, some people love the chaos and call it a fun experiment in community-driven software. They like that everyone can vote, not just argue about code style. Others worry that too many silly changes could make the codebase messy or useless. There are jokes about how quickly the site will break. Some compare it to “Twitch Plays Pokémon,” saying it’s a creative way to learn about open source teamwork. A few developers discuss problems like merge conflicts or security. Some wonder if this approach could work for serious projects, or if it’s only good for throwaway sites. Most agree it is fun to watch, even if they would not use the code for anything important. Some suggest adding rules or moderation, but others say chaos is the point. OpenChaos.dev is a playful, new way to think about open-source, and people are curious to see what the internet does with it.

Next is Trails, a project that uses the Claude Code AI tool to find connections between 100 books. The website groups book ideas by themes, so you can see how certain concepts repeat across different books. Each “trail” collects summaries and shows common ideas. For example, there are trails about self-deception, group knowledge, project failure, innovation by copying, isolation for new ideas, and patterns like “winner’s lock” or “simplification trap.” Technical trails include how systems fail or how measurements build trust.

Claude Code read the book texts and suggested these links, making a map of ideas that goes beyond simple summaries. This helps people see how lessons from one book show up in others. In the comments, many people like how the tool helps with “syntopic reading”—finding links across many books. Some say it’s a good way to discover new books. Others wonder about the quality of the AI’s summaries and links, or if the author checked for mistakes or bias. Some wish the site had more books or let users add their own trails. A few say human curation is better for deep links, but AI is great for a first pass. There’s talk about using this method for research or schools. Most like the creative use of AI, but see both its strengths and limits.

Now, let’s look at “Worst of Breed,” a website that makes fun of bad software and over-complicated tech projects. The site uses fake testimonials and silly examples to show how some teams choose complex, flashy solutions instead of simple ones. The main joke is about “resume-driven development,” where engineers pick new tools just to look good, not to solve real problems. Examples include using blockchain for cookies, splitting small scripts into microservices, or rewriting a simple page in Rust to call it “web scale.” The site lists fake “design patterns” that make software hard to maintain, like using a database for messaging or running six versions of React. The manifesto says they value complexity over simplicity, making fun of how tech teams sometimes work.

Hacker News commenters laughed, saying the jokes felt too real and shared their own stories. Some said business or politics, not just developers, push for complex solutions. Others noted that new tech is good if used right, but most projects should stay simple unless they need to scale. Some worried newbies might copy these bad habits. Many liked the humor and said it’s a good reminder to focus on real value, not just trendy tools. Some wanted even more examples, while others joked they had worked at companies just like those on the site. The comments showed both laughter and real concern about growing software complexity.

Our next story is about Ghostty, a terminal emulator that had a big memory leak, using up to 37 GB of memory after running for days. The problem was tricky and is now fixed in the next release. Ghostty uses a special memory system called PageList. Most of the time, it uses standard-size memory pages, but sometimes it needs larger, special pages for things like emojis or links. When the terminal history got too big, Ghostty tried to reuse the oldest page. If it was a normal page, this worked fine. But if it was a special page, there was a bug: Ghostty only changed its label, not freeing the extra memory. Later, when deleting, it skipped the needed delete step, so memory leaked.

This only happened in rare cases, usually with command-line tools that produced complex outputs. As more users tried these tools, the leak became easier to notice. The developer fixed the bug by making sure special pages are never reused—they are always deleted properly. They added better memory tracking on macOS and a test so the leak won’t return.

In the comments, people praised the clear write-up and the simple fix. Many said memory bugs are common, even in well-tested code. Some suggested more advanced tracking, but most agreed simple is better. Developers shared their own stories with similar bugs, and some liked the use of macOS memory tagging. Others said real-world tests are important, since this bug did not show up in normal unit tests. Many thanked the community for reporting details and helping to find the exact problem.

Next, we have a story about how new AI tools are making it hard for some software companies to make money, using Tailwind Labs as an example. Tailwind Labs had to lay off most of its engineers because fewer people visit their docs—AI can now answer developer questions directly. The company used to earn money when users visited their site and bought Tailwind Plus, a paid set of UI pieces. Now, AI gives answers and code, so fewer developers visit the site, breaking their sales model.

The article says AI can easily copy things that are clearly described, like docs or code samples. AI was trained with Tailwind’s docs and can now give that info to users directly. The writer thinks this is unfair, because AI companies don’t pay the original creators.

The article explains that real value now comes from things you can’t just write down—like running servers, keeping apps working, and fixing problems. Open source code is just a pathway to services. Some open source projects, like CSS tools, are harder to turn into strong businesses because AI can easily copy them.

In the comments, people agree AI changes how docs and code are used. Some say companies need to add value beyond simple code or docs. Others think it’s unfair for AI to use free content without giving back. Some suggest companies should focus on things AI can’t do, like support or hosting. A few say open source was never a great business for small tools, and AI made the problem worse. Some worry that if AI keeps taking from open source without helping, fewer people will want to share. Others think open source will adapt. There’s hope that more open source businesses will still succeed, even if they need to change how they work.

Now, let’s talk about the end of the Dark Sky weather app and why people miss it. Apple bought Dark Sky in 2020 and shut it down in 2023, saying its tech would go into Apple Weather. But users feel something important was lost. Dark Sky was loved for its design and clear data visualization. It showed weather from the current moment, not old data. You got local, real-time information, and could quickly see when rain would start or stop, how temperatures would change, and more.

The app used simple visuals: arrows for wind, color scales for storms, and “temperature pills” showing real differences. Sometimes it used rough categories like “light” or “heavy” instead of numbers. It was also context-aware, showing the most important details for your needs at that time.

Fans say Apple Weather is not as clear or helpful. Some miss the special rain graph, and others feel Apple’s app is harder to use or missing key details. In the comments, users remember how Dark Sky made weather data feel alive and easy. They liked the focus on real-time, local info and simple design. Some hope future apps will use Dark Sky’s ideas to make data more useful. There’s talk about how hard it is to replace a tool that fits daily routines. Some note that good data visualization makes a big difference, not just for weather apps. Overall, people agreed Dark Sky set a high bar for clear, helpful design.

Now to science: researchers in Germany have filmed brown rats catching flying bats in mid-air, something never seen before. The team used cameras in a cave with thousands of bats. They saw rats waiting on ledges, then jumping to grab bats as they flew by. In five weeks, they saw 13 kills and found a pile of 52 bat bodies, showing the rats were real hunters, not just eating dead bats.

Brown rats usually eat slow or easy prey, not flying animals. Here, they used two hunting styles: jumping to grab bats at the cave entrance, and attacking bats crawling on the ground. Scientists think rats use their whiskers and hearing to find bats in the dark. This ability to learn new ways to hunt is called “behavioral plasticity.” The crowded cave with many bats made a perfect place for rats to learn.

This is worrying for bats, because rats are invasive and could hurt bat numbers, especially since bats already face other problems. The study suggests controlling rat numbers near big bat roosts. The most amazing part is seeing regular city rats acting like skilled hunters, showing how animals can surprise us.

In the comments, people were shocked that rats could catch flying bats, calling it “nature is metal.” Some said rats are clever and will eat almost anything. Others pointed out this shows how invasive rats are a danger to wildlife, not just in cities. Some worried this will make it even harder for bats to recover. Others joked about city rats learning “ninja moves.” Some wondered if this happens elsewhere but has not been filmed. Many said it’s a good reminder to watch and study even common animals, because they can surprise us.

Next, let’s talk about using ASCII art—simple text characters—to design user interfaces before making them in code or design tools. The writer explains that UI design tools have gone from rough sketches to very polished tools and even AI that builds whole UIs. But new AI tools can make UIs look finished too early, making teams argue about details before the structure is ready.

To fix this, the author uses “ASCII-Driven Development”—drawing wireframes with just keyboard characters. This keeps things simple, so people focus on structure and ideas, not colors or fonts. ASCII designs are quick, easy to change, and anyone can edit them, even without design tools. The article shows examples for dashboards, product pages, Kanban boards, and more. AI models are good at making and changing ASCII layouts, and once the layout is right, you can send it to code-generating tools.

ASCII layouts are easy to share, track in version control, and edit by anyone on the team. The article says ASCII-Driven Development helps teams move faster and focus on what matters first: structure and flow. Only after that should you move to detailed design.

In the comments, many people like the idea. Some say ASCII reminds them of how they used to draw UIs on whiteboards or paper. Others say ASCII layouts are easy to share in code reviews or docs. A few worry that ASCII is not great for very complex layouts, or that some people can’t “see” the final product from text. Some already use similar low-fi methods, like drawing in Markdown. Some ask about tools to convert ASCII to code. A few say ASCII can be a barrier if team members are not comfortable with it, but most agree it’s easier than learning Figma. One person joked that ASCII is the “universal design language” for developers. Another said it helps keep design and development closer. Overall, many showed interest in trying this method, especially for early brainstorming.

Finally, let’s talk about ChatGPT Health, a new product from OpenAI that helps people manage health questions and connects to their medical records and wellness apps. OpenAI says privacy is a big focus, but the article warns that in the past, OpenAI has not protected user privacy well. On most ChatGPT plans, privacy is not automatic—you have to pay more for better protection. With ChatGPT Health, OpenAI claims strong privacy, but the writer is suspicious, saying OpenAI is losing money and might collect health data to sell services.

ChatGPT Health links with apps like Peloton and Apple Health, and uses a company called b.well to connect to your medical records. b.well mainly works for insurance companies, not regular people. This means ChatGPT Health is not just giving advice, but building a system where insurers and brands can reach users directly. Your health data becomes a product that others can buy or use.

US law called HIPAA protects health data at the doctor’s office, but not when you give it to a tech company like OpenAI. This means OpenAI can set its own privacy rules and change them anytime. ChatGPT Health is not available in the EU, UK, or Switzerland, which have strong privacy laws.

In short, the article warns: OpenAI says you are the customer, but really, your health data may be the product for sale.

In the comments, some people agree and say this is another example of “surveillance capitalism”—making money from private data. They worry about sharing health details with a tech company, especially one under financial pressure. Others add that tech companies often change privacy policies, so you can’t really trust them. Some say US health privacy laws are not strong enough. Some are upset that health data could affect insurance costs or be used for targeted ads. A few note that many people already share health info with apps, and the real problem is weak regulation. Some think ChatGPT Health might help those confused by the US healthcare system, but still worry about data use. One person notes that skipping Europe shows OpenAI knows it can’t meet their privacy standards. Some hope for open-source or offline AI health tools. Others wonder if this will push lawmakers to create better privacy laws, or if people will just accept the risks.

That’s all for today’s episode. Thank you for listening to Hacker News Daily Podcast. We’ll be back tomorrow with more stories from the world of software, technology, and science. Have a great day.