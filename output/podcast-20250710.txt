Hello everyone, this is the 2025-07-10 episode of Hacker News Daily Podcast. Today, we have stories about scaling problems in Postgres, the final report on the Alaska Airlines door plug accident, new AI tools for your phone and browser, a blog that draws linear algebra, a new encrypted git service, Bret Victor’s Dynamicland, the launch of the Flix programming language, a voice AI startup, and a study showing how AI affects real open-source developers. Let’s get started.

First, let’s talk about a scaling problem with Postgres’s LISTEN/NOTIFY feature. The team at Recall.ai found that when they used LISTEN/NOTIFY with many bots writing to the database at the same time, Postgres became very slow and sometimes stopped working. They found out that every time a NOTIFY command is used in a transaction, Postgres locks the whole database during the commit. Only one commit can happen at a time if NOTIFY is called, making all other processes wait. During high load, the team saw many queries waiting for an AccessExclusiveLock, which is hard to notice and not well known. By checking logs and reading the Postgres source, they learned this lock is by design, to keep NOTIFY events in order. As a test, they removed NOTIFY, and the database ran fine. Their final fix was to move notifications to HTTP requests instead. 

In the comments, many agreed that LISTEN/NOTIFY is not for high-scale systems and shared their own scaling stories. Some pointed to external pub/sub tools like Redis or Kafka as better choices. A few said Postgres’s global lock is needed for order, but wished for finer controls or better docs. There was debate about documentation, with some saying this limit is not clear enough. Many users switched to other tools after hitting this wall, and most agreed that external systems are better for high-scale event notification.

Now, let’s look at the final report on Alaska Airlines Flight 1282. In January 2024, a door plug blew out of a Boeing 737-9 MAX soon after takeoff, causing a loud noise and a hole in the side of the plane. The crew acted quickly and landed safely, with only minor injuries. Investigators found that bolts meant to hold the door plug were missing, because Boeing did not give enough training or clear instructions to workers. The FAA’s checks also failed to catch the problem. The report says both Boeing and the FAA must improve how they build and check planes. It recommends safer door plug designs, better tracking, more training, and a review of Boeing’s safety culture.

In the comments, people said Boeing has deep problems with quality control and blamed cost-cutting and weak management. Others said the FAA should have done more. Some shared airline industry stories and said mistakes like missing bolts should never happen. A few praised the flight crew. There were questions about whether other planes have the same risk, and calls for more transparency. Some think the problems are bigger than just Boeing and need industry-wide changes. Most agree that strong action is needed now, even though air travel is still safer than most other options.

Next, we have news about Cactus, a new app that lets you run large language models like Llama right on your phone, without the cloud. Cactus works on both Android and iOS. You can pick from models like Llama 3 or Phi-3, and everything runs locally—no internet needed, and your data stays private. The app is open-source, supports voice chat, and is designed to be fast and save battery by using special phone hardware. Some big models need a lot of memory, so the app may work better on newer phones. The idea is inspired by Ollama, which runs AI on personal computers.

Comments were positive about privacy and control, and many liked that Cactus is open-source. Some worried about speed and battery drain, and a few said their older phones struggled with big models. There was interest in more models, and some developers discussed updates and security. A few raised questions about the risks of letting anyone run strong AI on their own device. Overall, people are interested and hope the app keeps improving.

Another tool in the news is BrowserOS, an open-source web browser with local AI agents built in. BrowserOS is a fork of Chromium, so it looks and works like Chrome and supports all Chrome extensions. The big idea is privacy: AI agents run on your own computer, not in the cloud. They can automate tasks like scheduling meetings or filling forms, and there’s even an AI-powered ad blocker. The browser uses Ollama for local AI, and has features like a highlighter, smart bookmarks, and the ability to summarize or search through your browsing history. You can ask BrowserOS to research topics or find posts on sites like LinkedIn and Twitter. It’s available now for Mac.

On Hacker News, people liked the privacy and local control. Some asked about security, since giving a browser so much power could be risky. Others compared BrowserOS to tools like Perplexity Comet and said open source and local AI are big pluses. There were questions about memory and CPU use, and whether the ad blocker is as good as existing tools. Some want Windows or Linux versions. There’s debate about whether AI in the browser is useful or just adds complexity, but many are curious and want to know more about real-world use.

Let’s turn to something different—a blog called Graphical Linear Algebra. This blog explains linear algebra using drawings instead of just numbers and formulas. Each episode shows how math ideas like adding or copying can be seen with simple diagrams. Later episodes go into matrices, fractions, feedback loops, and even category theory, all with pictures. The author invites readers to join research or help translate the blog, and the goal is to help people “see” how math works.

Comments show readers find the blog helps them understand linear algebra better. Engineers say they use similar diagrams at work. Some ask about PDFs or translations, while others ask technical questions about how the diagrams connect to computer science. A few share their own projects or want more posts about special topics. People like the clear, episode-based style, and wish for more updates.

Now, let’s talk about FOKS, a new open-source service for end-to-end encrypted git and key-value storage. FOKS is designed to be safe even against quantum computers, using post-quantum cryptography. All your files and even file names are encrypted before they leave your device. FOKS is federated, so anyone can run a server, and teams can work together across different servers. The system uses simple but strong cryptography, with device keys, user keys, and team keys. Changes are tracked with signature chains and Merkle trees, so you can trust the server.

FOKS is open-source, self-funded, and tries to avoid vendor lock-in. You can run your own server, or use the official ones. Comments are positive about the open and federated design, with many comparing FOKS to Keybase. Some like the post-quantum security, while others wonder if it adds too much complexity. There are questions about setup and risk if you lose device keys. Some are interested in using FOKS for password management or sharing private data. Users say trust depends on the code being truly open. There’s also doubt about whether companies will switch away from GitHub, but most agree more private and secure options are good.

Next, let’s look at Bret Victor’s Dynamicland project. Dynamicland is a physical space and system called Realtalk, where people work together with real objects, not just screens. The idea is to make a “humane dynamic medium”—a way for anyone, not just experts, to use computers by arranging cards, objects, and drawings. Everything is open and visible, and there’s no black box. Dynamicland is against closed, mass-produced systems, and wants communities to build and adapt their own tools. 

The FAQ explains that current AI is the opposite: it is a black box, hard to understand, and makes people dependent. Dynamicland wants tools that help people learn, not just do things for them.

Comments showed support for Bret Victor’s vision, with some saying today’s tech is too closed and hard to understand. Others think Dynamicland is too idealistic or hard to scale. Some developers are excited by mixing real objects and computers, but wonder if it can be as powerful as normal software. There are also privacy questions, but Dynamicland does not track people. Many like the idea of open, understandable computing, but doubt it can compete with big AI systems today.

Now, for developers, there’s Flix, a new programming language focused on managing side effects. Flix mixes ideas from functional, imperative, and logic programming. It tracks effects like file reading or variable changes, and lets you define your own effects and handlers. Flix uses algebraic data types, pattern matching, extensible records, and traits. It can tell if code is pure or not, and handles effects in a clear way. Flix supports Datalog, a logic programming language, and runs on the JVM, so you can use Java tools and libraries. It compiles code in parallel and has a strong standard library and VS Code integration.

Comments are excited about the effect system and the mix of functional and logic programming. Some compare Flix to Haskell, OCaml, and Scala, and like the clear tracking of purity and custom effects. There’s interest in Datalog, but some wonder about the learning curve and real-world adoption. Performance is a question, since Flix runs on the JVM, but others like the JVM integration. Some worry that the language may be complex, but others think the features are good for advanced users. The community seems positive and interested.

Leaping is a new company making a voice AI that learns and improves over time. The AI listens to users, tries to understand what they want, and learns from mistakes without needing special rules or code. The team says their system is fast, works for many types of businesses, and can handle both phone calls and chat. Companies can add their own info, and Leaping hopes to help businesses give better help to customers. Leaping is part of Y Combinator and looking for early users.

Comments show people are interested in how Leaping’s AI learns. Some are excited about not needing rules, but others wonder if the AI can really understand tricky questions or just memorize answers. There are worries about privacy, support for different languages and accents, and if companies will trust a new AI. Some think it could be good for doctor’s offices or small shops. Many wish the team good luck.

Finally, we have a new study about how AI tools affect experienced open-source developers. The main finding is surprising: using AI made skilled coders 19% slower on real tasks. The study was fair—16 top developers worked on real issues, sometimes with AI tools and sometimes without. Developers thought AI would make them faster, and even after seeing the slowdown, they still believed AI helped. The study checked many reasons, but the result was clear: for complex, high-quality open-source work, AI sometimes gets in the way.

Comments showed surprise and some skepticism. Some said this matches their own experience: AI is good for small scripts or learning, but not for deep work. Others argued that AI tools are still new, and developers need more practice to use them well. Some pointed out that benchmarks and real-world stories can both be misleading, and careful studies like this are needed. Most agree that the impact of AI on real coding is still an open question and changing fast.

That’s all for today’s Hacker News Daily Podcast. Thank you for listening, and see you next time.