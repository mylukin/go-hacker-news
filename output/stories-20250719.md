# Hacker News 故事摘要 - 2025-07-19

## 今日概述

Today’s top Hacker News stories talk about better ways to back up your data, using offline AI and Wikipedia, and new computer chip factories. There are also stories about fast travel, old UNIX computers, and new medical methods to prevent disease. Security is a big topic, with deep looks at hardware attacks and problems in AI tools. Many readers share personal stories, worries, and ideas for the future.

---

## Make Your Own Backup System – Part 1: Strategy Before Scripts

- 原文链接: [Make Your Own Backup System – Part 1: Strategy Before Scripts](https://it-notes.dragas.net/2025/07/18/make-your-own-backup-system-part-1-strategy-before-scripts/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44618687)

This article talks about how to make a strong backup system for your data, focusing first on planning before writing any backup scripts. The author says many people do not think enough about backups and often make mistakes that only become clear when it is too late.

The article warns that just copying files is not a real backup, especially for things like live databases, because these copies might not work when you try to restore them. It is important to make backups that you can actually use, in open formats, and test them regularly. The author gives examples from his own experience, like losing servers to fire, flood, and ransomware, but being able to recover quickly because of good backups.

Before starting backups, you should ask yourself what data you need to protect, how much risk you can take, and how much downtime you can accept. Where you store your backup is also important; keeping it on the same machine is risky because if the machine fails, you lose everything. Backups stored far away are safer but can be harder and slower to use when needed.

There are two main ways to back up: full disk backups or backing up individual files. Full disk backups are good for fast recovery and work well with virtual machines, but they use more space and might need you to stop the computer for a while. Individual file backups are more flexible and can run while the system is live, but they need careful planning and may also use a lot of space.

To make sure backups are reliable, the article says to use snapshots before copying data. Snapshots freeze the system at a single point in time, so the backup is always consistent. The author talks about different snapshot tools like ZFS, BTRFS, LVM, and DattoBD, and some problems he has seen, like system freezes with LVM under heavy load.

Another big question is whether the backup should be "pushed" from the client to the backup server, or "pulled" by the server. The author prefers "pull" backups with a secure, dedicated server, so that if the client is hacked, the backups are still safe. He also suggests using snapshots on the backup server itself to protect against problems like ransomware.

A good backup system, in his view, should allow fast recovery, use outside storage, be secure, manage space well, and be easy to use.

In the comment section, many readers agree that too many people forget to test their backups until it is too late. Some share stories about losing data because they trusted simple file copies, while others say snapshot tools like ZFS or btrfs have saved them. A few readers argue that cloud providers should do more, but others remind everyone that the cloud is not a backup by itself. Some like the "pull" backup method for security, but one person warns that if the backup server is hacked, all data could be lost anyway. Others talk about balancing speed, cost, and safety, and say there’s no one-size-fits-all answer. Many agree with the author that the most important thing is to have a backup plan, test it, and make sure you can actually restore your data when needed.

---

## Local LLMs versus offline Wikipedia

- 原文链接: [Local LLMs versus offline Wikipedia](https://evanhahn.com/local-llms-versus-offline-wikipedia/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44617078)

This article talks about the idea of using local large language models (LLMs) and offline versions of Wikipedia in situations without internet, like during a disaster. The author compares the file sizes of different LLMs you can run on your own laptop to the sizes of offline Wikipedia downloads.

The author lists out several LLMs from the Ollama library and different Wikipedia downloads from Kiwix. For example, the "Best of Wikipedia" (50,000 articles, no details) is about 356 MB, while a small LLM like Qwen 3 0.6B is 523 MB. The full Simple English Wikipedia is 915 MB, and bigger models like Llama 3.2 3B are about 2 GB. The biggest full Wikipedia download is over 57 GB, while some of the largest LLMs are around 20 GB.

The author points out that this comparison is not exact because LLMs and encyclopedias work in different ways. LLMs generate text and can answer questions, but they may not always be accurate. Wikipedia is a collection of facts and articles, but you have to look up information yourself. LLMs also need more computer power and memory, while offline Wikipedia works on older, slower machines.

There are also different ways to use both tools. You can pick only the Wikipedia articles you care about, or use a smaller LLM that runs well on your computer. The author admits the choices in the comparison are not scientific, but just interesting to think about.

One fun point the author makes: the best 50,000 Wikipedia articles are about the same size as a 3-billion-parameter LLM. Sometimes, Wikipedia is smaller than even the smallest LLM, and sometimes it’s much, much bigger.

In the comments, people share many ideas. Some say that Wikipedia is more reliable for facts, while LLMs can make mistakes or even make things up. Others point out that LLMs are more flexible—they can help you search, summarize, or explain things in different ways, not just repeat text. A few people mention that running LLMs offline still needs a good laptop, while Wikipedia can be stored easily on old devices and even phones.

Some users think it's smart to have both: Wikipedia for facts, and an LLM for help with understanding or explaining. Others worry about the trustworthiness of LLM answers, especially if you need exact information. A few mention that LLMs can be trained or tuned for special topics, while Wikipedia is more general. Some people talk about storage space and say that for most people, downloading just a part of Wikipedia or a small LLM is enough. Others think that in an emergency, simple tools that work everywhere are better than fancy ones that need a lot of power. In the end, most agree it depends on what you need: facts, explanations, or both.

---

## Trigon: Exploiting coprocessors for fun and for profit (part 2)

- 原文链接: [Trigon: Exploiting coprocessors for fun and for profit (part 2)](https://alfiecg.uk/2025/07/16/Trigon.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44618959)

This article talks about improving a kernel exploit called Trigon to work on more Apple devices. The main goal is to get full control over the system by using special hardware pieces called coprocessors.

The author first explains how Trigon originally worked on A10 devices by using a hardware security feature called KTRR. KTRR protects part of the system’s memory so only safe code runs. But older devices use different protections, and newer ones add more layers, so the exploit needed updates. On A10 and A11, the author found a way to find the kernel’s memory location by using something called IORVBAR, which tells where the processor starts after a reset. This helps find the start of the protected region safely.

On even older devices like A9, this method doesn’t work because that register points to a special secure area called TrustZone, which is off-limits to the exploit. To get around this, the author and a friend tried attacking a different chip inside the device, called the Always-On Processor (AOP). The AOP runs its own code and controls some special tasks, and its memory is easier to reach.

They figured out how the AOP’s memory was set up, even though some addresses looked strange at first because of a trick called AXI remapping. By overwriting the AOP’s code, they could make it run their own code every time a certain function was called. This let them read and write any value in the device’s memory, which is very powerful for exploiting the system.

The author gives lots of code examples showing how to map memory, install hooks, and read or write data. They also mention how these tricks don’t work on the oldest devices, but on those, the kernel’s location does not move, so other methods are possible.

In the comments, some people are amazed at the level of detail and skill shown in this research. They say this kind of work is very rare and valuable for understanding deep system security. Others point out how Apple’s use of coprocessors and memory protections has changed a lot over time, making these kinds of exploits more difficult every year. A few readers ask technical questions about the AXI remapping, wondering how common this is in other devices. Some are worried about the security risks if this information falls into the wrong hands, but others argue that public research helps improve safety for everyone. One user is surprised at how the exploit uses the AOP as a way around normal protections, while another thinks it shows why hardware security is so tricky. There is also some discussion about how these methods compare to older jailbreaks, and if future devices will close these gaps even more. Overall, most readers are impressed and thank the author for sharing so much technical knowledge.

---

## TSMC to start building four new plants with 1.4nm technology

- 原文链接: [TSMC to start building four new plants with 1.4nm technology](https://www.taipeitimes.com/News/front/archives/2025/07/20/2003840583)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44618762)

TSMC is going to build four new factories in Taiwan to make very small computer chips using 1.4-nanometer technology. These factories, called Fab 25, will be in the Central Taiwan Science Park and are planned to start making chips by late 2028.

The article says TSMC has already leased the land, and the science park has handed over the area for building. Before building the factories, they will make water ponds and do work to protect the soil and water. The first factory will test production in 2027 and then begin making chips in large numbers in 2028. Each month, the first factory hopes to make 50,000 wafers. The park director thinks this project will help the area earn more than NT$1.2 trillion (about US$40 billion) each year. TSMC also shared plans to use this new technology, called A14, in 2028. Besides Taiwan, TSMC is putting a lot of money into new factories in Arizona, USA. By the time these US plants are done, about 30% of TSMC’s most advanced chip production will be in Arizona. TSMC is preparing to build more chip factories and research centers in both Taiwan and the US to meet high customer demand.

Hacker News commenters have different views on this news. Some are very impressed by how small 1.4nm chips are, saying it shows how fast technology is moving. Others worry about how difficult and expensive it is to make such tiny chips, and if it is really needed for most products. Some people talk about the huge amount of money TSMC and the US are spending. Others say this is good for the US because it means more chip production happens outside Asia, making supply chains safer. A few are concerned about the impact on the environment, especially water use, since chip factories need lots of water. Some believe only a few big companies will be able to use these very advanced chips, while most devices will still use older, cheaper chips. Others discuss the skills and people needed to run these new factories and wonder if there will be enough workers. Some think this is a sign that Taiwan will remain important in the tech world, while others feel the US is catching up quickly.

---

## The Future of Ultra-Fast Passenger Travel

- 原文链接: [The Future of Ultra-Fast Passenger Travel](https://spaceambition.substack.com/p/beyond-the-sound-barrier)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44619258)

This article talks about the future of very fast passenger travel, like flying faster than the speed of sound. It explains how new companies and big aerospace firms are trying to build planes and rockets that can travel much faster than today’s jets, hoping to make long trips much shorter.

The Concorde was the last supersonic passenger jet, but it stopped flying in 2003 because it was too expensive, used too much fuel, and faced rules banning loud flights over land. Now, new ideas use better materials, new engines, and cleaner fuels to try to solve these problems. Traveling faster than sound is called supersonic (Mach 1–5), and even faster is hypersonic (Mach 5+). For hypersonic or rocket travel, planes must go very high where air is thin, so there’s less drag and heat. Rockets, like SpaceX’s Starship, go above most of the air, travel at amazing speeds, and then come down in a big arc, needing a lot of fuel to launch and land.

Modern passenger planes use big engines called turbofans. These are fuel efficient but can’t go supersonic because the fans become too much drag. Supersonic planes need smaller engines or special engines like ramjets. Faster planes always use more fuel per person for each kilometer. For example, a Boeing 787 uses about 1.3 kg of fuel per passenger per 100 km, Concorde used about 10.3 kg, while rockets or hypersonic jets can use up to 30 kg or more. This means faster flight is much less fuel efficient, and also worse for the environment, because it releases more CO₂, water vapor, and other gases at high altitudes.

Because of the high cost and bigger environmental impact, these fast flights will likely be used mainly by business travelers, luxury tourists, or the military—not for regular people. For instance, Boom Supersonic plans tickets for its future supersonic jet to cost $4,000–$5,000, like business class today, but early prices may be even higher. The market for regular supersonic flights could still be $30–$60 billion, but companies like Boom need billions more dollars to finish building and certifying their planes. There are also rules today that ban supersonic flight over land because of noise, but NASA is working on quieter planes that might help change these rules in the future.

In the Hacker News comments, some users are excited about the technology and hope that new engines or fuels can make fast flights cleaner and cheaper. Others worry about the climate impact, saying that burning so much fuel just to save a few hours is not worth it for the planet. A few point out that most people care more about cheap tickets than fast travel, so this will always be a small market. Some mention that the airport experience—security, waiting, boarding—adds a lot of time, so the total trip may not be much shorter. Others remember Concorde and doubt that airlines or customers will pay so much for speed again. There are also comments about government and military being the main users at first, since they can afford it and need fast travel. A few investors say they see the risk: huge costs, tough regulations, and long timelines before these planes make money. Some are hopeful that tech will improve and prices will drop over time, but many think regular people will not fly supersonic soon.

---

## The Curious Case of the Unix workstation layout

- 原文链接: [The Curious Case of the Unix workstation layout](https://thejpster.org.uk/blog/blog-2025-07-19/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44616760)

This article looks at the inside design of UNIX workstations from the 1990s, comparing them to PC layouts from the same time. The writer owns several old UNIX machines like the Silicon Graphics Indigo², HP 9000 series, and DEC Alphastation, and notices they share similar internal layouts.

First, the article explains that early UNIX workstations, like the Silicon Graphics IRIS Indigo, used a tower case with big circuit boards and a VME bus, which is very different from typical PCs. But the next model, the Indigo², switched to a flat desktop case with a mainboard at the bottom, ports at the back, and a riser card on the left—much like the LPX layout that was common in cheap PCs of the mid-1990s. This new style also used standard PC memory and PS/2 ports.

The HP 9000 line also changed over time. The early HP 712 model had a pizza-box style like old Macintoshes, but later models like the Visualize B132L+ used a flat mainboard, rear ports, and left-side risers—again, almost the same as LPX PC designs. The DEC Alphastation 500 followed this trend too: it had connectors at the rear, a riser on the left, ATX-like power connectors, and standard PC memory.

The author wonders if all these companies copied the LPX PC design, worked together secretly, or just arrived at the same solution by chance. There is no clear answer, but the author finds it funny that so many different UNIX workstations ended up looking like cheap PCs inside.

In the Hacker News comments, some people agree that hardware designs often converge when manufacturers want to save money and use standard parts. Others say LPX was popular because it made machines cheaper and easier to build, so it makes sense that UNIX workstation makers followed this trend. A few commenters remember working on these old machines and share stories about fixing or upgrading them, noting how much easier it was with PC-like layouts.

Some users point out that these design changes also made UNIX workstations more familiar for people used to PCs, lowering the learning curve. Others think it’s just a coincidence—when you need to fit certain parts in a box, only a few layouts actually work well. One commenter jokes that “everything becomes a crab,” referencing the idea that different things evolve to look the same over time.

There are also technical discussions about the benefits and problems of riser cards, such as space-saving versus making cooling harder. Some say the move to standard memory and ports was good for users, while others miss the unique, purpose-built designs of older machines. A few people wonder if these changes made UNIX workstations less special and more like everyday PCs, both inside and out.

---

## Babies made using three people's DNA are born free of mitochondrial disease

- 原文链接: [Babies made using three people's DNA are born free of mitochondrial disease](https://www.bbc.com/news/articles/cn8179z199vo)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44587116)

Scientists have helped some babies be born without mitochondrial disease by using DNA from three people. This new technique is called mitochondrial replacement therapy.

In normal cases, babies get DNA from just two parents. But sometimes, a mother’s mitochondria—the tiny parts of cells that make energy—carry bad mutations. These mutations can cause serious diseases. To prevent this, doctors take the nucleus (the main DNA) from the mother’s egg and put it into a donor egg that has healthy mitochondria. This donor egg’s nucleus is removed first, so only its healthy mitochondria stay. Then they add the father’s sperm to make the embryo. The baby grows mostly with DNA from the two parents, but a small part—less than 1%—comes from the donor. This process aims to stop mitochondrial diseases from being passed down in families.

The article explains that this technique is new and not common yet. It was first allowed in the UK, and only a few babies have been born this way. Some people are excited because it could help families have healthy children. Others worry about safety, as long-term effects are not fully known. There are also questions about the ethics of using DNA from three people.

Many Hacker News users say this is a big step for medicine. Some think it’s great that technology can stop these diseases. They talk about how this could help many families. Others are concerned about possible risks, like unknown side effects for the children. Some wonder if this opens the door to more changes in human DNA, which makes them nervous. A few point out that mitochondria only have a small amount of DNA, so the baby is still mostly from the parents. Some users debate whether this is different from other medical treatments. Others talk about the need for careful laws and good oversight. There are also worries about fairness—will only rich people get this treatment? Some are hopeful but want more research. Overall, people are both excited and cautious about this new method.

---

## MCP Security Vulnerabilities and Attack Vectors

- 原文链接: [MCP Security Vulnerabilities and Attack Vectors](https://forgecode.dev/blog/prevent-attacks-on-mcp/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44617910)

This article talks about security problems in MCP, a protocol for how AI models talk to outside tools. MCP tries to make it easy to connect AI to tools, but the author says security has not been a main focus.

The protocol is simple: the AI asks for a list of tools, gets back their descriptions, and can then use them. The problem is, those tool descriptions are just normal text, and if someone controls the MCP server, they can put anything they want in those descriptions. This means an attacker could trick the AI by putting hidden instructions in a tool description. The AI might then do something bad, like steal data, without the user knowing.

The author tested this on several MCP implementations and found most did not check or clean the tool descriptions. Worse, users never see these descriptions, so they would not know if something bad happened. Authentication is also weak; many servers barely check who is connecting, or only check some types of requests. One server only asked for an API key on GET requests, but not on POST, which can be dangerous.

Another big problem is supply chain attacks. MCP tools are shared as packages, and many have too many permissions. If an attacker gets a bad tool into the system, it could read private data or pretend to be the user. The author found tools with too much access and little code review.

Most of these attacks would not show up in normal logs. For example, if a user asks the AI to check their calendar, a bad tool could do something else, and the logs would still show “success.” The author suggests some fixes: check and clean tool descriptions, use structured data, do real authentication with OAuth, and only give tools the permissions they need.

The latest MCP spec now requires OAuth and some other improvements, but the main risks, like tool description injection and risky tools, are still not fully fixed. The author warns these problems should be fixed soon, before MCP spreads too much.

In the Hacker News comments, many people agree this kind of boring security is easy to forget but very important. Some say they have seen similar problems in other protocols, not just MCP. Others point out that security usually gets better once real attacks happen, but it is better to fix things early.

A few users think the main issue is that developers want to move fast, so they skip security steps. Some commenters ask why tool descriptions cannot be made more strict or use safer formats, like only allowing certain words or fields. Others warn that with AI, any small security hole can become a big problem because AIs can act quickly and at scale.

Some people say the fixes, like OAuth and permission limits, are good steps, but real protection needs both better defaults and continued review. A few think the protocol should have stronger rules built in, not just hope for every developer to do the right thing.

There are also comments about how supply chain attacks are a growing problem everywhere, not just with MCP, because code sharing is so common now. Finally, some suggest more tools to help check for these problems, and hope the community will work together before something really bad happens.

---

## Show HN: Am-I-vibing, detect agentic coding environments

- 原文链接: [Show HN: Am-I-vibing, detect agentic coding environments](https://github.com/ascorbic/am-i-vibing)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44616688)

This project is a library that helps you check if your code is running in an AI-powered coding environment or editor. It can be used in command-line tools and Node.js apps to detect if an AI agent or assistant, like Claude, Copilot, or Cursor, is running your code.

The library can tell if your program is being run directly by an AI agent, in an interactive AI shell, or in a hybrid setup. It supports many popular AI tools, such as GitHub Copilot Agent, Claude Code, Replit, and more. The main use is for your app or CLI tool to change its output depending on who or what is running it—for example, to show more machine-friendly logs or error messages for AI agents, and normal messages for humans.

The library gives you a simple API to check the environment. You can use functions like `isAgent()`, `isInteractive()`, and `isHybrid()`. It also has a CLI you can run from the terminal to get a quick answer, with options for different output formats, quiet mode, and debug mode that dumps lots of details about your environment and process tree.

Results include info like the detected tool’s name, type (agent, interactive, or hybrid), and a provider ID. The CLI gives exit codes, so scripts can react differently if an AI is detected. There’s also an example of how you might use the library to give extra information to an LLM if an error happens, such as special tags and suggestions.

The library is open source, written mostly in TypeScript, and uses the MIT license.

In the comments, some people are excited about this tool and see it as helpful for debugging or building apps that work well with AI agents. A few users worry that AI detection could be used in bad ways, like blocking AI tools or tracking users. Others ask how reliable the detection is, since some environments might hide the fact that an AI is running the code. One person points out that false positives are possible, for example, if a user opens a Copilot terminal and runs commands themselves. Some developers suggest ways to improve detection, like checking more process variables or environment hints. A few are interested in using this for analytics or to adapt their apps for AI use. There are also jokes about “vibing” and making code feel friendly to AI agents. Overall, the response is a mix of technical ideas, privacy concerns, and curiosity about where this kind of tool could lead.

---

