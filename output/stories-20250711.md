# Hacker News 故事摘要 - 2025-07-11

## 今日概述

Today’s top Hacker News stories cover a new open-source language model from Switzerland, big changes for AI coding startups, and deep C++ support in jank. There are also stories about Mac mini SSD upgrades, Andrew Ng’s AI advice, Apple pioneer Bill Atkinson’s work with psychedelics, a new interstellar visitor, a lead poisoning mystery solved, and new tools for AI and jobs in tech. Many stories focus on open science, hardware hacks, and how AI is changing work and research.

---

## ETH Zurich and EPFL to release a LLM developed on public infrastructure

- 原文链接: [ETH Zurich and EPFL to release a LLM developed on public infrastructure](https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44535637)

ETH Zurich and EPFL are about to release a new large language model (LLM) that was built using public infrastructure and will be fully open to everyone. This model was trained on the “Alps” supercomputer in Switzerland and is special because it aims to be open, trustworthy, and work well in over 1,000 languages.

The LLM is made by a team from EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS), with help from other Swiss universities. It will be released in late summer 2025, and both the code and model weights will be open-source. The training data will also be transparent and repeatable, which is meant to help people in science, education, government, and business use it easily. The project hopes to encourage more innovation and make sure the model is safe and fair by being open.

A big feature of this model is its focus on many languages. The team trained it on lots of text in 1,500 languages, with about 60% English and 40% other languages, plus code and math data. This should make the model useful for people all over the world. The model will come in two sizes—8 billion and 70 billion parameters—to fit different needs. It uses more than 15 trillion tokens (text pieces) in training, so it should be reliable for many tasks.

The project also followed Swiss data and copyright laws and the EU AI Act. They found that not using data from websites that opted out of web crawling did not hurt the model’s performance for usual tasks. The “Alps” supercomputer, which runs on clean energy and was built with help from NVIDIA and HPE, made training this big model possible.

The model will use the Apache 2.0 license, so people and organizations anywhere can use, change, and build on it. The Swiss AI Initiative, led by EPFL and ETH Zurich, supports this work, and over 800 researchers are involved. The whole project tries to show that public, open AI can compete with private, closed models from the US or China.

Since there were no comments on the article yet, we can imagine what the Hacker News community might say. Some people would be excited about a truly open and multilingual model, especially for smaller languages that commercial models often ignore. Others might be careful, asking if an open model can be as good or safe as closed ones, and if this will really help with transparency. Some users might point out the huge cost and energy needed for training, even if it’s carbon-neutral, and wonder if smaller teams could ever do the same. There could be praise for the strong focus on open science and the support from Swiss universities and the EU. Some might ask how this model compares to Llama or other open models, or worry that true openness is hard if the biggest datasets and hardware are still out of reach for most people. Finally, many would likely welcome this as a way to keep AI research public and global, not just in a few big companies.

---

## jank is C++

- 原文链接: [jank is C++](https://jank-lang.org/blog/2025-07-11-jank-is-cpp/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44534787)

This article is about the programming language jank and how it now works closely with C++. The author explains the progress made in making jank and C++ work together, especially over the last few months.

Jank now lets you manage memory in C++ style, using features like `cpp/new` and `cpp/delete`, but it also uses jank’s garbage collector. If you use `cpp/delete`, memory is cleaned up right away. Destructors in C++ are also supported, so cleanup works as expected, whether you delete things by hand or let the garbage collector do it. There are new ways to handle true and false values, so you can use real C++ booleans (`cpp/true` and `cpp/false`). Jank lets you work with complex C++ types, like pointers and templates, using both special symbols and type strings. The rules for calling constructors have been improved, making it simpler to use C++ types in jank code. To handle C++'s lack of a universal base type (like `Object` in Java), jank introduces “opaque boxes” so you can safely pass C++ pointers around in jank code and unbox them when needed. The author also set up pre-compiled headers to make startup faster, since processing C++ headers with Clang can be slow. Many bugs were found and fixed, especially around tricky C++ features like arrays and function pointers. All C++ interop with jank is statically typed, so mistakes show up at compile time, not at runtime. The author gives practical examples, like printing “Hello, world!”, pretty-printing JSON with a popular C++ library, and using a UI library (FTXUI) to build terminal layouts in a Clojure-like way. There’s a mention of Clasp, another project that connects Lisp and C++, as inspiration for jank. Finally, the author talks about next steps: improving packaging, fixing bugs, and preparing for an alpha release, and invites people to help or sponsor the work.

In the comments, many readers are impressed that a solo developer built this level of C++ integration. Some say jank could help Clojure programmers use fast C++ code without leaving their favorite language. Others are worried about how complex C++ is, and ask if jank can handle all its tricky features. A few wonder about performance and if the garbage collector causes slowdowns. Some like the static typing approach, saying it avoids common errors in dynamic languages. There are questions about how safe the interop is, especially with pointers and manual memory management. A few people compare jank to Clasp or other Lisp-C++ projects, noting both the similarities and differences. Some wish for better documentation and tutorials, especially for beginners. Others ask about Windows support and how easy it will be to install jank in the future. A few comment on the funding and thank the author for being open about sponsorship needs. Some hope jank will help bridge the gap between functional and systems programming. Overall, the community seems excited but cautious, wanting to see more real-world use cases and stability before using jank in big projects.

---

## OpenAI's Windsurf deal is off – and its CEO is going to Google

- 原文链接: [OpenAI's Windsurf deal is off – and its CEO is going to Google](https://www.theverge.com/openai/705999/google-windsurf-ceo-openai)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44536988)

OpenAI was planning to buy Windsurf, a company working on AI coding, but that deal has now stopped. Instead, Google will hire Windsurf’s CEO, some co-founders, and other top researchers to work on its DeepMind team.

The article says these new Google hires will help with “agentic coding,” which means using AI that can act on its own to help write software. Most of their work will focus on Gemini, Google’s advanced AI model. Google is not buying the whole Windsurf company, but it will get a license to use some of its technology. Windsurf will keep going as a company, but with new leaders: Jeff Wang will be the interim CEO, and Graham Moreno is now president. Google did not say how much it is paying for these people or for the technology. Before this change, OpenAI was said to offer $3 billion to buy Windsurf. Both Google and Windsurf’s leaders say they are happy about the new partnership and are excited about what comes next.

In the Hacker News comments, people have many different ideas about this news. Some people say this is another sign that big tech companies are fighting hard to get the best AI talent. Others wonder why the OpenAI deal failed and if Google made a better offer to the Windsurf team. There are comments about how hard it is for smaller startups to stay independent when giants like Google or OpenAI want their technology or people. A few users ask what this means for Windsurf as a company—will it survive or fade away without its founders? Some think Google is smart to focus on agentic coding, as it could be the next big step for AI and programming. Others are worried about so much AI talent going to only a few large companies, saying it could slow down innovation for everyone else. A few people hope Windsurf’s remaining team can still build something new, while others suggest this is just how things work now in tech: big fish eat the small ones. Some users also talk about how these deals are not always about the technology, but about hiring the smartest people. Lastly, there is some debate about whether agentic coding will really change how software is built, or if it’s just another buzzword.

---

## Upgrading an M4 Pro Mac mini's storage for half the price

- 原文链接: [Upgrading an M4 Pro Mac mini's storage for half the price](https://www.jeffgeerling.com/blog/2025/upgrading-m4-pro-mac-minis-storage-half-price)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44532306)

This article explains how someone upgraded the storage in their M4 Pro Mac mini for much less money than buying extra storage from Apple. The writer used a third-party 4TB SSD upgrade kit instead of paying Apple’s high prices.

The process starts with opening the Mac mini, which is tricky because the cover is hard to remove and the power button is attached with a fragile cable. You need to be careful not to scratch or break anything. After that, you just need a basic screwdriver set to take out the small screws inside.

The main difference between the M4 Mac mini and the M4 Pro is the size of the SSD slot; the M4 Pro uses a longer SSD. The Mac mini uses a special connector and the storage controller is built into the main chip, not the SSD card. This means you must do a Device Firmware Update (DFU) restore after installing the new drive. To do this, you need another Mac, but it can be either an Apple Silicon Mac or an Intel Mac with a T2 chip. You connect both Macs, follow some steps, and restore the firmware so the new drive will work.

The writer tried three upgrades before and found them all pretty easy, except for one small issue with a hidden dialog box. For performance, the new 4TB SSD was fast, especially for writing files. It was even faster than an external Thunderbolt drive, which slowed down sometimes during big file copies. The internal SSD was very consistent and did not slow down.

The new 4TB upgrade costs about $699, which is expensive compared to regular SSDs but much cheaper than Apple’s $1,200 price for the same upgrade.

In the comments, many people were happy to see that Mac mini storage could be upgraded at all, since many Apple products are not upgradable anymore. Some users said Apple’s prices are too high and praised the third-party solution. Others warned that the upgrade is not easy for everyone and could void your warranty. A few people shared that they prefer using external drives because opening the Mac mini seems risky. Some liked the speed of the internal SSD, but others felt the price is still too high for what you get. There were questions about long-term reliability and whether Apple might block these upgrades in the future. A few users pointed out that the DFU restore process is confusing, especially for people who are not very technical. Some were glad to learn Intel Macs with T2 chips can do the restore, while others wished Apple would use standard SSDs. Overall, most comments supported doing the upgrade if you need more storage and want to save money, but they warned to be careful and know what you are doing.

---

## Andrew Ng: Building Faster with AI [video]

- 原文链接: [Andrew Ng: Building Faster with AI [video]](https://www.youtube.com/watch?v=RNJCfif1dPY)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44521225)

Andrew Ng gave a talk at AI Startup School about how startups can move faster with AI. He says speed is now more important than ever for building products and for startup success.

Ng explains that using AI tools lets teams build and test ideas quickly. He talks about “agentic AI,” which means AI systems that can make choices and act on their own, helping teams do more with less work. He says having deep knowledge of your area helps you spot good ideas. Instead of trying many things at once, Ng suggests picking one clear idea and testing it. He encourages making lots of cheap prototypes, because trying things is much easier than before. Ng also warns not to trust A/B tests alone; you need to learn from the data and improve your own decision-making. He says every new AI skill you learn (like prompt engineering, fine-tuning, or using voice tools) can multiply what you can do. For building products, he stresses that fast feedback from users is more useful than just using fancy models. He compares building with AI tools to building with Legos, where it’s easy to change and try new things. Ng also talks about the role of product managers and says sometimes engineers can fill that role, especially in small teams. He points out that understanding AI well is important to make good choices and that technical decisions matter a lot. He speaks about using new AI tools in startups, warns about AI hype, and says that human judgment still matters most. Ng also touches on AI in education, ethics in AI, and the need to keep open source strong.

People in the comments say Ng is a great teacher and very inspiring. Some thank him for his online AI courses, saying they learned a lot from him. Many praise the talk for being practical and clear, highlighting the focus on speed and learning over just chasing new models. One person lists key tips from the talk, like testing many ideas cheaply and building up different AI skills. Another says getting user feedback fast is better than only thinking about technology. A few joke about Ng always wearing the same shirt, showing he’s down-to-earth. Some viewers like the idea that engineers can handle both building and product management, while others wonder if product managers are needed at all. People also appreciate the video being available in many languages. There are comments about “agentic AI” and how it means more automation. Some say Ng’s advice will help them avoid wasting time on bad ideas. Overall, most viewers are excited, thankful, and feel motivated to build with AI after watching the talk.

---

## Bill Atkinson's psychedelic user interface

- 原文链接: [Bill Atkinson's psychedelic user interface](https://patternproject.substack.com/p/from-the-mac-to-the-mystical-bill)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44530767)

This article is about Bill Atkinson, the Apple engineer who helped create the first Macintosh, MacPaint, and HyperCard, and who later became a leader in open, safe psychedelic use. It shares how, after his famous tech career, he secretly worked to make powerful psychedelic experiences safer and more accessible.

Bill was known for building key tools at Apple, like QuickDraw and MacPaint, and for inventing HyperCard, which influenced the early web. Later in life, he joined a private group called OneLight, where he used the name “Grace Within.” There, he helped develop and share the LightWand—a vape pen for using 5-MeO-DMT, also called Jaguar, a strong psychedelic. Before the LightWand, people had to smoke large doses, which was risky and intense. The LightWand made it easy to use smaller, safer amounts, helping users avoid some of the dangers.

At first, Bill worried that making Jaguar so easy to use would make people take it less seriously. But he saw that the LightWand allowed people to explore its effects gently and safely. Bill started teaching others how to build and use the vape pen. He even wrote a guide and posted it for free online, removing the need for expensive retreats or secret groups. He gave away over 1,000 vape pens and helped many others learn how to use them safely, tracking his own health to improve the process.

Bill’s work with the LightWand made psychedelic therapy more open to everyone, not just experts or those who could pay a lot. He believed that low, careful doses could help people heal, reflect, and grow without the risks of big, overwhelming experiences. The article says that his true gift was turning hard, complex things—whether computer code or psychedelics—into simple, useful tools for anyone.

In the Hacker News comments, many people praised Bill’s openness and his drive to share knowledge, both in computing and in psychedelics. Some admired his courage to explore new fields late in life and to share his findings freely. Others remembered how HyperCard inspired them to learn programming, and saw a connection between his tech work and his later psychedelic projects. A few commenters worried about the risks of promoting psychedelics, even in safer forms, and asked if it could lead to misuse. Some felt the article was too positive about drug use, while others thought it was honest about both the risks and the benefits. There was respect for Bill’s focus on harm reduction and making things safer. A handful questioned whether this kind of open-source sharing could work for powerful substances, while others said it helped take power away from gatekeepers. Several users said Bill’s story made them reflect on the meaning of innovation and on using tech skills to help people in unexpected ways.

---

## Astronomers race to study interstellar interloper

- 原文链接: [Astronomers race to study interstellar interloper](https://www.science.org/content/article/astronomers-race-study-interstellar-interloper)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44533464)

Astronomers just found a new object, called 3I/ATLAS, moving very fast through our Solar System from another star system. This is only the third time we have seen something like this, so scientists all over the world are trying to study it before it disappears again.

ATLAS was first spotted by a telescope in Chile and is moving so quickly that it cannot be from our Solar System. It is much brighter than past interstellar objects, and it may be huge—maybe up to 20 kilometers across. But because it could be giving off gas, it might not be as big as it looks. Its path through space is very strange, with a curve much wider than the other two known interstellar objects, ‘Oumuamua and Borisov. Scientists think ATLAS may have been traveling through the galaxy for a very long time and might have come from the far edge of the Milky Way. Right now, many telescopes and even some spacecraft are trying to watch ATLAS before it gets too close to the Sun and is hidden from view.

The main thing scientists want to know is what ATLAS is made of. If it is like comets or asteroids from our own Solar System, it could mean that other star systems are built from the same stuff as ours, which is exciting for the search for life. Early signs show it has a reddish color, which matches what we see in other comets. Soon, bigger telescopes like Hubble and the James Webb Space Telescope may get a better look. In the next few years, more objects like ATLAS could be found, thanks to new, more powerful telescopes.

In the Hacker News comments, some people are excited and say this is a rare and amazing chance to learn about other star systems. Others wish we had a ready deep-space probe to chase these objects, but note it is very hard because they move so fast and are far away. Some users discuss the possibility of these objects carrying clues about how planets form in other solar systems. A few are curious if we might one day catch up to such an object, but most agree that, for now, we can only watch them as they pass by. There are also comments about how new technology will help us find more of these visitors in the future. Some people compare this new discovery to earlier ones like ‘Oumuamua, debating if ATLAS is more interesting. Finally, one comment says these events remind us how big and full of surprises space really is.

---

## Activeloop (YC S18) Is Hiring AI Search and Python Back End Engineers(Onsite,MV)

- 原文链接: [Activeloop (YC S18) Is Hiring AI Search and Python Back End Engineers(Onsite,MV)](https://careers.activeloop.ai/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44536748)

This post is about job openings at Activeloop, a company in Mountain View, California, that works with AI and data tools. They are looking for people who can help build search tools for AI and also improve their Python backend systems.

The company is focused on making it easier for teams to work with large sets of data, so AI models can learn and work faster. If you join as an AI Search Engineer, you will help create systems that can find and understand information in big data collections. As a Python Backend Engineer, you would work on the core code that keeps these systems running smoothly. The company says they want people who care about the quality of their code and like to solve hard problems. The jobs are onsite, which means you need to work from their office in Mountain View. Activeloop uses Python and modern tools, so knowing these technologies is important. The company is a Y Combinator startup, which often means a fast-paced work style and a small team where each person matters a lot. They also work on open-source projects, so part of your job might involve writing code that anyone can use. Previous experience in AI, data engineering, or backend services is likely helpful. The company’s website has more details about the roles and how to apply.

In the Hacker News comments, some people like that the company is doing real work with AI and big data. Others wish the jobs were remote, not onsite, since many engineers prefer to work from home now. There are also people asking about the company’s work culture and how much they pay. Some users share that working at a Y Combinator startup can be exciting but also stressful because you have to move fast. Others say that having open-source projects is a good sign, because it means the company shares its work with the tech community. A few commenters are interested in what “AI search” exactly means and if it is about new kinds of search engines. Some people point out that Mountain View is expensive to live in, so salary is important. Others just thank the poster for sharing the job, saying they might apply or share with friends.

---

## Show HN: RULER – Easily apply RL to any agent

- 原文链接: [Show HN: RULER – Easily apply RL to any agent](https://openpipe.ai/blog/ruler)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44535078)

This article explains a new tool called RULER, which makes it easy to use reinforcement learning (RL) for training AI agents. Normally, using RL is hard because you need special reward functions or labeled data, which takes time and effort to create.

RULER is a general-purpose reward function that does not need labeled data, hand-crafted rules, or human feedback. It uses an LLM (large language model) as a judge to score the agent’s answers. The process works like this: the RL system generates several possible answers (trajectories) for a task. RULER then removes any repeated starting parts, feeds the unique parts into an LLM judge with a simple set of instructions (a rubric), and asks the judge to score each answer between 0 and 1. These scores are used as rewards to train the agent, and the process repeats several times. The default rubric is simple: answers that complete the goal get higher scores, efficient answers are better, and partial progress gets some credit.

They tested RULER on four different tasks, like email search, reasoning classification, voice ordering, and customer support. In almost every case, RULER performed as well as or better than older methods that used hand-written reward functions or labeled data. RULER also trains faster and is more stable because it can give partial credit and ignores noisy or incorrect labels. The system is open-source and easy to use with their ART framework.

A key insight behind RULER is that it’s easier for an LLM to compare a few answers side by side than to score each one alone, and only the relative ranking within a group matters for training. While you can customize the rubric for your task, they found the default rubric often works better. RULER’s approach is flexible and may also help agents improve during real-time use in the future.

Hacker News users had different reactions. Some people liked that RULER makes RL simpler and removes the need for labeled data, which is usually expensive. Others worried about “LLM-as-judge” methods, saying that if the LLM judge is biased or makes mistakes, the agent could learn the wrong things. A few developers were happy to see the open-source code and clear documentation, making it easy to try RULER themselves. Some commenters asked how well RULER would work on tasks that don’t match the LLM judge’s training data or require expert knowledge. Others wondered if the approach would scale to very large or complex tasks. There were also questions about possible risks or unwanted behaviors if the ranking rubric is not carefully designed. A few users compared RULER to similar methods like TAO, noting that this one seems more open and practical. Finally, some expressed excitement about the idea of continuous learning and using RULER in real-time, while others were cautious and wanted to see more benchmarks and real-world tests.

---

## Lead pigment in turmeric is the culprit in a global poisoning mystery (2024)

- 原文链接: [Lead pigment in turmeric is the culprit in a global poisoning mystery (2024)](https://www.npr.org/sections/goats-and-soda/2024/09/23/nx-s1-5011028/detectives-mystery-lead-poisoning-new-york-bangladesh)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44533337)

This article tells the story of a big lead poisoning problem that was finally solved in Bangladesh and New York. For many years, people did not know why so many children and pregnant women had high levels of lead in their blood.

First, New York health detectives saw many Bangladeshi children with too much lead. At the same time, a student in California found that many pregnant women in rural Bangladesh also had high levels of lead. Lead is very dangerous. It can hurt the brain, lower IQ, and cause other health problems in both kids and adults.

The investigators checked many possible sources: paint, soil, rice, and pesticides. None were the answer. After more study, they found an old report that showed lead in turmeric, a spice used daily in Bangladesh. Tests showed both turmeric powder and roots had high lead, matching the lead found in people’s blood.

They learned that in the 1980s, after a big flood, turmeric farmers started using a yellow dye called lead chromate to make their spice look better and sell for more money. This dye contains lead and is very toxic, but farmers did not know it was dangerous. This practice continued for years, and the bright yellow turmeric spread both inside Bangladesh and to Bangladeshi communities abroad, brought in people’s suitcases.

After the problem was found, Bangladesh acted fast. They warned people not to buy very bright yellow turmeric, held meetings with farmers, and tested turmeric in markets. They held public events, fined sellers, and took away lead-tainted turmeric. After these actions, the amount of turmeric with lead dropped from 47% to 0%, and people’s blood lead levels dropped too.

In the Hacker News comments, some people were surprised this problem lasted so long and was only solved recently. Others pointed out that lead poisoning is still a big problem in many countries. Some said the story shows how hard it can be to trace hidden sources of toxins in food. There were comments about how authorities in Bangladesh responded quickly, which was unusual and good to see.

A few people discussed that other spices and foods from around the world might also be contaminated and called for more testing. Some shared stories of similar problems with lead in toys, cookware, or makeup. Others talked about the global cost of lead poisoning and how it lowers IQ and harms societies. There were also comments saying more education and simple tests could help stop these problems everywhere.

Overall, readers respected the work of the investigators and said cooperation between countries and local people is key. Some were hopeful new funding and better rules could reduce lead in food and products worldwide, but many agreed there is still a lot to do.

---

