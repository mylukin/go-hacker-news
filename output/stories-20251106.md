# Hacker News 故事摘要 - 2025-11-06

## 今日概述

Today’s top Hacker News stories focus on a huge data leak, a new open-source AI that thinks step-by-step, and a big change in what we know about the universe’s expansion. Other stories cover AI tools for work, book recommendations from millions of reviews, Swift coming to FreeBSD, and the move from Microsoft to open-source software in big groups. There is also news about AI privacy and how language models understand problem difficulty.

---

## Two billion email addresses were exposed

- 原文链接: [Two billion email addresses were exposed](https://www.troyhunt.com/2-billion-email-addresses-were-exposed-and-we-indexed-them-all-in-have-i-been-pwned/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45839901)

A huge new data set with about 2 billion email addresses and 1.3 billion passwords was leaked and added to the Have I Been Pwned service. The data mainly comes from many old data breaches and from malware that steals login details, making it a very large and risky collection.

The author checked some of the data himself and found old passwords he used years ago, as well as passwords that were not his, showing the mix of real and recycled information. He also asked some Have I Been Pwned subscribers to check if the passwords linked to their emails were real. Many people confirmed that at least one password was theirs, sometimes still in use, which shows the risk of reused passwords. Some passwords were very old, while others were still active on unused accounts. Sometimes, people could not recognize a password, which might mean they forgot it, never used it, or it was auto-generated for them. The author points out that finding your email in the data means your info was part of a breach, but it does not always mean every password is yours.

The author explains that Have I Been Pwned does not link passwords and emails together when making passwords public. This helps protect users, as nobody can see both the email and password pairs. You can check if your password is leaked by searching in Pwned Passwords, using their website, an API, or the 1Password manager. The author stresses that this is not a Gmail leak; the data comes from many different email providers, and Gmail is just the biggest, not the cause. Processing so much data was hard and costly, needing special database tricks and more server power. Notifying millions of users about the breach without getting marked as spam was another challenge, so they used a slow, careful email delivery method.

In the comment section, many people thanked the author for the hard work and public service. Some were surprised and concerned by the huge number of leaked emails and passwords. Others discussed how password reuse is still a big problem, and why people do it even when they know the risks. Some users pointed out that even complex passwords are not safe if reused, and praised tools like password managers. A few worried about the privacy of the Have I Been Pwned database itself, but others explained that the service does not store email and password pairs together, which makes it safer. There was also discussion about how old some passwords in the data are, and whether it still matters if they are no longer used. Some users shared tips for checking your own passwords and suggested turning on two-factor authentication everywhere. Finally, a few people argued that websites should do more to protect users and support new login tech like passkeys, while others said personal responsibility is still most important.

---

## Kimi K2 Thinking, a SOTA open-source trillion-parameter reasoning model

- 原文链接: [Kimi K2 Thinking, a SOTA open-source trillion-parameter reasoning model](https://moonshotai.github.io/Kimi-K2/thinking.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45836070)

Kimi K2 Thinking is an open-source AI model focused on step-by-step reasoning, coding, and internet search. It is designed to solve very hard problems by making hundreds of decisions in a row and using tools like code interpreters and web browsers. The model has set new records on benchmarks that test logic, coding, and searching skills, such as Humanity’s Last Exam (HLE), BrowseComp, and SWE-Bench Verified.

Kimi K2 can handle up to 200–300 actions in a sequence without help, making it useful for complex tasks. It does this by planning, thinking, searching, then thinking and coding again in cycles, which helps it solve big or unclear problems in smaller steps. For example, it was able to solve a PhD-level math problem by breaking it into 23 steps, using both reasoning and tools.

The technical report explains how the model works as a “thinking agent.” It uses many “tool calls” and long text inputs (“thinking tokens”) to scale up its reasoning. It also shows strong results in programming, especially in building websites and components from a single prompt. In coding tests, Kimi K2 Thinking scores higher than earlier models, handling tasks in many languages and front-end frameworks.

For searching and browsing, Kimi K2 outperforms even humans on tough benchmarks, finding answers from the web when information is hard to locate. The model’s strength is in its agentic (goal-driven) approach: it can plan, search, gather, and verify information over long sessions. This makes it good at research, code generation, and answering open-ended questions.

The article also walks through a detailed math example involving probability in hyperbolic space. Using known formulas and step-by-step logic, the AI computes the answer, showing how it can tackle advanced mathematics.

In the Hacker News comments, many readers are impressed with the technical achievement, especially the ability to chain hundreds of tool calls. Some developers are excited by the open-source promise, hoping to use the API for their own apps. Others ask if the benchmarks are fair, wondering if the model is really “state-of-the-art” or just better at certain tasks. A few people question how much of the reasoning is actually new, and how much depends on large-scale training data and tool use.

There are also concerns about resource costs: running such a big model can be expensive, and some wonder if it’s practical for smaller companies. Some users discuss the ethical side, like whether these agentic AIs could act in unexpected or risky ways if not managed well. Others debate if “agentic” models are really safe or if they might need new tools to control them.

Overall, people are curious to see if Kimi K2 Thinking will help build smarter software agents, or if it’s mostly an academic step forward. Some are optimistic about the “agentic” future, while others want to see more real-world results and open benchmarks before they decide.

---

## Universe's expansion 'is now slowing, not speeding up'

- 原文链接: [Universe's expansion 'is now slowing, not speeding up'](https://ras.ac.uk/news-and-press/research-highlights/universes-expansion-now-slowing-not-speeding)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45840200)

A new study says the universe’s expansion is now slowing down, not speeding up as most scientists believed. The research challenges the old idea that “dark energy” is making galaxies move apart faster and faster.

For over 20 years, astronomers thought dark energy was causing the universe to accelerate. This was based on studying type Ia supernovae—big star explosions used as “standard candles” to measure distances in space. But the new study found that these supernovae are not as standard as people thought. Supernovae from younger stars are dimmer, and those from older stars are brighter, even after standard corrections. The researchers looked at 300 host galaxies and found this age effect is very strong. When they corrected for the age of the star that made the supernova, the data no longer fit the standard model (called ΛCDM) that includes constant dark energy. Instead, the data matched a model where dark energy changes over time and is now getting weaker. This means the universe is not accelerating anymore—it is actually slowing down now. They checked their results with other data from BAO (sound waves from the early universe) and the cosmic microwave background, and it all agrees. New instruments like DESI and the Vera Rubin Observatory will help test these findings further. If confirmed, this could change how we understand the universe.

In the Hacker News comments, some people were excited by the idea that a big shift in cosmology might be coming. Others were more skeptical, saying we should wait for more data before changing the standard model. A few pointed out that the standard candles problem has been discussed before, and that this study just adds more weight to those worries. Some users wondered what this means for the future of the universe—if it will stop expanding or even collapse. Others talked about how science always changes when new data comes in and that this is a normal part of progress. One commenter said that even if the result is true, it doesn’t mean dark energy doesn’t exist, just that it might act differently than we thought. A few people explained that having more and better data from new telescopes is very important, and that we might get a clear answer soon. There was also some confusion about what “decelerating expansion” really means and whether the universe could shrink. Overall, the discussion showed excitement, curiosity, and a healthy amount of doubt.

---

## You Should Write An Agent

- 原文链接: [You Should Write An Agent](https://fly.io/blog/everyone-write-an-agent/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45840088)

This article says everyone should try writing an LLM agent. The author explains that, like learning to ride a bike, you only really understand agents if you build one yourself. Large language model (LLM) agents seem confusing, but actually, they are easy to create—you just need a few lines of code using the OpenAI API. The article shows simple Python code for an agent that remembers what you and the AI say, making a chat feel real. It also explains how to add tools, like a ‘ping’ command, so the agent can do things like check internet connectivity. The LLM decides when to use the tool by itself, which feels almost magical. 

The author says you can make your own coding agents quickly, and you do not need many outside tools or plugins. If you want to manage conversations or store more data, you can use something like SQLite. The article also talks about “context engineering”—keeping track of how much information you give to the agent, since there is a limit to how much it can remember at once. You can even make “sub-agents” that talk to each other or do special jobs. The author thinks this is real programming work, not just writing fancy prompts. He says nobody really knows the best way to build these agents yet, and that is exciting because anyone can experiment and maybe discover something new. 

Commenters have different opinions. Some agree that making a simple agent is easier than expected and say it helped them understand LLMs better. Others warn that toy agents are not ready for real-world tasks; they point out problems like security risks and how LLMs sometimes give wrong answers. Some people like the idea of “context engineering” and say it is an important skill, while others think these agents are mostly for fun and not yet useful for serious work. A few mention that adding more tools makes the system harder to manage, and you must be careful not to lose track of what the agent is doing. Some developers are excited that they can build their own agent instead of relying on big company products. Others are more skeptical, saying LLM agents still make mistakes and do not replace careful programming. There is also discussion about whether this technology will change software development or if it is just a short-term trend. Many agree that trying it yourself is the best way to learn, even if you end up not liking the results.

---

## Show HN: I scraped 3B Goodreads reviews to train a better recommendation model

- 原文链接: [Show HN: I scraped 3B Goodreads reviews to train a better recommendation model](https://book.sv)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45825733)

Someone created a new website that gives book recommendations using a machine learning model. To build this, they scraped three billion reviews from Goodreads, a big site where people rate and review books.

The website asks you to enter books you have read. Then, the model suggests new books you might like. The system only shows books that are popular enough—less popular ones are handled on another part of the site. If you enter at least three books, the recommendations get better. The search tool helps you pick books, and you need to type at least two letters to see results.

The article does not go into the technical details of the model, but it is clear that it uses the huge dataset of reviews to find patterns. The goal is to make better book recommendations than other sites. Using real user reviews helps the model understand what people actually enjoy reading.

In the Hacker News comments, some people are impressed by the scale of the data scraping and the effort to build a new model. Others worry about the legal and ethical side of scraping so many reviews from Goodreads, since the site probably does not allow this. Some users think the recommendations could be more diverse if less popular books were included. A few wonder how well the model works compared to other book recommendation engines. There are also questions about how the model handles books with mixed or controversial reviews. Some people want to know if the system can be used for other types of recommendations, like movies or music. Others think it is a fun tool for finding new books and are excited to try it. A few mention that CAPTCHAs on the site make it hard to use. There is also a discussion about whether this kind of project could help people discover hidden gems, not just popular books.

---

## Swift on FreeBSD Preview

- 原文链接: [Swift on FreeBSD Preview](https://forums.swift.org/t/swift-on-freebsd-preview/83064)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45837871)

Swift is now available as a preview for FreeBSD 14.3+ systems, letting developers try out the Swift compiler and runtime on x86_64 machines. The team has shared a downloadable bundle, but warns that it is still early and not all features are ready.

To use Swift on FreeBSD, you need to install some system packages first: zlib-ng, python3, sqlite3, libuuid, and curl. The release is not fully stable yet. There are some known problems. For example, the thread sanitizer gives wrong error reports. The LLDB debugger cannot run Swift code expressions. Command plugins in Swift Package Manager may hang and not finish. If you use Swift’s C++ interop with common types, you might see an error about a missing symbol called "__voidify". When you import C libraries, you still have to use “Glibc,” but this will change to "import FreeBSD" later. Also, the tools lld and lldb need a library called libxml2.so.2, which FreeBSD’s package manager does not provide.

The developers are working to fix these issues. They want to add support for aarch64 hardware and make Swift available for all minor releases of FreeBSD 14. They are asking users to report bugs on GitHub and to join the forums if they want to help.

Hacker News users had mixed reactions. Some people were excited, saying this is great for FreeBSD and could help bring more users to the OS. Others pointed out that some key Swift features, like full debugger support and package manager stability, are not ready, making this less useful for now. A few liked that more programming languages are coming to FreeBSD, but worried about the extra work needed to keep everything up to date. Some wondered if Apple will support this officially, or if it will remain a community effort. There was praise for the team’s hard work, but also reminders that most Swift development still happens on macOS and Linux. A few users discussed the technical challenges of porting modern languages to smaller platforms, and some shared hope that FreeBSD will get more love from language developers in the future. Overall, people saw this as a positive step, but agreed it is a work in progress.

---

## Hightouch (YC S19) Is Hiring

- 原文链接: [Hightouch (YC S19) Is Hiring](https://job-boards.greenhouse.io/hightouch/jobs/5542602004)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45840612)

Hightouch is a company that builds AI tools to help marketing teams work faster and smarter. They are looking to hire a software engineer who can help design and build AI systems, especially using large language models (LLMs).

The article explains that Hightouch combines new AI technology and modern data tools like Snowflake and Databricks. Their customers include big companies like Domino’s and Spotify. The company wants team members who are good at working with others, can solve problems from the basics, and are eager to learn. For this role, they want someone who is creative with AI, can build things quickly, and understands what customers need. Example tasks include trying out new AI ideas, choosing the best AI tools, making smart AI agents that help answer business questions, and making the user experience smooth. You do not need past experience with LLMs, but you should be strong in backend software work and product thinking. The job pays between $180,000 and $320,000 per year and is remote. The hiring process includes several interviews: a short chat with a recruiter, a system design exercise, a talk with a manager, and a deep discussion about building AI agent systems. They do not require traditional programming interviews, because they don’t find them helpful.

In the comments, some people like that Hightouch skips coding tests, saying this makes the process less stressful and more focused on real skills. A few worry that “no coding interviews” might mean the company cares less about programming ability, but others point out that system design and architecture interviews often show real skill better than whiteboard coding. Some users discuss the high salary range, noting it is attractive, especially for remote work. Others wonder how much AI experience is needed, but agree that being curious and strong in backend work is most important. One commenter says the focus on “product-minded” engineers is rare and good, since many jobs only look for technical skills. A few people ask about the company’s growth and how stable the business is, and others share positive stories about Hightouch’s products. There is also some talk about the value of understanding customer problems, not just building tech. Overall, commenters think this is an interesting job for engineers who enjoy both AI and product work.

---

## ICC ditches Microsoft 365 for openDesk

- 原文链接: [ICC ditches Microsoft 365 for openDesk](https://www.binnenlandsbestuur.nl/digitaal/internationaal-strafhof-neemt-afscheid-van-microsoft-365)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45837342)

The International Criminal Court (ICC) is moving away from Microsoft 365 and switching to Open Desk, an open source office suite from Europe. The reason for this change is growing worry about being too dependent on big American tech companies, especially after political problems between the US and the ICC.

The article explains that Open Desk was made by a German group, Zendis, for the German government. Open Desk is part of a bigger European project to protect digital independence. The ICC’s decision comes after the US, under President Trump, put sanctions on the ICC’s chief prosecutor, which led Microsoft to cut his access to email. Microsoft says it never stopped service for the ICC as a whole, but trust was damaged. This event made many in Europe more aware that using American software can be risky, especially for important government work.

The Netherlands is also testing Open Desk under a program called "Mijn Bureau," where different government groups work together to use European open source tools. The Dutch government says digital independence and safety are now top goals. Other cities and governments in Europe are also looking at ways to limit their use of US tech giants.

In the comment section, some people feel this is a smart move for Europe. They say public groups should not risk losing access to their own data because of another country’s politics. Others are less sure, arguing that open source tools often have fewer features or can be harder to support. Some worry about costs and if Open Desk will really work as well as Microsoft 365. A few users mention that switching software is hard for big groups and can take a long time.

Some commenters point out that this is not only about software, but about control and trust. They like that Europe is building its own tools for key jobs. Others think the US could react by making it harder for European tech in return. There are also jokes about how open source software sometimes has bugs or is less user-friendly. Still, many agree that being less dependent on one country for important tech is a good idea, even if it is difficult at first.

---

## LLMs Encode How Difficult Problems Are

- 原文链接: [LLMs Encode How Difficult Problems Are](https://arxiv.org/abs/2510.18147)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45838564)

This paper looks at how large language models (LLMs) understand the difficulty of problems, like math or coding tasks. The authors want to see if LLMs “know” how hard a problem is, similar to how humans judge difficulty. They tested this by using “linear probes” on many models and layers, comparing the models’ sense of difficulty to human labels on a dataset called Easy2HardBench. The results show that the human idea of difficulty is easy to find inside the LLMs, and it works better as the models get bigger. But the way LLMs themselves rate difficulty is less clear and does not improve with bigger models. When the researchers made the models think about problems as “easier,” the models made fewer mistakes and gave better answers. They also found that, during reinforcement learning, the link between human difficulty and model accuracy became stronger, but the LLM’s own difficulty sense became weaker. This means that models learn to follow human ideas of difficulty, and this helps them solve problems better. The team shared their code so others can check their work.

In the Hacker News comments, some users are excited that LLMs can “know” which problems are hard or easy, saying this could help make models more reliable. Others are surprised that models do better when pushed toward “easy” thinking, wondering if this is just making them avoid taking risks. A few people point out that LLMs failing on easy problems is a common frustration, and this research might help fix those issues. Some users think the human difficulty labels are still too subjective, so models trained on them might just copy human mistakes. There’s also debate about whether reinforcement learning is really the best way to improve models, since it sometimes makes the model’s own sense of difficulty worse. A handful of commenters discuss how this could be useful for auto-grading or tutoring systems, making them safer for students. Others worry about overfitting to benchmarks and losing general skill. One user jokes that maybe we need an LLM just to judge question difficulty. Some are happy the code is open, while others ask for more real-world tests beyond math and code. Overall, the discussion is hopeful but cautious about what this means for future LLM tools.

---

## Open Source Implementation of Apple's Private Compute Cloud

- 原文链接: [Open Source Implementation of Apple's Private Compute Cloud](https://github.com/openpcc/openpcc)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45824243)

This project is about OpenPCC, an open-source version of Apple’s Private Compute Cloud for private AI inference. It wants to let anyone run AI models while keeping user data—like prompts and outputs—private and secure.

OpenPCC uses encrypted streaming to protect data as it moves, hardware attestation to check the compute node is safe, and unlinkable requests so nobody can match who asked what. It’s built to be open and clear, so anyone can check how it works. The idea is for OpenPCC to become a new, trusted standard for AI privacy. The code includes a Go client, a C library (for Python and JavaScript use), and in-memory services to test things locally. There is also a managed service called CONFSEC, built on OpenPCC, for users who want a ready solution. You’ll find example code for making requests, and tools for running local tests. The project is governed by the community and uses the Apache-2.0 license, so it is free and open for anyone.

In the comments, some users are happy to see a real open-source effort for private AI, not just a closed system like Apple’s. People like that the project is transparent and can be checked by anyone. Others wonder how strong the privacy really is, especially if the hardware or cloud provider is not trusted. Some users ask if “provably private” is a fair claim, or if there are still risks. There are questions about real-world deployment—will people run their own nodes, or will it just be used by companies? Some comment that setting this up might be hard for individuals, but it’s great for researchers and companies who care about privacy. A few are excited about using this with their own AI models. Others point out that having open code is not enough—people must check and audit it for true safety. There is some worry about performance and speed, since privacy layers can make things slower. Some compare this to other privacy tech, like zero-knowledge proofs, and are curious how it stacks up. In general, most are positive, but want to see how it works in practice and if it really protects user data as promised.

---

