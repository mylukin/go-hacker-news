# Hacker News 故事摘要 - 2025-07-31

## 今日概述

Today’s top Hacker News stories cover slow projects that last for centuries, new open-source AI tools, old and new ways of programming, updates to network protocols, and useful tech tips. Many stories discuss building things to last, the balance between speed and care, and how open-source software helps everyone. If you like stories about long-term thinking, new AI models, or practical tech advice, there is something interesting to read today.

---

## Slow

- 原文链接: [Slow](https://michaelnotebook.com/slow/index.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44748934)

This article talks about very long-term projects—things humans build or study over many years, decades, or even centuries. The author asks which problems can only be solved slowly, and how we can make organizations to handle these slow projects.

The article gives many real examples. Some math problems, like Fermat’s Last Theorem, took hundreds of years and lots of new math to solve. Cathedrals like Notre Dame took over a century to build, and Sagrada Familia is still not finished after more than 140 years. There are science projects that need a lot of time too, like the Cape Grim Air Archive (collecting air samples since 1978), and the Framingham Heart Study (tracking people’s heart health since 1948). The Central England Temperature record has tracked weather since 1659. LIGO, the experiment that found gravitational waves, started as an idea in the 1960s and only succeeded in 2016. The E. coli long-term evolution experiment has been running since 1988, and the pitch drop experiment started in 1927 and is still going. The “Clock of the Long Now” is being built to last ten thousand years.

The author also mentions tech projects, like Linux or Wikipedia, which could still be important in a hundred years. Some standards like ASCII or TCP/IP might last for centuries, even if they change a little. The 2nd Avenue Subway in New York took more than 70 years before its first section opened. Some companies, like Kongo Gumi in Japan, lasted for over 1,400 years. The Study of Mathematically Precocious Youth has been running since 1971. The article asks which projects actually need to be slow, and which could be made faster.

In the comments, people share more long-term projects and talk about what makes them possible. Some think it’s amazing that humans can plan so far ahead and keep working over generations. Others say big projects often stop or fail because of wars, money problems, or people losing interest. One person wonders if modern society is too focused on fast results and doesn’t support slow, careful work anymore. Some mention how open source software could last a long time if people keep caring about it. Others point out that building for centuries is easier when you have stable governments or traditions, like in some old companies or cathedrals. A few think technology can speed up some slow projects now, but not all—some problems just need lots of time. Some worry that slow projects might not survive big changes in society. Others say it’s inspiring to think about building things that last longer than one lifetime. Some people ask if we even need more long-term projects, or if we should focus on faster results. Finally, a few users share personal stories about being part of slow research projects or buildings, and how it feels to work on something you might never see finished.

---

## Releasing open weights for FLUX.1 Krea

- 原文链接: [Releasing open weights for FLUX.1 Krea](https://www.krea.ai/blog/flux-krea-open-source-release)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44745555)

This article is about Krea releasing the open weights for their new image model, FLUX.1 Krea, which focuses on better aesthetic control and image quality. The model is built in partnership with Black Forest Labs and is meant to avoid the common “AI look” that many other models have.

The main goal is to make AI-generated images that don’t look fake or boring. The team explains that most models today try to score well on technical benchmarks, but this often leads to images with smooth, waxy textures, blurry backgrounds, and less interesting styles. They say that old ways of judging image quality, like FID and CLIP Score, are not enough for today’s advanced models. Even tools made to measure aesthetics, like LAION Aesthetics, often bring their own biases, like favoring soft, bright images or certain subjects. The team believes that human taste and style are too personal to be measured by a simple number.

To create FLUX.1 Krea, they used a two-stage training process. First is pre-training, where the model learns about the world from many different images, even “bad” ones, to understand variety and mistakes. Next is post-training, where they refine the model to focus on their preferred style and remove unwanted traits. They started with a “raw” model from Black Forest Labs that wasn’t already tuned to typical AI styles. Then, they used supervised fine-tuning with carefully picked images, some even made by older Krea models. After that, they used reinforcement learning with human feedback (RLHF), where people picked the best images to teach the model what “good” looks like.

They learned two big lessons: First, quality matters more than the amount of training data—a small, well-chosen set of images works better than a huge, messy one. Second, it’s better to train the model with a clear style or “opinion” instead of mixing everyone’s preferences, which can make results bland. They believe models should overfit to a strong style, then allow users to pick or personalize from there.

In the comment section, many users are happy about the open release and the focus on aesthetics, saying it’s refreshing compared to other models that all look the same. Some developers ask for more technical details about the training data and pipeline, hoping it helps their own projects. A few people worry about the risks of “opinionated” models, saying they might only fit a small group’s taste and limit creative options. Others point out that open weights allow the community to fine-tune the model in new directions, fixing this problem. There’s also debate about whether benchmarks and scores are still useful; some say they help compare models, while others agree with the article that real user feedback is more important. Lastly, some users express excitement for future research about personalizing aesthetics, while others note that collecting enough personal feedback could be hard for smaller teams.

---

## Programmers aren’t so humble anymore, maybe because nobody codes in Perl

- 原文链接: [Programmers aren’t so humble anymore, maybe because nobody codes in Perl](https://www.wired.com/story/programmers-arent-humble-anymore-nobody-codes-in-perl/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44726347)

This article talks about how programmers used to be more humble, and connects this idea to the old programming language Perl. The writer explains that Perl was very popular in the early days of the web, powering big sites like Amazon, Google, and Craigslist.

Perl was known for being messy and hard to read, but it was also flexible. The creator of Perl, Larry Wall, believed that there was not just one right way to write code. In Perl, you could do things in many different ways, just like people use language in many styles. This made Perl code hard to understand, even for the person who wrote it. The writer says Perl felt “human” because it was not perfect and had a lot of quirks, just like real languages. 

The article says that newer languages like Python became popular because they are cleaner and easier to read. The writer himself prefers Python, but he misses the humility that Perl taught. He thinks that modern programming is trying too hard to be perfect and strict, forgetting that computers and code are made for people, not just for machines. He argues that we need to accept that things are messy and complicated, and Perl’s way of doing things can remind us to stay humble.

Looking at top Hacker News comments, some users share fond memories of working with Perl. They say it taught them to be creative and to find their own solutions. Others agree that Perl was hard to read and made big projects confusing, so they like that newer languages are simpler. A few users point out that every language has its problems, and that programmers can be proud or humble no matter what language they use.

Some people in the comments think Perl’s decline is not just about the language, but about changes in the tech industry. They say that as programming has become more popular, more people want clear rules and easy-to-follow code. Others wish that today’s programmers would remember the lessons of the past and not become too proud of their tools. A few even hope that Perl, or its ideas, could become popular again, but most agree this is unlikely. Overall, the comments show a mix of nostalgia, criticism, and hope for a better balance between order and creativity in programming.

---

## QUIC for the kernel

- 原文链接: [QUIC for the kernel](https://lwn.net/Articles/1029851/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44746948)

This article talks about bringing the QUIC network protocol into the Linux kernel. QUIC is a new way for web servers and browsers to talk faster and more securely, but it has mostly lived outside the Linux kernel until now.

QUIC was made to fix problems with TCP, the old protocol for most Internet traffic. TCP makes new connections slower because it needs a “three-way handshake.” It can also get stuck if a single data packet goes missing, stopping everything until that packet is fixed. On top of that, TCP shows a lot of its “meta-data” to anyone in the network, which can be a privacy problem and makes it hard to improve the protocol because “middleboxes” expect TCP to work in a certain way. QUIC, instead, runs on top of UDP, and all its important data is always encrypted, so it’s much harder to block or inspect. It also lets you send many data streams at the same time, so if one packet drops, the others can keep going.

Right now, most QUIC support is in user space, not in the kernel. This was done on purpose to make it easier to update and test, but now, with QUIC more stable, putting it in the Linux kernel could make it even faster and let more programs use it. The new patch adds a “QUIC” type you can use with normal socket system calls, so programs could use it like they use TCP, but there are some differences. For example, QUIC uses TLS for encryption and authentication, but this setup still happens in user space.

The article says the first version of QUIC in the kernel is not as fast as current TCP or even as fast as user-space QUIC yet. Some reasons are missing hardware support (like “segmentation offload”), extra data copying, and the need to encrypt more data. Over time, these problems could be solved, especially as hardware makers add more support for QUIC.

Even though speed is not perfect yet, many projects want to use kernel QUIC. Samba, some file systems, and even curl are looking into it. But it will still be a long time before this work is finished and added to the main Linux kernel—maybe not before 2026.

In the comments, some people are excited to see QUIC finally coming to the kernel, saying it’s about time since so much web traffic uses it now. Others point out the poor performance in early tests, saying there’s no reason to switch yet if TCP is faster. Some users worry that moving protocols like QUIC into the kernel will be hard to maintain and could slow down improvement, while others think it will help make the Internet faster for everyone. A few commenters talk about how hard it is to make changes to old protocols once hardware and middleboxes expect things to stay the same, so adding new protocols like QUIC is important. Some people wonder if kernel QUIC is even needed, since user-space QUIC works well and is easy to update, but others remind them that kernel support can make it easier for more apps to use QUIC without extra libraries. There are also discussions about security, with some saying kernel code is riskier to change, while others think the benefits are worth it. Overall, people agree this is a big step, but there are many challenges left before QUIC is as fast and trusted as TCP in the Linux kernel.

---

## Show HN: Mcp-use – Connect any LLM to any MCP

- 原文链接: [Show HN: Mcp-use – Connect any LLM to any MCP](https://github.com/mcp-use/mcp-use)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44747229)

This article is about MCP-Use, a tool that lets you connect any large language model (LLM) to any Model Context Protocol (MCP) server. With MCP-Use, you can build custom AI agents that use tools like web browsers, file systems, or even 3D modeling software, all using open-source code.

MCP-Use is easy to install with pip and works with many LLMs supported by LangChain, such as OpenAI, Anthropic, Groq, or Llama models. You only need a few lines of code to create an agent. You can set up your agent by using a Python configuration file or by passing settings straight in your code. MCP-Use supports real-time streaming, so you can see the agent’s actions and results as they happen, which is useful for chatbots or dashboards.

There are example use cases for web browsing with Playwright, searching Airbnb listings, and creating 3D objects in Blender. You can connect to MCP servers locally or over HTTP, and even use more than one server at the same time. There is a feature to limit what tools an agent can use, for safety. For running MCP servers without extra setup, you can use a cloud sandbox option.

Debugging is built in, with different levels of logs you can turn on by setting environment variables or using options in your code. The project is open source, has documentation, and welcomes contributions. MCP-Use runs on Python 3.11 or higher and uses the MIT license.

From the comments, some users are excited about how easy it is to connect different AI models to powerful tools using this project. People like that it is open source, as many similar projects are closed or cost money. Some users share ideas about using MCP-Use for building agent workflows or automating tasks in real-world projects.

Others ask questions about security, since giving AI access to tools like file systems and browsers can be risky. There is discussion about the tool restriction feature and how important it is to limit what an AI can do. A few developers are interested in the sandbox mode, as it can help keep the host computer safe.

Some users wish for more easy-to-use documentation and better examples for beginners. There are comments about integration with other frameworks, like LangChain, and some hope for support for other languages besides Python and TypeScript. A few people share concerns about needing many dependencies or running certain versions of Python. Overall, most comments are positive, seeing MCP-Use as a helpful step toward more useful and flexible AI agents.

---

## Gemini Embedding: Powering RAG and context engineering

- 原文链接: [Gemini Embedding: Powering RAG and context engineering](https://developers.googleblog.com/en/gemini-embedding-powering-rag-context-engineering/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44747457)

This article talks about how Google’s Gemini Embedding model is used to make AI systems smarter, especially for things like search, answering questions, and giving context to AI agents. The main focus is on how Gemini Embedding helps with Retrieval-Augmented Generation (RAG) and a new idea called context engineering, where AI uses important information like past conversations or documents to do a better job.

Gemini Embedding is used by different companies. Box uses it to answer questions and pull insights from complex documents in many languages. Their tests show Gemini finds the right answer more than 81% of the time and does better than other models. Financial company re:cap uses it to sort and understand thousands of bank transactions, and they saw their accuracy score (called F1) go up compared to older models. Everlaw, a legal tech company, uses Gemini to search through millions of legal documents with higher accuracy than Voyage and OpenAI models. Roo Code, which helps developers search their code, found that Gemini understands what a developer wants even with unclear queries, making code search more useful. Mindlid, a mental wellness app, uses Gemini to understand chat history and give fast, relevant support, beating OpenAI on speed and recall. Finally, Interaction Co.'s email assistant Poke uses Gemini to quickly scan and find important emails, doing the job much faster than other tools.

The article says these improvements happen because Gemini Embedding packs important info into a small space and works well with different languages. The model helps AI systems use the right context to answer questions, search, and help users more effectively.

In the Hacker News comments, some users are excited about the technical gains and speed improvements. They like how Gemini Embedding helps with multilingual support and makes it easier to search large sets of data. Others say the numbers should be taken with caution since the tests come from the companies themselves, not from independent sources. A few developers ask about the cost of using Gemini versus other models, and whether it’s open source or locked to Google’s platform. There’s a discussion about the “Matryoshka property,” with some people wanting more detail about how it works and why it matters for storage and speed. Some users feel the real-world impact is small for now, mostly helping big companies, but others see it as the start of better AI tools for everyone. A few worry that as context engineering grows, privacy and data security will become bigger problems. Lastly, several developers share tips about trying Gemini in their own projects, saying setup is simple and results look promising, but they want to see more community benchmarking before making a switch.

---

## Ubiquiti launches UniFi OS Server for self-hosting

- 原文链接: [Ubiquiti launches UniFi OS Server for self-hosting](https://lazyadmin.nl/home-network/unifi-os-server/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44746603)

Ubiquiti has released UniFi OS Server, letting people self-host their UniFi network tools on their own computers instead of using Ubiquiti’s cloud hardware. This means you can run UniFi Network, InnerSpace, and even UniFi Identity all together, which was not possible before.

To use UniFi OS Server, you need at least 20GB of free space and either Windows (with WSL 2) or Linux (with Podman 4.3.1+). The article explains that on Windows, installation is simple: download the setup, click next, and wait for it to finish. The program sets up a Linux environment in the background. After starting, you name your server and log into your Ubiquiti account to manage it remotely—though you can skip this step if you don’t need remote features. If you already have a UniFi setup, you can import your old network or restore from a backup.

UniFi OS Server comes with UniFi Network ready to go. If you want InnerSpace, you can add it through the settings. The server keeps running even if you close the window, but you can stop it by exiting from the system tray. You can also access the server from your browser at https://localhost:11443. For Linux users, there are command-line steps: install Podman, download the file, make it executable, and run the installer. For HTTPS, there’s a script to help install a Let’s Encrypt SSL certificate.

The article ends by saying it’s a big step for self-hosting UniFi and hopes that UniFi Protect will be added soon.

From the Hacker News comments, many users are happy about self-hosting, especially those who don’t want to rely on cloud hardware. Some like that it’s now possible to run UniFi Identity and InnerSpace together. A few people are excited to use their own hardware with more power or storage than Ubiquiti’s consoles. Others mention that this could make it easier to manage larger or more complex networks.

However, some users warn about Ubiquiti’s past issues—like surprise firmware changes or cloud lock-in. A few people are not sure if Ubiquiti will keep supporting self-hosted options, remembering when other features disappeared. There are questions about security updates, long-term support, and whether all UniFi services (like Protect) will ever work on self-hosted servers. Some technical users point out the need to open many ports and wonder about security risks.

Others compare Ubiquiti’s move to other network vendors, saying this is a step closer to open systems. Some wish for official Docker support, or easier updates for Linux users. A few people share tips about using WSL and Linux, while some say they are waiting for more features before switching from their current setup. Overall, most people see this as a good change but are careful about trusting Ubiquiti’s long-term plans.

---

## MacBook Pro Insomnia

- 原文链接: [MacBook Pro Insomnia](https://manuel.bernhardt.io/posts/2025-07-24-macbook-pro-insomnia)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44745897)

This article talks about a MacBook Pro with an M1 Max chip that started losing battery overnight, even when not being used. The writer explains how this problem got worse and shares how he tried to fix it.

He first used the Mac’s built-in terminal tool, “pmset -g log,” to look at power logs. These logs were hard to read, so he made his own tool to help understand them better. Even with his tool, he could not find the real cause of the battery drain. He tried changing system settings like “tcpkeepalive,” but nothing helped.

Later, he found a program called Sleep Aid. This app made it easier to see what was waking up the MacBook and allowed him to change some settings quickly. In the app, he noticed that “Wake for maintenance” was turned off. The app said that disabling this could actually cause the Mac to wake up more often. When he turned the setting on again, his MacBook stopped losing battery during the night.

People in the Hacker News comments shared a lot of similar stories. Some said their MacBooks also drained battery while sleeping, and that it was hard to find out why. A few users said Apple’s sleep system is sometimes confusing, and that the logs are too hard to understand for normal users. Others liked the idea of the writer making his own tool to read the power logs.

Some people recommended using tools like Sleep Aid or third-party apps to manage sleep better. Others warned that certain apps, like messaging or cloud syncing tools, can keep Macs awake. A few users said that their older Intel MacBooks didn’t have these problems, and wondered if it was an M1 issue.

Some commenters said “Wake for maintenance” is confusing, because you expect less wake-ups if it is off, not more. There were suggestions to check system settings after every MacOS update, since updates sometimes change sleep behavior. A few people hoped Apple would make sleep management simpler and clearer in the future.

---

## How was the Universal Pictures 1936 opening logo created?

- 原文链接: [How was the Universal Pictures 1936 opening logo created?](https://movies.stackexchange.com/questions/128020/how-was-the-universal-pictures-1936-opening-logo-created)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44744454)

The article explains how the famous Universal Pictures logo from 1936 was made, which shows a spinning globe and bright stars. This logo was special because it was much more complex than the earlier ones and used many effects, even though computers were not used at that time.

To make the logo, the art director Alexander Golitzen used plexiglass and followed the new Art Deco style. The moving stars were built out of thin plexiglass, which was covered with a layer of shiny zinc sulfide—a chemical that glows and is used in things like glow-in-the-dark signs. The stars spun around, and special lights circled them, making bright reflections and patterns. The camera used a very small opening, so only the brightest parts would show, making the stars look like they were shining down their length.

Next, the team joined the stars to a model of the Earth. The globe was painted black and covered inside with a weaker version of the same glowing material, so it would not be too reflective. First, they filmed the globe alone. Then, they projected the footage of the spinning stars onto the globe, so it looked like the stars’ light was moving across it. After that, they made another globe, bigger and also black, and attached the company’s letters to it. This globe was spun by hand and filmed with the glowing letters showing up brightly. They used special camera tricks to put all the different parts together on top of each other, so the final image showed the spinning globe, moving stars, and shining letters all at once. The background used a screen behind the globe for the last layer. It took about six months to finish the logo. Later, the same globe model was reused in another movie as part of a science device.

In the comments, many people were impressed by how much work and skill went into making the logo without computers. One person liked that the project took half a year, saying it showed real pride and care. Another person explained how zinc sulfide works—not just as a reflector, but as something that glows, which is important for the brightness of the stars in the logo. Someone else was amazed that such an advanced effect was made so early in film history. There were also questions about how the shimmering star effect was created, with one user saying it looked like a special camera effect used later in movies. Some users pointed out that the globe model was used again in another film, which they thought was a fun piece of trivia. Overall, most people appreciated the creativity, patience, and technical skill behind making this iconic logo.

---

## Secure boot certificate rollover is real but probably won't hurt you

- 原文链接: [Secure boot certificate rollover is real but probably won't hurt you](https://mjg59.dreamwidth.org/72892.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44747843)

This article talks about Secure Boot certificate rollover and what it means for most users. Secure Boot is a feature on many computers that helps keep the boot process safe by only allowing trusted software to start. Sometimes, the certificates used by Secure Boot need to be changed, or “rolled over,” to newer, more secure ones.

The article explains that Microsoft is now rolling out a new Secure Boot certificate. Some people worry that changing this certificate might stop old systems or software from working. The author says that, for most users, this change will not cause problems. Most computers have already been updated to handle the new certificate, and most operating systems will keep working as before.

The article gives some examples of when problems could happen. If you use old versions of Linux or special bootloaders, you might need to update them. Some people who run custom setups, old operating systems, or self-signed bootloaders could have trouble, but these cases are rare. The article also says that most regular users will not notice anything different. The author reminds people to keep their systems updated to avoid any problems.

In the comment section, some readers are worried about future hardware becoming more locked down. Others are glad that Microsoft is making Secure Boot safer. A few people share stories about older hardware that stopped working when certificates changed in the past. Some developers say this is why they like open-source tools that let them control their own boot process. Others point out that most users will never see any issues unless they use very old software. One commenter explains that Secure Boot is important for stopping some types of malware. Another says the system is too complex and should be simpler. Some people debate if Secure Boot is really needed for everyone. Others wish there was a better way to handle these changes without user worry. A few users mention that Secure Boot can be turned off on most computers if needed. Overall, most agree that this certificate rollover is not a big problem for most users.

---

