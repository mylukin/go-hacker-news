# Hacker News 故事摘要 - 2025-07-31

## 今日概述

Today’s top Hacker News stories cover long-term projects that last for decades, a new open image AI model, and a debate about why homes are expensive. There are also stories about a website made to make people cry, a new network protocol (QUIC) coming to Linux, and how an old movie logo was made. Other stories include self-hosted network tools, Mac battery issues, new AI models, and ways to connect AI to real-world tools.

---

## Slow

- 原文链接: [Slow](https://michaelnotebook.com/slow/index.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44748934)

This article talks about very slow, goal-driven projects that take many years or even centuries to finish. It asks what kinds of problems people can only solve if they work on them for a very long time, and how we can build groups or systems to do this work.

The author lists many famous slow projects. For example, Fermat’s Last Theorem in math took hundreds of years to solve, with many people helping over generations. Big buildings like Notre Dame took more than a century to build. The Sagrada Familia church in Spain started in 1882 and is still not done. There are science projects that keep going for decades, like the Cape Grim Air Archive (saving samples of air since 1978), and the Framingham Heart Study (tracking heart health since 1948).

Some experiments are meant to go on for a long time. The Central England Temperature series has measured temperature since 1659. The LIGO project, detecting gravitational waves, took many years of design and building before it worked in 2016. The E. coli long-term evolution experiment runs since 1988, and the pitch drop experiment started in 1927 and is still dropping. There’s also the Clock of the Long Now, which is designed to last for 10,000 years.

The article wonders if things like Linux, Wikipedia, ASCII, and TCP/IP will still exist in some form after many years or even centuries. Some subway projects, like the 2nd Ave Subway in New York, took almost 75 years to complete. Some companies, like Kongo Gumi, lasted for over a thousand years before closing. The Study of Mathematically Precocious Youth has been running since 1971 to track gifted children. The author also links to more examples of long-term experiments.

He asks readers to think about which projects really needed to take so long, and which could have been faster.

In the comments, some people like the examples and share more long-term projects, such as space missions or old trees. Others say that slow projects can be risky because later generations might lose interest or funding. Some think that slow work helps us build things that last, while quick projects often break or get forgotten. A few say that modern people are too focused on speed, and we should learn from these slow projects. There are worries about whether we can even start such projects today, since many companies or governments want fast results. But some believe that open source software can last a very long time if enough people care. A few mention that slow projects teach patience, and that seeing results after many years can be very rewarding. Others wonder if technology can help us finish big projects faster in the future. There’s also talk about how hard it is to keep groups working together across generations. Some people feel inspired, saying they want to start something slow that could last beyond their own lives.

---

## Releasing open weights for FLUX.1 Krea

- 原文链接: [Releasing open weights for FLUX.1 Krea](https://www.krea.ai/blog/flux-krea-open-source-release)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44745555)

The article is about the open release of FLUX.1 Krea, an image AI model made by Krea and Black Forest Labs, which gives users more control over image style and quality. Krea wanted to make AI images that look less "AI" and more like real art or photos, focusing on their own taste for aesthetics.

The team explains that many AI models have a common "AI look"—like blurry backgrounds, waxy skin, or boring designs. This happens because most models are trained to get high scores on certain tests, not to make images people actually like. Krea’s main goal was to avoid these problems and create a model that can make images with a more natural and pleasing style.

They describe two main training steps: pre-training and post-training. In pre-training, the model is taught about the world and learns many styles and objects using lots of different images, even some with "bad" qualities. This helps the model understand what to avoid when making images later. In post-training, the model is shaped to match Krea’s special style. Here, they use a mix of hand-picked image data and feedback from people, carefully guiding the model to make images that fit their own preferences.

The team learned that using a small amount of very high-quality data is better than using a huge amount of random data. They also found that models trained on broad, mixed feedback often end up with a boring, average look that pleases nobody. Instead, by focusing on a clear, opinionated style, the model gives better results for users looking for a certain kind of art. Krea believes it’s better to "overfit"—or really focus—on one style than to try to please everyone.

In the future, Krea wants to make models that can be personalized for each user’s taste, and let users mix and explore different styles more easily. They thank Black Forest Labs for their help and invite others to join or build on their work.

In the Hacker News comments, some users praise the open release, saying it helps the community and will let more people experiment with image AI. Others like the honest talk about the "AI look" and agree that many AI images feel strange or fake. Some users discuss the importance of good training data and careful curation, saying that too much focus on benchmarks can make art boring. A few people are excited to try the model and see if it really avoids the common AI flaws.

Some commenters mention concerns about "opinionated" models, wondering if this focus on a single style might limit creativity or user choice. Others like the idea, saying it’s easier to use a model that just works for a certain style, rather than needing lots of prompts or tricks. There are also questions about how easy it is to fine-tune or personalize the model for new styles.

A few users share technical thoughts, suggesting ways to further improve aesthetics or reduce unwanted biases. Some ask about the license and how open the model really is, hoping it stays free for both hobbyists and businesses. Lastly, several people thank the Krea team for sharing their process in detail, saying it helps everyone understand the real work behind making better AI art.

---

## The anti-abundance critique on housing is wrong

- 原文链接: [The anti-abundance critique on housing is wrong](https://www.derekthompson.org/p/the-anti-abundance-critique-on-housing)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44750416)

This article looks at why some people say big homebuilders are making homes too expensive by acting like monopolies, and argues that this claim is wrong. The author examines these “anti-abundance” arguments, especially ones that blame big homebuilders for high prices and low supply, and finds little evidence to support them.

The main points start with the author calling experts and researchers quoted by anti-monopoly writers. Many of these experts say their work was misused or misunderstood. For example, one key economist’s research is often used to claim Dallas is an oligopoly, but the economist himself says this does not fit Dallas’s facts—big builders there do not control enough of the market to be called a monopoly. In Dallas, the largest two builders only build about 30% of new homes, and the top six about 50%, far from the 90% you’d expect in a true oligopoly. Also, homebuilding in Dallas per person has gone up, not down.

The author checks other cities too. Out of the 50 largest homebuilding markets in the U.S., only one meets the economist’s threshold for being an oligopoly. Most markets are not that concentrated. The research that anti-monopoly writers use also has problems, like using 2006 (a bubble year) as a baseline, and claiming $100 billion in lost homes each year, which seems unlikely if most big cities are not highly concentrated.

Next, the article talks to a Dallas housing expert who is quoted by anti-monopoly writers; this expert disagrees with the monopoly claims and says zoning and land use rules are the bigger problem. These rules make it hard to build cheaper, starter homes. Another industry analyst, whose charts are often used by anti-monopoly advocates, also says that market concentration is not a big issue; in fact, big builders often help build more homes because they have more resources.

The author then speaks with a monopoly expert at Duke University, who explains that just saying “X companies build Y% of houses” is not enough to prove market power. You need to look at whether prices are going up because of concentration, whether quality is dropping, or whether there are other harms. In homebuilding, new homes compete with old homes, which keeps prices in check.

The article also notes that much of the anti-monopoly argument is just claims based on other claims, without strong data. The chain of sources often leads back to the same few papers or even just news articles that do not have real proof. The author warns that blaming big builders without good evidence can hurt the housing market and make antitrust reform less trusted.

In the comment section, many readers agree with the author that zoning laws are a bigger problem than big builders. Some say local governments make it too hard to build new homes, especially smaller or cheaper ones. Others note that big builders can actually help by building more homes quickly and at lower cost, thanks to their size.

A few commenters do worry about big companies having too much power, but even they admit the evidence for monopoly behavior is weak in most cities. Some point out that small builders went out of business during the last recession, and if only small companies were left, there might be even fewer homes built now. One reader says that focusing on breaking up big builders is a distraction from the real issues of land use and regulation.

Other commenters share their own experiences in the housing market, saying that getting permits and following local rules is the hardest part of building. Some say that big builders just have the money and lawyers to deal with these rules, which small builders do not. A few people argue that both regulation and market concentration matter, but agree that the data does not show big homebuilders are holding back supply on purpose.

In summary, most readers and the article itself think that zoning and building rules are a bigger problem for housing supply and prices than the power of big homebuilders. The claims about monopolies in homebuilding do not fit the facts in most cities, and blaming big builders could do more harm than good.

---

## I made a website that makes you cry

- 原文链接: [I made a website that makes you cry](https://www.cryonceaweek.com)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44705960)

A developer made a website that tries to make people cry by showing sad stories and videos. The goal is to see if a website can really touch your feelings, not just make you laugh or waste time.

The website shows different types of sad content, like stories about losing pets, memories of loved ones, and touching videos. It uses pictures, music, and simple text to help you feel strong emotions. The creator says most websites today are made to make you happy, keep you busy, or sell you things, but very few try to make you feel sad on purpose. They wanted to explore if technology can make people cry the way movies or books do. They tested many types of content to see which ones worked best. Sometimes, the website even asks you questions about your own life to make you think about sad memories. The creator shares that some people did cry, while others just felt a little sad or thoughtful. They also explain that the website does not save your answers and is safe to use.

In the comments, some people say they like the idea and think it is creative. Others wonder if it is healthy to try to make people sad on purpose. A few think sad feelings are important and can help people grow or understand life better. Some users share their own stories about times when websites or games made them cry. Others warn that not everyone wants to feel sad online and that it could upset people who are already feeling down. One commenter suggests the site could help people process grief in a safe way. Another person says it made them remember a pet they lost and they cried a little. Some users talk about how movies and books can make them cry, so maybe websites can too. A few people say they did not feel much and think it depends on the person. Overall, the comments are mixed, with some people excited to try it and others more careful or unsure.

---

## QUIC for the kernel

- 原文链接: [QUIC for the kernel](https://lwn.net/Articles/1029851/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44746948)

This article is about adding QUIC, a modern network protocol, into the Linux kernel. QUIC is already used a lot on the Internet, but until now, it has only worked in user space, not inside the kernel itself.

QUIC was made to fix problems with TCP, the old protocol most websites use. TCP is slow to start because it needs a handshake, and if one packet is lost, all streams can get stuck. TCP also sends connection info out in the open, which can be read by anyone in the middle, and this makes it hard to update or improve TCP. These problems make web browsing slower and less secure.

To solve this, QUIC skips the slow handshake and uses UDP, which is quicker and can handle many streams at the same time. If one stream has a problem, others keep working. QUIC also keeps all its important data encrypted, so middle devices can’t look inside or mess with it. This makes websites load faster and safer.

QUIC is explained in RFC 9000 and is already used by web browsers and big companies like Google. But until now, all QUIC code runs in user space. This is because developers wanted to move fast and not wait for every Linux kernel to update. Now, people want QUIC in the kernel for even better speed and to allow more apps to use it easily.

A developer named Xin Long has posted the first set of patches to add QUIC to the Linux kernel. With these changes, programs can use QUIC in almost the same way as they use TCP, with socket functions like bind(), connect(), and listen(). QUIC uses TLS encryption, and the hard work for certificates is still done in user space, but after setup, data can flow fast. QUIC also remembers past connections, making future connections even quicker.

Right now, the in-kernel QUIC is not as fast as expected. TCP is still faster in some tests, sometimes by a lot. Reasons include missing features like segmentation offload, extra data copying, and needing to encrypt more data. But as hardware makers add support for QUIC and the code gets better, performance should improve.

Even though it’s not the fastest yet, people are excited. Projects like Samba and curl are already working to use kernel-based QUIC. But this code is new, big, and will take time to review before it’s part of the main Linux kernel—maybe not until 2026 or later.

In the comments, some people are happy to see QUIC coming to the kernel. They say it will help with speed and help future-proof the Internet. Others worry that QUIC is very complex, and putting it in the kernel could bring new bugs or security problems. Some users point out that kernel code is hard to change, so if there are mistakes, they could last a long time. A few commenters wonder if QUIC will really be faster, since hardware and kernel support for TCP is already very strong. Some remember how long it took for new protocols like Homa to get accepted, so they think this might take a while too. Others are excited about new uses, like faster file sharing and better performance for cloud apps. There’s also debate about whether the encryption in QUIC will cause trouble for firewalls or network tools that rely on seeing inside traffic. Finally, many agree that even if QUIC is not perfect now, it’s important to keep improving the Internet’s main building blocks.

---

## How was the Universal Pictures 1936 opening logo created?

- 原文链接: [How was the Universal Pictures 1936 opening logo created?](https://movies.stackexchange.com/questions/128020/how-was-the-universal-pictures-1936-opening-logo-created)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44744454)

The article looks at how the Universal Pictures opening logo from 1936 was made. This logo showed a spinning globe with shining stars and moving, glowing letters, and people wanted to know how it was created before computers existed. The work was led by Alexander Golitzen, a famous art director, who used Art Deco style and new materials for the time. 

First, they made spinning stars using thin plexiglass coated with a special shiny and glowing powder called zinc sulfide, which is also used in x-rays and glow-in-the-dark items. These stars were spun and filmed using careful lighting and a tight camera aperture, which made the light move along the stars. The footage of the stars was then projected onto a black globe that had a duller version of the same glowing coating inside. The globe was filmed in front of a large projection screen, making the stars reflect and shine on its surface.

Next, they made another globe, even larger, painted black and polished. This globe had the UNIVERSAL letters mounted on it. The globe was spun by hand and filmed at high speed with the letters lit up. The camera took several passes: one to show the letters reflecting on the globe, one without lighting, and one for the background. All these pieces were combined to make the final logo, showing a glowing rotating world with moving stars and bright letters. The whole process took about half a year to finish.

In the comments, people were amazed at how much work went into this effect, saying it was a masterpiece and even better than some modern logos. Some liked that it took so long—they felt the creators had real pride in their job. One person pointed out that the same globe was used later in a different Universal movie set as a prop. Another commenter explained more about the special glowing powder, saying it was not just reflective but gave off its own light, which made the effect look so good from all angles. Someone else was curious about how they got the "streamer" effect from the stars and wondered if it was similar to later film tricks. Many agreed that for 1936, the result was very advanced and creative, showing what artists could do with physical models and careful filming.

---

## Ubiquiti launches UniFi OS Server for self-hosting

- 原文链接: [Ubiquiti launches UniFi OS Server for self-hosting](https://lazyadmin.nl/home-network/unifi-os-server/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44746603)

Ubiquiti has released a new product called the UniFi OS Server, which lets people run UniFi’s network software on their own hardware instead of only on Ubiquiti devices. This is a big change because before, users needed special Ubiquiti hardware to manage their networks with UniFi, but now they can use their own servers.

The UniFi OS Server is made for people who want more control and flexibility. It allows you to self-host, meaning you install UniFi’s software on a computer or server you already own. This can save money if you already have good hardware. It also helps people who want to customize or scale their networks beyond what Ubiquiti’s hardware can do. The software supports things like network management, security cameras, and Wi-Fi access points. Ubiquiti gives instructions on how to install the software, and it works on many types of computers, including Intel and AMD systems. With self-hosting, you can also keep your network data private and not depend on cloud services. Some people like self-hosting because they can fix problems or update software without waiting for Ubiquiti to release updates for their hardware. The UniFi OS Server is aimed at tech users, small businesses, and home network fans who want more from their network setup.

In the Hacker News comments, some people are very happy about this change. They like that Ubiquiti is giving users more freedom and choice. A few users say this will help them run bigger networks using their own strong servers. Other people wonder about updates and security—will self-hosted versions get updates as fast as Ubiquiti devices? Some worry that running software on their own machines could be harder to manage or troubleshoot. A few users question if Ubiquiti will keep supporting this option or if it will go away in the future. Others point out that this move is part of a bigger trend: more companies are letting users self-host instead of forcing them to buy special hardware. Some people share tips about hardware that works well with the UniFi OS Server. There are also voices who still prefer the old way, saying the all-in-one Ubiquiti devices are simpler for many people. Overall, the comments show both excitement and some caution about making this switch.

---

## MacBook Pro Insomnia

- 原文链接: [MacBook Pro Insomnia](https://manuel.bernhardt.io/posts/2025-07-24-macbook-pro-insomnia)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44745897)

This article talks about a MacBook Pro with an M1 Max chip that started losing battery overnight, even when not in use. The author describes how this problem got worse and led to a deep search for the cause.

First, the author checked the Mac’s power logs using the terminal command `pmset -g log`. These logs were hard to read, so the author made a tool to help analyze them. Even with the tool, the cause was not clear. The author then tried changing power settings, like `tcpkeepalive`, but these changes did not help much. After more research, the author found a program called Sleep Aid. This program shows when the Mac wakes up and gives easy ways to change sleep settings.

In Sleep Aid, the author saw that the “Wake for maintenance” option was turned off. Sleep Aid explained that when this setting is off, the Mac may wake up often. The author turned this setting on, and the battery problem stopped. Now, the MacBook Pro keeps its battery during the night when not plugged in.

Many Hacker News users said they had seen similar battery drain problems with their MacBooks. Some people said that background apps or network settings sometimes wake up the Mac. Others explained that some updates or cloud syncs may also cause wake events. A few comments warned that changing power settings can have side effects, like missing updates or notifications.

Some users liked the Sleep Aid tool and said it helped them solve their own sleep issues. Others suggested checking Activity Monitor for apps that keep the Mac awake. A few commenters said Apple’s documentation about sleep problems is not clear or easy to use. Some people felt that modern Macs have more complex sleep issues than older ones. Others wished for a simple way to always keep the Mac asleep unless it’s opened by the user. Overall, users agreed that tracking wake events and trying tools like Sleep Aid can help fix battery drain problems.

---

## Gemini Embedding: Powering RAG and context engineering

- 原文链接: [Gemini Embedding: Powering RAG and context engineering](https://developers.googleblog.com/en/gemini-embedding-powering-rag-context-engineering/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44747457)

This article talks about Google’s Gemini Embedding model and how it helps AI systems understand and use information better. The model is used in many areas like document search, email, code, finance, and mental health apps.

The article explains that Gemini Embedding is not only for simple tasks like classifying data or searching text, but also for something called context engineering. Context engineering means giving an AI agent all the needed background—like past chats, tools, or documents—so it can help users much better. For example, Box uses Gemini Embedding for answering tough questions about documents in different languages. Their results show Gemini gets the right answer over 81% of the time, better than older models.

In finance, a company called re:cap uses Gemini Embedding to sort and classify thousands of bank transactions. Their tests show that Gemini improved accuracy (called F1 score) more than older models, helping them give better insights to customers.

For legal work, Everlaw uses Gemini Embedding to search through millions of complex documents. Gemini found correct answers 87% of the time, beating models from Voyage and OpenAI. It also uses something called the Matryoshka property, which means it can work with smaller data sizes without losing much quality—helpful for saving space and making searches faster.

For developers, Roo Code uses Gemini Embedding to help find code across many files quickly, even if the search isn’t exact. This makes code search work more like talking to a teammate.

A mental wellness app called Mindlid uses Gemini to keep track of chat history and give better, faster advice. Their system answers in less than half a second and finds the right information 82% of the time, better than the previous OpenAI model.

Another company, Interaction Co., uses Gemini for an email assistant called Poke. With Gemini, Poke can find important emails and information much faster—embedding 100 emails takes only 21 seconds, while it used to take much longer.

The article ends by saying that good embedding models like Gemini are key for future smart AI agents. These agents will need to remember and use lots of information to be helpful.

Commenters on Hacker News have mixed opinions. Some developers are excited about better search and smarter AI assistants, saying Gemini’s performance numbers look strong. Others question if the improvements (like a few percent higher scores) matter much for real users, or if the tests are fair since they’re reported by companies, not independent groups.

Several people talk about the importance of multilingual support, saying it could help more people worldwide, not just English speakers. A few are interested in the Matryoshka property, asking if it really saves space or money at big companies.

A common theme is doubt about “context engineering”—some think it’s just a new name for what embeddings already do, while others see it as a real step forward. Developers share their own struggles with embeddings, mentioning that tuning and updating data is still hard work.

Some also wonder if switching from OpenAI or other models to Gemini is easy, or if it means a lot of new coding. Finally, a few people worry about Google’s history of shutting down products, asking if Gemini Embedding will be around for the long term.

---

## Show HN: Mcp-use – Connect any LLM to any MCP

- 原文链接: [Show HN: Mcp-use – Connect any LLM to any MCP](https://github.com/mcp-use/mcp-use)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44747229)

This article is about MCP-Use, an open-source tool that lets you connect any large language model (LLM) to any MCP server. It helps developers build custom AI agents that can use tools like web browsers, file systems, or even 3D modeling software.

MCP-Use makes it easy to set up an agent with just a few lines of code. You can use many different LLMs, such as OpenAI’s GPT models, Anthropic’s Claude, or Meta’s Llama, as long as they support tool use. The tool uses LangChain to connect with these models and tools. You can set up MCP-Use by installing it with pip, and you can add providers for OpenAI, Anthropic, and more.

Agents can be configured using simple JSON files, so you can quickly switch between different tool setups. MCP-Use supports streaming output, so you get real-time updates as the agent works. There are examples for using web browsing, searching Airbnb, and even making objects in Blender 3D. You can use many MCP servers at the same time, let the agent choose the best one for each job, or pick one yourself.

For safety, you can block the agent from using risky tools like file access or the network. MCP-Use can run everything in a cloud “sandbox,” so you don’t need to install extra programs on your own computer. There’s also strong support for debugging and logging, which helps find and fix problems. If you want more control, you can build your own agent using the provided adapters.

Now let’s look at what people on Hacker News think. Some users are excited about how easy it is to connect LLMs to real-world tools, saying this can make AI agents much more useful. Others are happy that it’s open source and not tied to one company or product. A few people are worried about safety—giving LLMs tool access can be risky if not done carefully, so the tool restrictions and sandboxing features are important.

One commenter likes that you can plug in different LLMs, not just OpenAI, and use whichever works best. Another mentions that the config-file setup makes it simple to test out new tools without much coding. Someone points out that MCP-Use could help with research and building prototypes quickly.

There are also more technical questions: some want to know how secure the sandboxing really is, or how well the system scales for lots of agents at once. A few users say they wish there were more examples and simpler guides for beginners. Finally, some people are interested in using MCP-Use for non-traditional tasks, like controlling hardware or making art, and ask about future support for this. Overall, the reaction is positive, with lots of ideas for what people want to try next.

---

