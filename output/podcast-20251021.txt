Hello everyone, this is the 2025-10-21 episode of Hacker News Daily Podcast. Today, we have a full lineup of tech news, discoveries, and discussions from the developer and hacker community.

First, let’s talk about how Idealist.org slashed their Heroku bill from $3,000 a month down to just $55. The problem was Heroku’s high per-environment costs, especially for staging setups that needed to copy production closely. Each staging environment on Heroku cost about $500, since you had to pay for separate web servers, workers, and database add-ons. Even with small servers, the price was high because costs grew with the number of environments, not just their size. At first, Idealist.org only ran two permanent environments, but they wanted more for development and testing, which would have been even more expensive.

To save money, the team tried a Hetzner server for $55 a month. Using the Disco tool, they kept their easy “git push to deploy” workflow. All staging environments shared one Postgres database, which was fine for testing and saved a lot by skipping Heroku’s database fees. Disco provided more than just Docker Compose: it handled zero-downtime deployments, free SSL certificates, and a web UI for managing logs and environments. After the switch, they could run six full staging environments on one server, where Heroku would have cost $3,000. The server’s CPU and memory use stayed low.

There were some downsides. They had to set up DNS and CDN by hand, and manage their own server security and updates. Hetzner’s servers are mostly in Europe, which could be a problem for US production, but for staging it was fine. The team also had to adjust their app’s network for Docker, which took a day. Six months later, this setup became normal. Staging environments became easy and cheap, so any developer could create one as needed, without permission or cost worries.

In the comments, many praised the big savings and liked how staging became easy and disposable. Some warned that running production this way is riskier because you lose Heroku’s backups and support. Managing your own servers takes extra work, like security updates and monitoring, which not all teams want. Others said Heroku’s high price is for their support and ease of use, which can make sense for less technical businesses. Some suggested cheaper alternatives like Render or Fly.io, but most agreed a simple server is much cheaper if you can manage it. A few shared similar stories of saving thousands by moving off big platforms. But some warned about hidden costs of self-hosting, like time spent on patching, backups, and outages, which can distract from product work. Most agreed that for staging and test environments, this approach gives a big win in both cost and developer freedom.

Next, we look at a website that tracks all the times people have predicted the end of the world and been wrong. It lists hundreds of failed doomsday predictions, from ancient times to now. For each, it shows who made the prediction, what was supposed to happen, and what really happened. It also tracks active predictions, such as the “Fourth Turning,” which says the US will face a crisis by 2026, and the “Limits to Growth” forecast, warning about collapse from 2020 to 2030. The site gives details like the person or group, the type of disaster, and the belief system—religious, scientific, or political.

The site has features like a scoreboard of failed and pending predictions, a timeline, and even lists scams and hoaxes, like the 1806 “prophetic hen” where someone faked messages on eggs. It’s clear from the data that almost every type of prediction has failed—religious, political, even scientific. Some people kept moving the date, others said the event was “spiritual,” or that faith saved the world. New predictions keep appearing, but none have come true.

Hacker News commenters found the site both funny and a bit sad. Some say it’s a lesson in human psychology—people want to believe in big endings or special events. Others noted that even smart people, or whole societies, have made such predictions. Some wonder why this thinking is so common, while others joke that the only successful prediction would mean no one is left to record it. There were comparisons to failed tech predictions or market bubbles, and some said that even scientific forecasts can be wrong, though science at least updates its views. Many said the site is a good reminder to be careful with bold predictions and to look for real evidence before believing big claims.

Moving on, we have an article explaining how to build a simple key-value database from scratch. It starts with saving key-value pairs in a single file. Updates or deletes mean searching the file and changing data, which can be slow. An improvement is to make the database append-only: you only add new records to the file, and mark deletes with special entries. This makes the file grow large and searching slow, so the article suggests splitting data into segments and compacting old files to remove deleted records.

To make searching faster, you use an in-memory index—a hash table mapping each key to its place in the file. This is fast but uses memory, and doesn’t help with range queries. For range queries, the article suggests keeping data sorted and using a “sparse” index. New data is kept sorted in memory, then written to disk in order. If the program crashes, you can lose in-memory data, so you also write to a backup file. This method is called an LSM tree, used in real systems like LevelDB and DynamoDB. LSM trees are fast and good for lots of data, but there are other ways, like B-Trees.

In the comments, people liked the step-by-step approach. Some said building a database from scratch is a great learning exercise. Others pointed out that real databases solve more problems, like handling crashes or scaling for millions of users. Some wished for code examples. There were discussions about trade-offs between LSM trees and B-Trees: LSM is good for write-heavy workloads, B-Trees for read-heavy or range queries. There were also comments about memory use and advanced indexing. Some shared stories about building simple databases and how things get tricky as they grow. Many modern databases mix ideas from both LSM and B-Trees.

Next is rlsw, a software renderer for OpenGL 1.1 inside the raylib library. It is written in C and is less than 5,000 lines of code. This lets raylib programs run on computers with no GPU. The renderer copies OpenGL 1.1 style and features, but everything runs in software on the CPU. It supports basic drawing modes, textures, depth and blend modes, and more. You can configure its limits, and it’s easy to use or plug into other projects. The license is MIT, so it’s open source.

Hacker News users are impressed that such a renderer can be so small. Some like that it lets raylib work on old hardware, virtual machines, or servers without a GPU. Others feel nostalgic about writing similar software. There’s talk about speed—it’s slower than a GPU but useful for learning or simple cases. Some say it’s great for debugging or cloud use. People like the code’s simplicity and readability, making it good for learning 3D rendering. Some discuss its limits for modern games, but find it perfect for simple visuals or retro games. Overall, people are happy to see open-source software rendering alive and plan to try it out.

Now let’s talk about large language models and “brain rot.” The article explains that if you train LLMs like ChatGPT on lots of low-quality, junk internet data, they get worse at reasoning, memory, and ethics. The authors tested this by training LLMs on junk data from Twitter/X, based on popularity and writing quality. They found that models trained on junk became worse at solving hard puzzles, handling long texts, and following safe behavior. Their score on tough challenges dropped, and they started skipping steps in their answers. The bad effects did not go away, even after more training with better data.

The researchers say this shows that training data quality is very important. Junk content can cause lasting problems, and popular posts are a risk sign. AI teams should check their models’ health and be careful with internet data.

In the comments, many agree and say it matches how people feel after too much social media. Others say this result is obvious. Some worry that so much of the internet is low-quality, making it hard to avoid this problem. Some are skeptical about how the junk data was chosen or if the tests really show “cognitive decline.” Others note that popular posts are not always junk, so filtering by engagement could throw out good data too. Many say this makes a strong case for better data curation. Someone joked that maybe LLMs are just becoming more like humans. Most agree the study is a good reminder that what you feed an AI matters—a lot.

Next, we cover why it’s hard for LLMs to work with audio, and how neural audio codecs help. Most LLMs today use text, so they miss emotion or tone in your voice. Audio has many more data points per second than text, making it slow and hard to model. Early models like WaveNet were slow and sounded bad. Neural audio codecs compress audio into tokens, which are easier for LLMs to handle.

The article explains that an autoencoder compresses audio, then vector quantization turns it into tokens. Residual vector quantization uses layers to keep quality high. The author built a simple codec and tried turning audio into tokens, feeding them to a model, then turning them back into audio. More quantization gives better sound, but not perfect. The article introduces Mimi, a modern codec using advanced techniques, which produces much better audio and is used in new models like Moshi and Sesame CSM.

The article also talks about “semantic tokens,” which carry meaning without details about the speaker. These help models focus on what is said, not just how. Models trained with semantic tokens are better at generating real words and making sense, though they still make mistakes. Even with the best codecs, audio LLMs are still behind text models in understanding and reasoning.

In the comments, some praise the clear explanation and code samples. Others are excited about helping LLMs understand emotion and tone. Some say speech is much more complex than text and models need better ways to capture meaning from audio. There are questions about music, sound effects, and audio search. Some worry about the size and cost of training, and about privacy if models can “hear” emotion. Some are skeptical, saying real understanding of emotion or sarcasm will need new model designs. Others share their own experience building audio models and agree that handling long audio is still a challenge. Many agree that audio LLMs are not perfect, but the work is promising.

Now, let’s talk about OpenFGA and running authorization checks fully inside Postgres. The team rewrote parts of OpenFGA, an authorization tool, to run inside their main Postgres database, because keeping two databases in sync was hard and caused problems. Authentication—who you are—is easy, but authorization—what you can do—is tricky. Their app needed ReBAC, relationship-based access control. OpenFGA uses relationship tuples like {user, relation, object}. But having a separate database meant extra work with syncing, rolling back changes, and deleting users.

The team thought about tools like Debezium but decided it was too complex. Instead, they built the same model in Postgres, using SQL tables, views, and recursive functions. This let them use Postgres features like cascading deletes and made things easier to maintain. They also wrote scripts for schema changes. The rewrite made their authorization system simpler and better for their needs, and they open-sourced it for others to use.

In the comments, many agreed that keeping auth and main data in sync is a pain, and liked using a single database. Some worried about performance with complex checks, but the author said this can be solved with materialized views. Some said this method is not always possible for big or high-security companies, but for most SaaS apps it makes sense. Others liked that it uses standard SQL and doesn’t lock you in. Some warned that as the system grows, you may still need to split services, but starting simple is smart. Many liked the practical approach for small teams.

Next, there’s a new math idea about undoing a rotation with a special move, like hitting a reset button. Usually, when you spin something, you need to spin it back to undo the move. But this new method lets you reset the spin in a clever way, using “quaternions,” which explain rotations in three or four dimensions. Quaternions are used in computer graphics and robotics.

The mathematicians found that if you do a certain loop—a path with the object—it can undo the spin. It’s like twisting your arm, moving it in a circle, and ending up untwisted, without spinning it back. This is called the “plate trick,” famous in physics. The new discovery is a hidden way to reset the spin, which could help robotics, video games, or quantum computers.

In the comments, people are excited and share stories about learning the plate trick or using quaternions in programming. Some say the trick connects to quantum physics, where particles have strange spin rules. Others say this could help robots move more smoothly or fix problems in 3D models. Some want simple examples or animations. Many agree this discovery shows that math can still surprise us, even with old ideas.

Let’s move on to John Searle’s famous 1980 paper, “Minds, Brains, and Programs.” Searle wanted to show that computers running programs do not really “understand” like people do. He explains the difference between “strong AI” (a computer really understands) and “weak AI” (computers are just tools). Searle disagrees with strong AI.

He uses the “Chinese Room” thought experiment: a person who doesn’t know Chinese sits in a room, gets Chinese symbols, and follows rules to reply with other symbols. From outside, it looks like he understands, but he is just following the rules. Searle says computers work the same way—they process symbols but don’t know their meaning. Real understanding, he says, needs more than just symbol manipulation; it needs the brain’s physical processes.

Searle answers replies: some say the whole system (person, rulebook, room) understands, but he disagrees. Others say a robot with sensors might understand, but Searle says not unless it has the same powers as a brain. He says simulating a brain is not enough, just as simulating fire doesn’t burn.

In the comments, people have many opinions. Some agree with Searle, others think the line between simulating and real understanding is not so clear. Some say humans also follow rules, and maybe understanding can emerge. Others argue that “intentionality” is not well defined. Some find the Chinese Room convincing, others say if a robot behaves like a human, maybe that’s enough. Some say Searle’s argument is philosophical and may not match science. Others point out that brains are also just networks processing signals. Many agree this debate shows how AI is still a mystery, and it’s important to keep discussing what it means to “understand.”

Finally, NASA might remove SpaceX from the mission to bring astronauts back to the moon by 2027. NASA’s chief said SpaceX is behind schedule, and the US is in a race with China to land people on the moon first. SpaceX got a $2.9 billion contract to build the lander, but Starship has had several failed flights and only a few short tests worked this year. NASA is now thinking of letting other companies, like Blue Origin, compete for the job. Blue Origin is already working on a lander for later missions and might be asked to step in sooner. NASA is also asking the whole space industry for ideas to speed things up. Some experts worry that the landers are very complex, and in-space refueling has never been tried. Another company, Dynetics, might join, but it’s not clear.

The rush is because China says it will send astronauts to the moon by 2030. US leaders want to make sure they get there first. Blue Origin says they are ready, but NASA might even look for new companies. The rules for picking a new company are not clear yet.

In the comments, some people think NASA is right to keep options open. Competition can lead to better results. Others worry that changing plans now will add more delays, as new companies must catch up. Some say both SpaceX and Blue Origin face hard problems, and delays may not be anyone’s fault. A few think NASA is making the mission too political. Some support SpaceX, saying test failures are normal. Others worry that splitting the job means no one has enough funding. Some wonder if rushing to beat China is worth risking safety. Others think NASA’s process is too slow and full of paperwork. A few hope more companies join, to bring new ideas. Many are just excited to see progress toward going back to the moon.

That’s all for today’s Hacker News Daily Podcast. Thank you for listening, and we’ll be back tomorrow with more stories and discussions from the world of technology.