Hello everyone, this is the 2025-07-31 episode of Hacker News Daily Podcast. Today, we have a rich mix of tech stories, from patient projects that last centuries, to the latest in AI, networking, and even internet art.

Let’s start with an article about very slow, goal-driven projects—those that take decades or even centuries to finish. The author invites us to think about problems that need many years and many people to solve, and how we can build teams or systems to support such work. He gives examples like Fermat’s Last Theorem, which took hundreds of years and many mathematicians to solve, and buildings like Notre Dame and the Sagrada Familia, which have spanned generations. There are also science projects like the Cape Grim Air Archive and the Framingham Heart Study, which have tracked air and health for decades. Some projects are designed to last, like the Central England Temperature series or the Clock of the Long Now, which is supposed to run for 10,000 years.

The author wonders if things like Linux, Wikipedia, ASCII, and TCP/IP will still be around in some form after many more years. He points out subway projects that took almost 75 years, companies like Kongo Gumi that lasted over a thousand years, and long-term studies like the Study of Mathematically Precocious Youth. He asks readers to think about which projects truly needed such a long time, and which could have been faster.

In the comments, readers share more examples, such as old trees or space missions. Some worry that slow projects risk losing support over time, while others say slow work helps us build things that last. There’s debate on whether today’s culture, which wants fast results, can even start such projects. But some believe open source software can last a long time if people care enough. Others say slow projects teach patience and can be very rewarding. There’s also talk about the challenge of keeping teams working together across generations. Some readers feel inspired to start slow projects that may last beyond their own lives.

Next, we have news about the open release of FLUX.1 Krea, an image AI model from Krea and Black Forest Labs. This model gives users more control over image style and quality, aiming to avoid the usual “AI look”—like blurry backgrounds or waxy skin. Krea’s team wanted their AI to make images that feel more like real art or photos, focusing on their own taste for aesthetics.

They explain that many AI models are trained to get high test scores, not to make images people actually like. Krea used two main training steps: first, pre-training with lots of varied images, even some with “bad” qualities, to help the model learn what to avoid. Then, post-training with hand-picked data and human feedback to shape the model’s style. They found that a small amount of high-quality data works better than a huge amount of random data. Krea believes it’s better to focus on a single style than to try to please everyone.

In the future, Krea wants to let users personalize models for their own taste and mix different styles. In the comments, many users praise the open release, saying it helps the community and pushes AI art forward. Some agree that many AI images feel strange or fake, and careful curation of data is important. Others wonder if focusing on a single style might limit creativity, but some like having a model that “just works” for one type of art. There are technical questions about fine-tuning, licensing, and how open the model really is. Many thank the Krea team for sharing their process in detail.

Now, let’s talk about a debate in the US housing market. Some people claim that big homebuilders act like monopolies, making homes too expensive. The article looks into this and finds little evidence to support it. Many experts, whose work is often used by anti-monopoly writers, say their research has been misunderstood. For example, in Dallas, the biggest two builders only make 30% of new homes, which is far from a monopoly. Most big US cities do not have markets dominated by a few builders.

The real problem, experts say, is zoning and land use rules that make it hard to build cheaper homes. Larger builders can actually help by building more homes quickly, thanks to their resources. The article warns that blaming big builders without proof could hurt the market and make antitrust reforms less trusted.

In the comments, most readers agree that local rules and permits are the main barriers, not big builders. Some worry about big companies getting too powerful, but admit the evidence for monopoly is weak. Others share stories of struggling with permits and local rules. A few say that both regulation and market concentration matter, but the data does not show big homebuilders are holding back supply on purpose. In summary, most readers and the article think that zoning and building rules are the bigger problems for home supply and prices.

Switching gears, there’s a unique project: a developer made a website designed to make people cry by showing sad stories and videos. The goal is to test if a website can touch your feelings, not just entertain or distract. The site uses pictures, music, and simple text, and sometimes asks users questions about their own life to trigger emotions. Some people did cry, others just felt thoughtful or a little sad. The creator says the website is safe and doesn’t save your answers.

In the comments, some people like the idea and think it’s creative. Others worry if it’s healthy to make people sad on purpose. Some users share their own stories about crying at websites or games, while others warn it could upset people who are already feeling down. Some think sad feelings are important and can help us grow. Overall, reactions are mixed, with some excited to try it and others more cautious.

Next, let’s talk about networking. There’s news about adding QUIC, a modern network protocol, into the Linux kernel. QUIC is already used widely on the Internet, but until now, it only worked in user space, not in the kernel. QUIC was made to fix problems with TCP, such as slow startup and blocking issues. QUIC uses UDP, supports many streams at once, and encrypts important data, making web browsing faster and safer.

A developer named Xin Long has posted patches to add QUIC to the Linux kernel. This will let programs use QUIC like TCP, with familiar socket functions. Right now, in-kernel QUIC is not as fast as TCP in tests, but this may improve as hardware and code get better. Projects like Samba and curl are already preparing to use kernel-based QUIC, but it may take until 2026 or later to be included in mainline Linux.

In the comments, some people are excited, saying it will help speed and future-proof the Internet. Others worry about the complexity of QUIC and possible new bugs or security issues. Some wonder if QUIC will really be faster, since TCP support is already strong. There’s debate about how encryption might affect firewalls and network tools. Many agree that even if it’s not perfect now, improving core Internet protocols is very important.

For a bit of movie history, there’s an article about how the Universal Pictures logo from 1936 was made, before computers. The logo showed a spinning globe with shining stars and moving letters. Art director Alexander Golitzen used Art Deco style and new materials. They created spinning stars from plexiglass coated with zinc sulfide, a glow-in-the-dark powder. The stars were spun and filmed with special lighting, and the footage was projected onto a black globe. Another globe, larger and polished, had the UNIVERSAL letters. It was filmed in several passes, with different lighting, and all the pieces were combined for the final effect. The whole process took about six months.

In the comments, people were amazed at the effort and skill. Some said the effect was better than modern logos. There was interest in the glowing powder and curiosity about the “streamer” effect from the stars. Many agreed it was a creative and advanced result for 1936, showing what artists could do with physical models and careful filming.

Now let’s talk about Ubiquiti’s new UniFi OS Server. This lets people run UniFi’s network software on their own hardware, not just on Ubiquiti devices. It’s a big change for users who want more control, flexibility, or privacy. The software can be installed on many types of computers, and supports network management, security cameras, and Wi-Fi. Self-hosting lets you update and fix your system without waiting for Ubiquiti.

In the comments, many users are happy about this move, saying it gives more freedom. Some plan to run larger networks on their own servers. Others worry about updates, security, and whether Ubiquiti will keep supporting this option. Some prefer the old all-in-one devices for their simplicity. Overall, the reaction is positive but cautious.

Next, there’s a story about a MacBook Pro with an M1 Max chip losing battery overnight. The author used power logs and even created a tool to analyze them, but couldn’t find the cause. After more research, the author tried a program called Sleep Aid, which revealed that turning on the “Wake for maintenance” setting fixed the problem. Now the MacBook holds its battery during the night.

Many users said they had similar problems. Some suggested background apps or network settings can wake up Macs, and warned that changing power settings can have side effects. Some liked the Sleep Aid tool, others suggested checking Activity Monitor for apps that keep the Mac awake. There’s a feeling that modern Macs have more complex sleep issues, and users wish for a simpler way to keep the Mac asleep. Overall, tracking wake events and using tools like Sleep Aid can help fix battery drain.

Moving on, there’s an article about Google’s Gemini Embedding model, which helps AI systems understand and use information better. Gemini is used in many fields, like document search, finance, coding, and mental wellness. The model is not just for simple search, but for “context engineering,” which means giving AI all the background it needs to help users well. Companies like Box, re:cap, Everlaw, Roo Code, Mindlid, and Interaction Co. use Gemini to improve accuracy and speed in their services.

In the comments, some developers are excited about better search and smarter AI assistants. Others question if small improvement percentages matter much for real users, or if the tests are fair. Some talk about the importance of multilingual support and the Matryoshka property, which can save space and speed up searches. There’s debate about whether “context engineering” is really new, and some share their own struggles with embeddings. Some worry about Google shutting down products, and whether Gemini Embedding will last.

Another technical article discusses MCP-Use, an open-source tool that lets you connect any large language model to any MCP server. This helps developers build custom AI agents that can use tools like web browsers, file systems, or even 3D modeling software. MCP-Use is easy to set up, supports many LLMs, and uses LangChain for connections. You can configure agents with simple JSON files, stream output, and use multiple servers at once. There are safety features like tool restrictions and sandboxing.

In the comments, users are excited about connecting LLMs to real-world tools and happy that it’s open source. Some worry about safety but like the sandboxing feature. Others like that you can plug in different LLMs and try new tools easily. Some want more examples and simpler guides. There’s interest in using MCP-Use for research, hardware control, or art, and questions about security and scaling. Overall, people are positive and have many ideas for what to try next.

That’s all for today’s episode. We saw how slow projects shape our world, got updates on AI, networking, movie history, and self-hosted tech, and explored how developers are solving real problems and building new tools. Thank you for listening to Hacker News Daily Podcast. See you tomorrow!