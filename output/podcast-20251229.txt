Hello everyone, this is the 2025-12-29 episode of Hacker News Daily Podcast. Today, we have a wide range of stories for software developers and tech enthusiasts. Let’s get started.

First, we look at a story about a business owner whose Google Ads suddenly stopped working. For years, Google Ads brought in new customers for their circus and magic shows. But recently, even after spending much more money and using bonus ad credits from Google, there were no new leads or sales. The author checked their ad campaigns more often, tried different settings, and even increased their budget, but nothing helped. After using up all the ad money, the owner decided to stop using Google Ads.

Now, the business is testing ads on TikTok and Instagram, because young people are there. Since half of their customers are returning clients, they started sending more email newsletters to remind people about their shows. The owner also plans to do in-person marketing, like going to markets to perform free shows and hand out flyers. They are working on new products, such as a Magic Poi project, and even offer to build websites or IoT projects for others.

In the Hacker News comments, many people agree that Google Ads are not as good as before, especially for small businesses. Some say Google’s ad system is now too complex and expensive. A few mention that search results on Google are getting worse, so people may use Google less. Some warn that social media ads can also waste money, but they might work better for a younger audience. Others suggest old-school methods like email newsletters and in-person networking, saying these are still powerful. There are also stories about word of mouth and building a strong community. Some developers joke that maybe it’s time to build a new kind of search or ad tool. Most agree that finding new customers is hard now, and small businesses need to try many things, not just depend on Google.

Next, let’s talk about a technical project: building a simple key/value server in Zig that only allocates memory once at startup. The author created a Redis-like server, called "kv", to learn about static memory allocation. The server asks the operating system for all the memory it will ever need when starting, and never allocates or frees memory again during normal operation. This avoids bugs like use-after-free and makes the server more predictable. It also means the server has clear limits, like the maximum number of connections or data it can handle.

For client connections, the server uses pools. When a client connects, it takes an object from the pool; when done, it returns it. If the pool is empty, new connections are rejected. The same idea is used for data buffers. Commands are parsed using a fixed-size buffer that resets after each request, which allows for fast, single-threaded processing.

For storing data, the server uses a hash map that does not own its memory. Instead, it uses a big pool of pre-allocated space for keys and values. This means the server must know its limits in advance and may waste memory if usage is less than the maximum. Deleting keys can be tricky, since the map must track empty spaces. The author suggests that a custom map might work better in the future.

The server’s memory use is set by configuration: number of connections, max key size, and so on. For 1,000 connections and 1,000 keys, it uses about 750 MB of memory right away. If you raise the limits, memory use grows fast.

In the comments, some praise Zig for making manual memory management easier and safer. Others note that static allocation is common in embedded or high-reliability systems, but not always needed in general apps. A few worry about wasted memory or the risk of setting limits too low. One person likes the predictable behavior, while another asks if it’s possible to reclaim memory after clients disconnect. Some suggest more advanced data structures. A few share stories about building servers with static allocation, saying it helped catch bugs early. Many agree that thinking through memory needs up front leads to better code, even if it feels limiting.

Now, let’s move to data visualization. An article compares three ways to show disk space usage: flame graphs, tree maps, and sunburst charts. Using the Linux kernel source code as an example, the author explains how each method works.

Flame graphs use long, labeled rectangles. The length shows how much space a folder or file uses. It’s easy to see the biggest folders, like “drivers”. Small files appear as thin lines and are less important. Tree maps use each box’s area to show size, but it’s harder to compare sizes quickly. Some tools let you hover for details, but screenshots don’t help. Labels are often missing, making them harder to read. Sunburst charts are round flame graphs. They look nice, but the way slices are drawn can be confusing—sometimes a smaller-looking slice is actually bigger. It’s harder to judge size by angles.

The author also mentions text tools like ncdu and du. Ncdu gives a simple text view with bars but only shows one level at a time. Du just lists sizes as numbers.

The advice is to offer all three visualizations if you’re building a tool, but flame graphs are usually best for seeing the big picture.

In the comments, some prefer flame graphs because they’re easy to read. Others like tree maps for finding big files. A few think sunburst charts look nice but can be misleading. Many say labels are important; without them, you get lost. Some say text tools are still fastest for daily use. One person says the best tool depends on your goal: flame graphs and tree maps both help, but in different ways. Another says flame graphs are also great for showing call stacks in code. Some suggest combining the methods or letting users pick. There are also reminders that visualizations can be hard for colorblind people, so clear labels and good design are key.

Our next story looks at why some people, especially the rich and powerful, focus on escaping Earth or preparing for disaster instead of fixing problems here. The article talks about “preppers” and tech elites, like Elon Musk, who want to leave the planet. It starts by discussing the “Left Behind” books, which describe a Christian idea where believers go to heaven and others must survive on a ruined Earth. This connects to modern preppers who build bunkers and store supplies.

The article then talks about wealthy people preparing in different ways, such as funding Mars missions or building futuristic, safe company headquarters. The authors say these ideas show a wish to escape Earth, not fix its problems. Some projects, like Apple’s “Spaceship” campus or the NSA’s control room, look futuristic but only help a few. The article calls this a “secular Rapture,” where only some get to escape while most are left behind.

In the comments, some agree that it’s worrying when the rich plan to leave Earth or save only themselves. Others think prepping makes sense because governments are not fixing big problems. Some point out that space projects have given us useful technology and can inspire people, but say we should not give up on Earth. A few feel the article is too harsh and believe that trying to leave Earth is a normal human dream. Some say prepping and survivalism are not new, and people always worry about disaster. Others mention that not everyone can afford to prepare, so these projects can make inequality worse. Some say it is possible to work on both space and Earth problems at the same time. The discussion shows people are divided: some feel we are ignoring real problems, while others see looking to the future as part of progress.

Moving on, let’s talk about the idea that AI and no-code tools will replace software developers. The article’s author has been a programmer for over 40 years and has seen these claims many times before. Each time, new tools came—visual programming, macros, no-code, and now AI code assistants—but demand for programmers only increased.

The hard part of programming, the author says, is turning human ideas into exact computer instructions. Even if AI helps write code, people still need to decide what the software should do. Layoffs in tech are more about business reasons, not AI replacing developers. AI code tools do not really understand code like humans—they just predict what looks right, which can lead to mistakes.

The writer does not believe AI will soon replace programmers. Instead, AI will help with small tasks, but important work will still need people. Big AI models also cost too much to run and update, so companies will still need programmers.

In the comments, many agree, saying every new tool just means more demand for programmers. Some think AI will change some parts of programming, making simple tasks easier, but not replacing human skill. Others are more hopeful about AI, but admit it cannot fully replace human judgment now. Some point out that as tools get better, problems get harder, so humans are always needed at the edge. There are warnings about too much dependence on AI, which could make code harder to fix. Many agree that programmers are needed to check AI’s work and keep software safe and reliable. Overall, most believe software developers will stay important, even as new tools come and go.

Next, we have a story about websites blocked by German internet providers. The list, tracked on cuiiliste.de, includes domains like Anna’s Archive, bs.to, and several streaming or file-sharing sites. The CUII group suggests which domains should be blocked, mainly for copyright reasons. The list shows which domains were added and when. Some are known for sharing books, movies, or music without permission. The article does not explain how the blocking works, but in Germany, ISPs often use DNS blocking. The list helps people see what is being censored and when.

In the comments, some worry about internet censorship, saying blocking whole domains can stop access to legal content too. Others point out that DNS blocks are easy to bypass with VPNs or other DNS servers. Some think blocking is needed to protect copyrights, especially for small creators. Others share ways to get around blocks, while some warn that easy bypassing could lead to stricter controls. Some say blocking lists should be public, so everyone knows what is censored. There are also worries about internet freedom and open access. Some compare Germany’s approach to other countries and fear it could become stricter. Others ask why legal streaming is not enough to stop piracy, or argue that blocking does not fix the real problems, like high prices or bad service.

Now, let’s look at large language models (LLMs) like ChatGPT and how they are compared to “humans” in research. The authors say that most studies use test data from people in Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies, but humans are not all the same. LLMs are trained mostly on Western data, so their answers match people from those backgrounds, but less so for others.

Ignoring this can lead to mistakes, especially when using LLMs in places with different cultures. The authors suggest using more data from a wider set of people and cultures, to make AI more fair and accurate for everyone. In the comments, some agree this is a big problem, similar to issues in psychology and medicine. Others say it’s hard to find enough data from less-represented cultures. Some think this bias may never go away, since the internet is dominated by Western sources. There is debate about whether it’s better to have specialized AIs for each culture or one big, global model. Some say users should always know about these biases, especially in health or law. Others wonder if adding more translated texts would help, but some reply that translation can change meaning. Some are hopeful that as more of the world goes online, future LLMs will become more inclusive. Others warn that just adding data is not enough; creators must also respect cultural context.

Next, we have a story about a website that lists all the games removed, or “delisted,” from Steam. The site shows over 1,000 games you can’t buy on Steam anymore, with details about each one. Games can disappear because of expired licenses, business problems, or legal issues. Some popular titles have been removed, and while a few have come back, many stay gone for years. The site also tracks games about to be delisted, so fans can act fast. There are ways to help get lost games back, and the site helps preserve game history.

In the comments, people say it’s sad to see so many games disappear, including big, famous titles. Some are frustrated that, even if you paid for a game, you might not be able to download it again if it’s removed. Some worry this will get worse as more games go digital. A few mention that piracy or backups may become the only way to keep old games alive. Others think companies should do more to let players keep access, or at least give warnings before removal. Some suggest an official “vault” for delisted games. A few say this problem affects movies, music, and books, not just games. Many thank the website for helping to preserve game history.

Our next story is about Aroma, a tool for detecting any TCP proxy just by measuring round-trip time (RTT) differences. The creator uses Fastly’s edge servers and simple math to check if someone is using a TCP proxy, without IP lists or databases. The tool looks at the minimum RTT and the smoothed RTT. If you’re using a proxy, especially a distant one, these numbers are more different.

Aroma can catch commercial services like Cloudflare WARP when it acts as a TCP proxy. It does not always catch VPNs or proxies at other network layers. The code is a proof of concept and not for production yet. You can try it to see if your connection is “allowed” or blocked. The creator explains that network timings are complex, and proxies make these timings different. There are examples to show how RTTs change.

In the comments, some find the idea clever. Others point out that similar fingerprinting is already used to block bots or suspicious traffic. Some worry this could make it harder to use proxies for privacy or to bypass censorship. A few discuss ways to defeat this detection, like adding delay, but note this would slow down connections. Some say timing attacks have been used in security for a long time. There are comments about how the method might not work well with mobile or unstable connections. Some like that the code is open and hope it leads to more research. Others debate if this kind of detection is fair to users who need proxies. A few developers share ideas for improving proxy tools.

Finally, we cover a new C++ hash table that uses grouped SIMD metadata scanning for better speed in big tables. It beats other top hash tables when the table has over 500,000 items, mainly for lookups. The key idea is to search 16 slots in a row (a group), then jump to another group farther away, using a quadratic pattern. This way, SIMD can read all 16 slots at once and make lookups faster. Each slot has a small metadata byte to skip impossible matches. For big tables, lookup hits are up to 1.69 times faster, and misses are 1.21 times faster than the popular ankerl::unordered_dense table. Inserts are a bit slower. The table is header-only, works with C++17, and needs SSE2. There is no deletion or resizing yet.

In the comments, some are excited about the clever use of SIMD and mention similar ideas from Google’s Swiss Tables. Others ask about the lack of deletion and resizing, saying these are important for real use. Some worry about the fixed size and SSE2-only support, since many servers use ARM. Some compare it to other open source tables and share tips on performance tuning. There is discussion about the trade-off between insert and lookup speed. Some hope the author will add deletion and resizing, while others just enjoy seeing new research in hash tables. Overall, the grouped probing trick is seen as interesting and a good addition to the open source world.

That’s all for today’s episode. Thank you for listening to Hacker News Daily Podcast. We’ll see you next time.