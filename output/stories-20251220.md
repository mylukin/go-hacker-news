# Hacker News 故事摘要 - 2025-12-20

## 今日概述

Today’s top Hacker News stories cover creative uses of tech: Excel as an esport, a huge backup of Spotify music, building hardware demos without a CPU, and advice on better software logs. There are also stories about coding 3D models, running GPUs on a Raspberry Pi, switching to IPv6, self-hosting Postgres, AI playing Pokémon, and a new fast Postgres index for wildcard searches. Most stories show people pushing tools in new ways or making tech easier and faster for everyone.

---

## Ireland’s Diarmuid Early wins world Microsoft Excel title

- 原文链接: [Ireland’s Diarmuid Early wins world Microsoft Excel title](https://www.bbc.com/news/articles/cj4qzgvxxgvo)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46339031)

Ireland’s Diarmuid Early just won the 2025 Microsoft Excel World Championship in Las Vegas, beating out 256 people from all over the world. The event is like a big sport final, with lights, music, fans cheering, and even live commentators, but the action happens on spreadsheets instead of a field or court.

Diarmuid is called the “LeBron James of Excel” because he’s so good. He took the title from Andrew Ngai, a three-time champion from Australia. The competition had a $60,000 prize pool, and Diarmuid won $5,000 and a big championship belt. The finals are tough: every five minutes, the person in last place gets kicked out, so you always have to be fast and careful. Most challenges are about solving tricky problems with Excel, like finding your way through a maze or sorting playing cards—so it’s not just about math or finance. Diarmuid says you must think quickly and use Excel in clever ways. He also shares his skills on YouTube, though he doesn’t like TikTok or Instagram much. Diarmuid now runs his own company in New York, and many clients come to him because of his Excel fame.

The Excel esports scene is growing fast. There’s a big community online, and more people join all the time. Diarmuid likes the fun side of it and says he doesn’t take himself too seriously, even though people compare him to NBA stars.

On Hacker News, many readers were surprised that Excel competitions are so popular and exciting. Some people joked about how strange it is to see a “sport” for office software, but others were impressed by the skills needed. A few commenters said they use Excel every day and never thought it could be this intense or fun. Others liked the idea of turning boring work tools into something you can compete in. Some people worried that focusing on Excel might make people ignore learning more powerful tools, but others said Excel is everywhere and important for many jobs. There were also comments about how these competitions help show that “nerdy” skills can be just as cool as sports. A few people wanted to know how to join or watch future competitions, and some even shared their own tips for using Excel faster. Overall, the story made many readers smile and see spreadsheets in a whole new way.

---

## Backing Up Spotify

- 原文链接: [Backing Up Spotify](https://annas-archive.li/blog/backing-up-spotify.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46338339)

This article talks about how Anna’s Archive made a huge backup of Spotify’s music and data. They scraped metadata and music files for about 256 million tracks and made it all available in torrents, with a total size close to 300TB.

The backup includes nearly all Spotify tracks’ metadata, and about 86 million music files—covering 99.6% of what people listen to. The files are sorted by how popular they are, and the most popular songs are kept in higher quality. Less popular songs are in a smaller file format to save space. Anna’s Archive usually saves books, but they wanted to help preserve music too. They saw that other music archiving efforts focus too much on famous artists, very high quality files, or don’t have a complete list. Their goal is to make a full, open music archive that anyone can mirror and use.

The collection is released in steps: first the metadata, then music files, then more details like album art. The metadata is saved as SQLite databases. The music is saved in a special file format, with extra info (like title, album, and cover art) added to each track file. They also fixed problems like duplicate tracks and missing info. The archive covers almost everything up to July 2025.

They analyzed the data and found interesting things: most Spotify songs are never played, and just a few songs get most of the listens. Most songs are singles, not part of albums. The music includes many AI-generated tracks, making it hard to find what’s valuable. They also looked at genres, song lengths, and track features like loudness, danceability, and tempo.

Now, let’s look at the top Hacker News comments. Some people think this is great for preserving music history, especially in case Spotify removes songs or goes away. Others worry about the legality, saying this could be seen as piracy. Some ask if it’s really “preservation” if most of the world can’t legally use the files. There are also concerns about the huge size—few people have hundreds of terabytes free to download or share. Some users are impressed by the technical work, like getting so much metadata and making it easy to query. Others point out this could help with music research, finding lost tracks, or making better music apps. A few people say they wish there were more focus on rare or deleted music, not just what’s already popular. Some say this project might push Spotify or labels to rethink their control over music. Finally, several users give ideas to improve the archive, like adding more ways to download single tracks, or making it easier for regular people to help seed the torrents.

---

## Pure Silicon Demo Coding: No CPU, No Memory, Just 4k Gates

- 原文链接: [Pure Silicon Demo Coding: No CPU, No Memory, Just 4k Gates](https://www.a1k0n.net/2025/12/19/tiny-tapeout-demo.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46337438)

This article is about creating graphics and music demos on a silicon chip, with no CPU and no memory, using only about 4000 logic gates. The author joined the Tiny Tapeout 8 competition and built two demos: a C64/Amiga-style intro and a Nyan Cat.

The challenge is strong: there’s no ROM, no RAM, and every bit of state uses up logic gates, which are precious. The chip outputs 2-bit RGB to VGA and 1-bit audio. All effects, visuals, and music must be generated in real time, one pixel or sound at a time, using only logic gates and flip-flops. The main demo shows a 3D checkerboard, a starfield, and scrolling text, with real-time shadows and music. There is no lookup table for sine waves or graphics—everything is calculated on the fly using math tricks, such as vector rotation instead of sine tables and linear-feedback shift registers for star patterns. Shadows and checkerboard effects are made by clever use of coordinate transforms and bit operations.

Prototyping was done on an FPGA, but the final design is synthesized for a real silicon process (Skywater 130nm). Fitting all the code and data was hard, so the author used algorithmic tricks to reduce state and reuse logic. Music is generated with simple square and triangle waves, using right-shift operators for envelopes, and all patterns are encoded as logic, not stored in memory. Audio is output with a simple sigma-delta DAC instead of PWM. For the Nyan Cat demo, the author extracted the animation frames and music from the original GIF and MIDI files, mapped them to simple color and note tables, and synchronized audio with video.

After much work, the chips were manufactured, but the company running the project almost shut down before shipping. In the end, the chips arrived, and the demos worked as expected on real hardware.

In the comment section, many readers are amazed by what’s possible with so few gates, calling it “real hardware wizardry.” Some ask about the practical use of such demos, while others say this is about learning and pushing boundaries, not usefulness. A few discuss the math tricks, like vector rotation for sines, and share links to classic demoscene intros. Some readers wonder if this approach can be applied to commercial ASICs or embedded devices, but others note the lack of flexibility without a CPU or memory. There’s praise for the chip design tools and open silicon movement, but also comments about how hard and time-consuming the whole process is, especially the step where the chip layout fails after hours of work. Several people are inspired to try Tiny Tapeout themselves, while a few wish for more beginner-friendly guides. Others share stories of their own chip projects and compare this to old-school demo coding on 8-bit computers. Finally, some discuss the limitations (like lack of high color or sound quality) but say the creativity and skill are what really matter.

---

## Log level 'error' should mean that something needs to be fixed

- 原文链接: [Log level 'error' should mean that something needs to be fixed](https://utcc.utoronto.ca/~cks/space/blog/programming/ErrorsShouldRequireFixing)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46301059)

This article talks about how software logs use the word “error.” It says that when a program shows an error log, it should mean that something is really wrong and someone needs to fix it. Many programs today log too many errors, even for small problems that do not actually break anything. This can make it hard for people to know which errors are important.

The writer explains that logs usually have different levels, like info, warning, and error. Each level should mean something clear. Info is just for normal events. Warning is for something strange, but the program keeps running. Error should mean something is broken and must be fixed. But often, errors are used for things that are not real problems, so people start to ignore them.

If people see too many error messages, they stop paying attention. This can be dangerous because real problems might be missed. The article says developers should be careful about which things they log as errors. Only log true errors—things that need someone to look at them. Warnings should be for things that might become a problem later, but are not broken yet. Good logging helps teams find and fix real bugs faster.

In the comments, some people agree and say they have seen too many useless error logs. They talk about times when error logs were always full, so nobody read them. Others say it is hard to decide what counts as an error, since systems are very different. Some developers think warnings are also important and should not be ignored. A few people say that sometimes business people want to see more logs, so developers add extra errors even if they are not needed.

One user points out that sometimes, what is an error for one team is not an error for another. Another person says that if logs are too noisy, people will miss big problems. Some suggest using tools to filter logs, so only real errors are shown. There is also a comment about teaching new developers how to use log levels correctly. In the end, most people want logs to be useful, not just full of messages. They agree that error should mean something is broken and needs fixing.

---

## OpenSCAD is kinda neat

- 原文链接: [OpenSCAD is kinda neat](https://nuxx.net/blog/2025/12/20/openscad-is-kinda-neat/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46337984)

This article talks about using OpenSCAD, a tool where you write code to create 3D models. The author wanted to learn OpenSCAD by making a simple battery holder, which they had first designed in Fusion 360, a more traditional CAD program.

OpenSCAD is different from other CAD tools because you make shapes by writing code, not by dragging things with a mouse. The author found this useful for simple designs, like boxes or holders, because you can change a few numbers—like how many rows or columns you want—and the whole model updates. They shared a short piece of code that builds a box and cuts out holes for batteries, using loops and math to place each hole. The author notes that you only need to adjust variables like `numRows`, `numColumns`, and `batteryType` to make different holders. They also mention using `let()` inside the loop to help with placing the holes, but found this part a bit confusing. Overall, the author thinks OpenSCAD is great for simple, practical parts and saves time compared to using big, expensive CAD software. It might not work well for complex shapes, but for things like spacers or organizers, it is very handy.

In the Hacker News comments, some people say they love OpenSCAD because it is easy to make and share models that can be changed with just a few numbers. Others point out that OpenSCAD is perfect for programmers, but can be hard for people who do not like to code. A few mention that for very complex shapes, OpenSCAD can get tricky or slow. Some users share tips, like using libraries to make code easier, or combining OpenSCAD with other tools. Others prefer visual CAD tools for most things, but still use OpenSCAD for quick jobs. Some say OpenSCAD is great for teaching code and 3D thinking. A few wish OpenSCAD had better error messages or a nicer editor. There are also comments about using OpenSCAD for fun projects, like puzzles or toys. Some users mention that once you learn it, you can make things faster than with mouse-based tools. A few warn that reading other people’s OpenSCAD code can be tough if it isn’t well-commented. Overall, many agree that OpenSCAD is a neat and useful tool, especially for simple designs and for people who like to code.

---

## Big GPUs don't need big PCs

- 原文链接: [Big GPUs don't need big PCs](https://www.jeffgeerling.com/blog/2025/big-gpus-dont-need-big-pcs)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46338016)

This article looks at using powerful GPUs with small computers like the Raspberry Pi, instead of the usual big desktop PCs. The author tests if the Pi can work well with high-end graphics cards for things like AI, media transcoding, and GPU tasks.

The Raspberry Pi only has one PCIe Gen 3 lane, much less bandwidth than a normal desktop. Still, the author finds that in many cases, the Pi can almost match a modern PC’s performance if the work is mostly on the GPU. For example, in AI tasks with large language models, a Pi setup with multiple Nvidia GPUs was only 2% slower than a big Intel server. Media transcoding works fine too, unless you need heavy disk speed. In 3D rendering tests, the Pi was almost as fast as the PC, especially with older GPUs that match the Pi’s PCIe speed. The Pi also uses much less power: only 4-5 watts when idle, compared to 30 watts for the PC. The whole Pi setup costs around $400, while the PC setup is closer to $2000, not counting the GPU. In AI benchmarks, the Pi was often more efficient, doing nearly the same work but using less electricity. The article also tries using two GPUs at once, but this only helps if the GPUs are the same model and can share memory. In most cases, it’s better to have one big GPU than two smaller ones. The author notes some trouble with certain AMD cards and drivers, and that gaming on the Pi is still difficult.

People in the comment section have many opinions. Some are amazed the Pi can handle such big tasks, saying it’s a great way to save money and power. Others point out that the Pi’s low bandwidth is still a problem for some jobs, like heavy video work or games. A few commenters say these setups are mostly useful for hobbyists and tech experiments, not for businesses or high-demand users. Some worry about the hassle of setting up drivers and hardware on the Pi. Others are excited about using small ARM computers with GPUs for AI at home, and hope for more support in the future. A few people share tips about using different PCIe switches or software to help the Pi work better with big GPUs. There’s debate about efficiency numbers, with some saying the tests may not cover all real-world cases. Some remind everyone that new ARM boards with better PCIe might make this even easier soon. Many see it as a fun project, not a replacement for a real server, but a good way to learn and tinker.

---

## I spent a week without IPv4 (2023)

- 原文链接: [I spent a week without IPv4 (2023)](https://www.apalrd.net/posts/2023/network_ipv6/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46338365)

A network engineer decided to use only IPv6 for a week to see how well it works and to test different ways to move from IPv4 to IPv6. They blocked all IPv4 traffic on their devices and set up special tools like NAT64 and 464XLAT to help connect to websites that only support IPv4.

The article explains that IPv6 is not just a bigger version of IPv4. The way you design a network with IPv6 is different and simpler, because you don’t need tricks like NAT (network address translation). IPv6 addresses are very long, so there are enough for every device. Devices can pick their own addresses, and you can have many addresses on one device for different uses. This can help home labs by making it easier to host servers, play games, and use VPNs without problems from NAT.

The author talks about several ways to move from IPv4 to IPv6. Dual stack means running both IPv4 and IPv6, but this is hard to manage in big networks. NAT64 lets IPv6-only devices talk to IPv4 websites by translating addresses, and DNS64 helps by making fake IPv6 addresses for old sites. 464XLAT is a method used by some ISPs and Apple devices, where both the network and the device help with translation.

After a week, the author found IPv6 works well, but only about half of the websites they use support it. They say more website owners need to turn on IPv6. They also found Apple devices work best with IPv6, while other operating systems are less reliable. The author thinks networks should be designed for IPv6 first, and old tricks like NAT are now only needed for rare cases.

In the comments, some people agree that IPv6 is the future and say their home or company networks already use it. Others say they tried IPv6 but stopped because many websites and services still do not support it or have bugs. Some worry that IPv6 is too complex or hard to manage, and that NAT, even if not perfect, gives some safety or control. A few users say ISPs don’t give them IPv6, so they have no choice but to use IPv4. One person says that tools like NAT64 are clever but can break some apps, especially old games or peer-to-peer tools. Others point out that Apple makes IPv6 easy, but Windows and Linux can have issues. Some are hopeful that more people will switch as IPv4 addresses run out, but others think it will take many more years. A few suggest that clear guides and better router support would help more people move to IPv6. Overall, the comments show both excitement and frustration about using IPv6 today.

---

## Go ahead, self-host Postgres

- 原文链接: [Go ahead, self-host Postgres](https://pierce.dev/notes/go-ahead-self-host-postgres#user-content-fn-1)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46336947)

The article says you do not have to be scared to run your own Postgres database instead of using a cloud provider. The writer explains that big cloud companies make self-hosting sound risky, but most of their services use almost the same open-source Postgres anyway.

Self-hosting, for the writer, has been much easier and cheaper than he expected. After two years, serving millions of queries, he only had one small problem during a migration. Most of the time, Postgres runs smoothly, and he saves a lot of money compared to managed services like AWS RDS. The article points out that, years ago, everyone self-hosted because there was no other choice. Cloud databases became popular after Amazon RDS started in 2009, making it easy to get backups, updates, and other help. But now, cloud prices are much higher, and the main thing they offer is management, not better technology.

The process to move from RDS to self-hosted was simple. The writer saw no drop in performance and sometimes saw improvements because he could tune settings himself. Daily tasks for self-hosting are not hard: check backups, look at slow queries, and update software once in a while. The main risk is that, if your server goes down, you must fix it. But managed services also have outages, and you often still have to solve the problem yourself.

Self-hosting may not be best for everyone. If you are just starting out, or if you are a huge company, managed services might be easier. Also, some jobs need special certifications that managed platforms provide. For self-hosters, the most important things are setting memory, managing connections (using something like PgBouncer), picking a good disk, and tuning write-ahead logs for safety and speed. The writer says most people can self-host with little trouble and big savings, especially for Postgres.

Hacker News commenters have many views. Some agree, saying they have also self-hosted databases for years with few problems. They think cloud databases are too expensive for what you get. Others warn about risks: missed backups, not testing disaster recovery, or forgetting updates. A few mention that cloud providers help with things like scaling and compliance, which can be hard alone. Some share stories of cloud outages where managed services did not help and they still had to fix things themselves.

A few users worry new teams might not have the skills to manage a database safely, leading to data loss. Others point out that with good automation, self-hosting can be easy, but you have to set it up first. Some say managed services are worth it for peace of mind or for teams with little time. There are also comments about legal rules—sometimes you must use a managed service to follow the law. Still, many agree with the article: for most projects, self-hosting Postgres is not as scary as it sounds.

---

## Gemini 3 Pro vs. 2.5 Pro in Pokemon Crystal

- 原文链接: [Gemini 3 Pro vs. 2.5 Pro in Pokemon Crystal](https://blog.jcz.dev/gemini-3-pro-vs-25-pro-in-pokemon-crystal)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46287848)

This article compares how two AI models, Gemini 3 Pro and Gemini 2.5 Pro, played the game Pokémon Crystal using the same game setup and rules. Both models were tested to see which could complete the game better, using special tools that help them remember places, plan steps, and even write small pieces of code.

Gemini 3 Pro finished the game and became the Johto Champion without losing a single battle, while Gemini 2.5 Pro got stuck early on and had trouble moving forward, especially in an area called Olivine Lighthouse. Both models had access to things like a Mental Map to track explored areas, a notepad for plans, and tools to help with puzzles. The system told the models to act like scientists: make guesses, test them, and not trust their own memories unless they saw proof in the game.

In the early parts, both played similarly, but Gemini 3 Pro was much more efficient, using fewer moves and less computer power. Gemini 2.5 Pro lost to a tough gym leader and then spent a lot of time just training, while Gemini 3 Pro quickly used the opportunity to move ahead. The biggest difference came in Olivine Lighthouse. Gemini 3 Pro was careful but eventually figured out the puzzle, while 2.5 Pro kept searching the same floors again and again, missing the key step and wasting thousands of turns.

Gemini 3 Pro showed several strengths: it made accurate mental maps, used markers to remember where important characters were, found clever ways to work around limitations, and planned several moves ahead. For example, in one puzzle, it realized that pushing a rock twice was needed to make progress, while 2.5 Pro would often get stuck. Gemini 3 Pro also began using vision (looking at the game screen) to solve problems, which is a big step forward.

Still, Gemini 3 Pro made mistakes. It sometimes guessed things and did not check if the guess was true, which wasted time. It also usually focused on one goal at a time, missing chances to do more at once, and sometimes wrote buggy code for its tools. In the final fight, Gemini 3 Pro used a smart plan called "Operation Zombie Phoenix" to beat the hardest boss, even with a weak team, by using healing items and clever moves.

When looking at the numbers, Gemini 3 Pro finished the game much faster and with far fewer steps than 2.5 Pro. The article suggests that Gemini 3 Pro is not perfect but is much better at using tools, making plans, and fixing mistakes.

In the comments, many people were impressed by how far AI has come, with some saying it’s exciting to see AI solve real games instead of test problems. Some users pointed out that the way the models are told to learn—by making and testing guesses—is smart and could be used in other areas. Others wondered if the AI’s good results are due more to better coding or bigger training data, not just the model itself.

A few commenters noted that getting stuck on puzzles is a sign that the models still lack common sense, and that even small mistakes or wrong guesses can waste a lot of time. Some suggested that future tests should use only vision (no hidden game data) to make things fairer, while others were excited to see tests on harder or different games next.

There was also debate about whether these AI models are really learning or just following patterns from their training. Some people were hopeful that this kind of agent could help with more than games, like coding or robotics, while a few remained skeptical, saying that real-world tasks are still much harder than Pokémon. Overall, the discussion was hopeful but realistic about the gaps still left in current AI.

---

## Biscuit is a specialized PostgreSQL index for fast pattern matching LIKE queries

- 原文链接: [Biscuit is a specialized PostgreSQL index for fast pattern matching LIKE queries](https://github.com/CrystallineCore/Biscuit)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46289884)

Biscuit is a new PostgreSQL index designed to make LIKE and ILIKE queries much faster, especially when you use wildcards like `%` and `_`. It works better than the usual pg_trgm index by removing the need to check results again, and it supports searching across more than one column at the same time.

The main idea behind Biscuit is to use bitmap indexes. It creates maps that show which records have certain characters at certain positions in the string, both from the start and the end, and also in a case-insensitive way. It also tracks string lengths, so it can filter results quickly based on how long they are. When you do a pattern search, Biscuit breaks the pattern into parts, looks up each part in its bitmaps, and uses fast set operations to find matching records. There’s no need to scan the whole table or do slow disk lookups—everything is in memory and uses fast bitmap math.

Biscuit has many speed tricks, like skipping unnecessary checks, ending searches early if nothing matches, and batching work to save time. It can handle multi-column indexes, and it even reorders your query conditions to check the most selective ones first, making searches even faster. Biscuit also uses Roaring Bitmaps for memory efficiency and speed. Benchmarks show that Biscuit builds indexes much faster than pg_trgm, and queries with wildcards run much quicker too.

You can use Biscuit for things like e-commerce searches, log analysis, customer support, code search, and analytics. It is simple to install, and you don’t have to tweak many settings—most optimizations happen automatically. However, Biscuit does not support regular expressions or locale-based string matching, and it does not provide naturally ordered scans (though PostgreSQL sorts small result sets quickly after filtering). It uses more memory for bitmaps, so you may need to rebuild the index if it grows too large. Write performance is about the same as other indexes but may be slower on heavy update workloads.

In the Hacker News comments, many developers are excited about Biscuit’s performance, especially for apps with lots of wildcard searches. Some are impressed by the detailed explanation and clear benchmarks. People who use pg_trgm mention the pain of slow LIKE queries and false positives, and they like that Biscuit removes these problems. Others point out that Biscuit does not support regex or fuzzy searches, so it won’t replace pg_trgm for every use case. Some users worry about memory usage with very large tables, since Biscuit stores a lot in RAM. A few discuss the lack of order support, but agree that PostgreSQL’s planner makes this less of a problem for small results. There are also questions about how Biscuit handles languages with complex characters or different collations. Some suggest that for simple, high-speed LIKE searches, Biscuit looks “almost too good to be true,” but they want to test it on real data. Others are happy to see more innovation in the PostgreSQL ecosystem and hope Biscuit inspires more index types in the future.

---

