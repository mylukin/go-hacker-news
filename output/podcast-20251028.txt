Hello everyone, this is the 2025-10-28 episode of Hacker News Daily Podcast. Today, we have a full lineup of stories covering software freedom, open data, random numbers, AI, and even cheese crystals.

First, let’s talk about a big change coming to Android. Google says sideloading—installing apps from outside the Play Store—is still allowed. But new rules mean all app developers must now register, pay fees, show ID, and get Google’s approval before their apps can be installed on almost every Android device. The article’s writer says this will make sideloading much harder, and could block users from installing any app they want, even their own or those from friends. Governments and businesses could also lose the ability to use apps that are legal in their country but blocked by Google.

Google claims these rules are for safety, saying apps from outside the Play Store are much more likely to have malware. But the article points out that the Play Store also has malware, and that the real reason may be to control app sales and keep everything inside Google’s system. The writer warns this is bad for competition and software freedom, and is just the latest move after Google made browsers less open and closed parts of Android. The article calls on users and developers to fight back, support open app stores like F-Droid, and contact lawmakers.

In the comments, many people worry that Android is becoming too much like Apple’s iPhone, where only approved apps are allowed. Some agree that sideloading is risky, but most feel these rules go too far and hurt small developers, open-source projects, and user privacy. A few say users will try to go around the rules with rooted phones or custom Android versions, but this is hard for most people. Many hope governments or courts will stop Google, but others say it’s up to users to fight for their rights.

Next, we have an article on why radio towers have blinking lights and why they use different colors. These lights help pilots see towers and avoid them. Towers may use bright white strobes in the day and red lights at night, based on FAA rules, tower height, and location. Sometimes both are used—to keep people nearby from being bothered by bright flashes at night. Most tower lights are now LEDs, which use less power than old bulbs. Towers under 200 feet often don’t need lights unless they’re near airports or helipads. Even cranes and power lines need lights during construction if they are tall enough.

Tower owners must check lights daily and report outages so pilots know if a light is out. In the comments, people share that red lights are less annoying at night, while white strobes can bother people living nearby. Some pilots explain how much they rely on these lights, especially in bad weather. There’s talk about photo sensors that sometimes fail, and why drones don’t need warning lights—because drones are supposed to avoid obstacles. Some wish towers wouldn’t light up the night sky so much, but most agree safety comes first. A few people are now more curious about tower lights and will pay more attention in the future.

Moving on, we look at a test of AI image editing models. The article compares how well top models can edit images using only text instructions, with no manual help. Simple edits, like giving a bald man hair, work well for most models. But harder tasks, like swapping colored blocks or changing small details, often fail. When asked to do many changes at once, like editing a movie poster, most models could do it, but repeated edits made the images look worse, a bit like making a copy of a copy too many times.

Some models, like Seedream 4, did better at tricky things, such as shortening a giraffe’s neck or straightening the Leaning Tower of Pisa. Other tasks, like changing card suits or removing only brown candies, were often too hard. In the comments, users are impressed by progress, but notice that real understanding and fine control are still missing. Some think single-prompt editing is both a good challenge and more useful for real life. There are worries about damage to image quality and hopes for more tests on real photos. Overall, people see this as a good way to show both the strengths and limits of today’s AI image editors.

Now let’s talk about Apple’s new Macs. Some people say that recent Macs—using Apple’s own silicon chips like the M1, M2, and now M5—are boring because they only get small, regular upgrades. But the article argues this is actually good. Before Apple made its own chips, Macs would wait years for updates, had problems with heat, noisy fans, or bad keyboards. Now, Macs are fast, cool, and have great battery life, and upgrades come every year.

In the comments, many agree that steady, reliable updates are what Mac fans wanted. Some wish for bigger changes, but most prefer boring progress over the old days of slow or unreliable upgrades. A few worry that less competition could make Apple lazy, but many say predictable upgrades are good for businesses and developers. The main feeling is that boring is better than waiting years for fixes.

Next, we have news about EuroLLM, a new large language model built in Europe to support all 24 official EU languages. EuroLLM has 9 billion parameters, was trained on over 4 trillion words, and is open source, so anyone in Europe can use it for free. The team plans to add support for images and speech soon. The project was built by European universities and companies and is already available on Hugging Face.

In the comments, users are happy to see an open-source model focused on European languages. Some like the idea of digital independence and trust, while others wonder if the model can really support smaller languages well. There is interest in fine-tuning the model for local projects, and questions about training data and bias. Many think this is a great step for European AI, and hope it helps local startups and governments.

Now, let’s turn to a new open dataset from EvE Bio. This group is mapping how every FDA-approved drug interacts with many important receptors in the human body, sharing the data so everyone can better understand off-target effects—what else a drug does besides its main job. Drug companies usually don’t test for this because it’s not profitable, but EvE Bio is testing about 1,400 drugs and sharing the results for free.

The data can help repurpose drugs for new uses, train machine learning models, and design safer or better drugs. The article explains that this kind of open data is rare, and even though lab tests can’t show everything that happens in real people, it can still help a lot. In the comments, people are excited about the open data, but some worry about funding and how to keep the dataset up to date. There’s talk about how this could help small companies and academic labs, and stories of off-label drug uses that worked well. Most agree that open science like this is valuable, even if it won’t fix every problem.

We also have a story about using large language models to control a robot in an office, with the simple job: “pass the butter.” The Butter-Bench test broke the job into small steps. A simple robot with a camera and lidar was used, and the LLM made high-level decisions. Humans could do the task 95% of the time, but the best AI robot only managed 40%. The AI often got confused, spun in circles, or wrote strange messages. Some models even gave away private info when tricked.

The article points out that LLMs are good at text, but not at moving or understanding space. In the comments, people joke about the robot’s “existential crisis,” but also say it shows real limits of current AI. Some are hopeful that things will improve, while others say robots need more than just language skills to be useful.

Next, there’s a story about someone who used AI to lower a huge hospital bill from $195,000 to $33,000 after a family member died without insurance. Using an AI tool, they found many billing errors and overcharges, wrote a strong letter, and forced the hospital to negotiate. In the end, the bill dropped by more than $160,000. The main lesson is to use knowledge and tools to fight unfair bills.

In the comments, people from other countries are shocked by the US health system. Some ask why the family paid at all, since the patient had died. Many share stories of hospitals dropping bills after being pushed, and most agree the system is broken and unfair. There’s talk about how hospitals inflate bills to later give discounts, and how this is not normal in most countries.

Switching gears, we have a quick story about cheese crystals. Many people think white spots on cheese mean mold, but they are usually harmless crystals that show the cheese is well aged. There are two main types: calcium lactate, which forms on the outside, and tyrosine, which forms inside hard cheeses like parmesan. These crystals add a crunchy texture and are a sign of quality.

In the comments, some say they used to throw away cheese with white spots but now know better. Others share more science about cheese, talk about making cheese at home, and joke about the “cheese police” in their families. Most agree that learning about food science helps waste less food.

Finally, let’s look at the history of random numbers and the Rust crate “oorandom.” Before computers, people used things like roulette wheels or printed books to get random numbers. Early computers used simple methods like the Linear Congruential Generator (LCG), which was fast but not very random. Later, better methods like the Mersenne Twister, xorshift, and PCG were invented. Oorandom uses PCG because it is simple, fast, and good enough for most uses, but not for cryptography.

In the comments, people like oorandom’s small size and stable API. Some warn that it’s not safe for security work. There’s talk about the trade-offs between speed, size, and quality, and stories about using bad random number generators in the past. Many people appreciate a tool that is simple and honest about what it can do.

That’s all for today’s episode. We covered app freedom, open data, random numbers, AI, cheese, and more. Thanks for listening to Hacker News Daily Podcast. See you next time!