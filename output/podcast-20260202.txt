Hello everyone, this is the 2026-02-02 episode of Hacker News Daily Podcast. Today, we have a busy show with stories about new AI products, open source changes, job threads, security bugs, and more.

First up, OpenAI has launched the Codex App, a new tool aimed at helping both developers and beginners write code faster and easier. Codex lets you type what you want in plain English, and it writes the code for you. For example, you can just say, “Make a button that says Click Me,” and Codex creates the right code. It works with popular languages like Python and JavaScript, and can also help debug code, explain what code does, and turn your ideas into working programs. Codex is built on GPT-3, but it is trained more on code and technical data, so it understands software tasks better than normal AI chatbots. The app runs in your web browser, so you do not need to install anything. Codex can work with APIs, and it helps you make simple websites and scripts very quickly. OpenAI says they want to help both professional developers and people learning to code. They are planning to improve Codex with user feedback, and hope to lower the barrier to programming.

In the comments, some people are excited and think Codex will help new developers learn much faster. Others worry that it might lead to more low-quality code, since people may copy code without really understanding it. Some fear that jobs could be lost if AI writes most of the code. Others point out that Codex still makes mistakes—its code is not always correct or safe. There are questions about security, like whether Codex could write code with bugs or vulnerabilities. Some users also wonder about licensing: will Codex copy code from open source projects? People want to see Codex work with more languages and frameworks, and they are curious about pricing and whether it will stay open to everyone. Overall, people are interested but cautious, and want to see how Codex develops over time.

Next, there is a story about the Mattermost chat platform and problems with its open source license. Mattermost’s license uses unclear wording, saying people “may be licensed” to use the code, but it does not explain the exact terms. Many users on GitHub have asked the Mattermost team to use a standard open source license, like AGPL, to make things clear. Right now, the license is confusing, so people are not sure if they can use or change the software. Some users say the license should not depend on what you do with the code, and it should allow all normal open source uses. Others point out that “may be licensed” is not normal for open source projects.

Mattermost staff tried to explain two license options: AGPL for open source users, and a commercial license for businesses. But people still said this was not clear, and that it only allowed making compiled versions, not other types of use. Over time, users kept asking for a fix, saying the license mess is a blocker for adoption and not really open source. Some even said the unclear license is a way for Mattermost to get free help while keeping control. Finally, a Mattermost team member said they understand the complaints, but the company will not change the license or make it clearer.

In the comments, many people were unhappy and said this is not real open source. They said projects with unclear or mixed licenses are hard to trust or use. Some blamed company leadership, saying they want open source benefits but do not want to share fully. A few were sad because they liked Mattermost as an alternative to Slack, but cannot use it because of the license. There was confusion about different parts of the code having different licenses. Many developers reacted with disappointment or anger when Mattermost closed the issue without fixing the license.

Now, let’s look at the monthly job thread on Hacker News, where companies and teams share job openings for software developers and other tech roles. Comments are posted by company staff or team leads, describing open roles, what kind of person they want, and how to apply. Most jobs are in software engineering, but there are also openings in design, product management, or data science. Many jobs are remote, and some are for cities like San Francisco, London, or Berlin. Both big companies and small startups are hiring. Posts often mention the tech stack, like Python, JavaScript, or AWS. Some jobs are full-time, others are internships or part-time. Salary ranges are sometimes listed, and some companies offer visa sponsorship.

In the comment section, people talk about hiring trends. Some ask about the rise of remote jobs, while others discuss how hard it is to get a job without a degree or years of experience. Some complain about job posts with “unicorn” requirements, like expecting five years of experience with a new technology. Others share tips for standing out, like writing a clear cover letter or showing side projects. There are also comments about salary transparency, with some users wanting more companies to list pay ranges. A few people thank those who post jobs, saying these threads helped them find work before. Some also discuss which cities have the most jobs or which languages are most in demand.

Alongside that, there is a regular thread for job seekers to share information about themselves. People post their skills, experience, and what kind of work they want. Many list programming languages they know, such as Python, JavaScript, or Go, and some mention special skills like machine learning or web development. Some want remote work, others are open to moving. A few prefer startups, while others want big companies. Many share links to their resumes or GitHub pages, and people from around the world post, so companies can find talent everywhere. In the comments, there is advice for writing a good message, and some talk about the job market. Employers also join in, sometimes asking for more details or offering tips. The thread is friendly and supportive.

Next, there is news about a major security problem in Moltbook, a new social network where only AI agents are supposed to post and talk. Researchers found that Moltbook’s database was left open, so anyone could read and change almost everything on the site. The database key was inside the website’s code, making it easy to find. With this key, people could get all the data—1.5 million agent API tokens, 35,000 email addresses, and even private messages. The Moltbook team fixed the problem quickly after being told.

Moltbook was built using AI tools, and the founder did not write code directly. This made the process fast, but important security steps were missed, especially Row Level Security in Supabase. Without this, the public API key let anyone read, write, or change all the data. Inside the database, most “AI agents” were actually made by humans, and there were only about 17,000 real users. Anyone could pretend to be an AI just by using a simple web request. Private messages, including some with secret API keys for outside services, were open to anyone. The article gives five lessons: going fast with AI tools makes security mistakes easy; fake agents are a problem; privacy leaks can hurt many people; letting anyone change posts is very risky; and fixing security is a slow process.

In the comments, people were surprised that such a popular AI site had basic security mistakes. Many said “vibe coding” is fun for building fast, but it is risky if you skip safety steps. Some pointed out that Supabase makes it easy to start, but you must set up security yourself. Others shared stories of seeing similar mistakes in AI projects. Some argued that the real problem is new builders not understanding security basics, not the AI tools themselves. Some praised the Moltbook team for fixing things quickly. Others wondered if letting humans pretend to be AIs is honest, or if it breaks the idea of an AI social network. People agreed that new AI tools are exciting, but security still needs human care.

Moving on, there is a thought-provoking article asking: what is the biggest number you can represent with 64 bits? Most people say it is a little over 18 quintillion, which is the highest value for a 64-bit unsigned integer. But if you use 64-bit floating point numbers, you can represent numbers up to around 1.8 times 10 to the 308th power. The author goes further: what if you use 64 bits to write a program, like “9^9^9^99,” which stands for a number too big to compute? Some languages let you describe huge numbers with very short code, especially if they have special math functions.

The post focuses on how simple languages, like Turing machines or lambda calculus, can “describe” very large numbers with short programs. The famous Busy Beaver function for Turing machines gives numbers too big to imagine, and with lambda calculus, you can get even bigger numbers in fewer bits. In fact, the largest number representable in 64 bits is not a normal integer, but the output of a special 61-bit lambda calculus program called w218, which is bigger than most famous big numbers in math.

In the comments, people are surprised by the idea of representing numbers as programs instead of just data. Some think it is “cheating,” since a program is more like a recipe than the number itself. Others compare this to Kolmogorov complexity, where the “size” of data is the smallest program that outputs it. Some ask if it’s fair to let programs call very powerful built-in functions, since then you can get even bigger numbers. There is debate about whether Turing machines or lambda calculus is better for this challenge. Some joke that these numbers are so big, it hardly matters which is bigger. Others ask if these ideas matter in real life, or if it’s just a fun thought experiment. Many are impressed by code-golf solutions that pack so much into so few bits. A few share their own tricks for writing compact programs, and some say this post made them realize just how strange and interesting “big numbers” can be.

Now, let’s talk about Google DeepMind’s Game Arena platform. DeepMind is updating Game Arena to test AI in new ways, adding the games Werewolf and poker alongside chess. The goal is to see how well AI can deal with real-world problems, like talking to people or handling risk, not just planning like in chess. Chess was first, because it is about perfect information and deep planning, and the latest Gemini 3 Pro and Flash models now lead the chess leaderboard. DeepMind is now adding Werewolf, a team game where players talk and try to spot liars, so AI must use language, spot lies, and work with others. This helps test how AI handles tricky social situations. The Gemini models are doing well here, too, showing they can reason about what others say and do. Poker is next: the AI must guess, take risks, and adapt, since not all cards are known. The best models will be shown in a live tournament.

Game Arena also has livestreamed events with famous human players. Each game—chess, Werewolf, and poker—tests something different, and together they show how smart, safe, and flexible new AI models are. In the comments, people are excited about moving past chess to games like poker and Werewolf, since these are closer to real life. Some think testing “soft skills” is key if AI is to work with humans. Others worry that these are still just games, and AI winning at games does not mean it is safe in the real world. Some users want more open data from matches, so anyone can study how the best models think and act. A few also wonder if AI agents playing Werewolf might learn bad habits, like lying too much. Several people praise DeepMind for making these tests public, and many look forward to seeing how AI from smaller teams will do. The community sees this as an important step, but wants to watch closely how AI “learns” from these new, more human-like challenges.

Next, there is an article explaining how a small but powerful inference engine, called Nano-vLLM, runs large language models. Nano-vLLM turns your prompt into tokens and manages how those tokens are processed to give fast and efficient outputs. It starts with a generate method, turning your prompt into tokens using a tokenizer. These tokens become sequences, which go into a waiting queue. A scheduler then batches prompts together, which helps use the GPU more efficiently, though it can mean some prompts wait longer.

There are two main steps: prefill (processing the prompt) and decode (generating new tokens). The scheduler tracks which prompts are waiting and running, moving them as needed. If the GPU’s memory gets full, the scheduler can pause prompts and resume others. The Block Manager divides each prompt into fixed-size blocks to make memory use more efficient. It reuses work when prompts have the same beginning to save time and memory. The Model Runner prepares input, moves data to the GPU, and uses CUDA graphs for speed. When picking the next word, a sampling step controlled by temperature decides how random or focused the output should be.

In the comments, people are impressed that so much is done in just 1,200 lines of Python code. They liked the clear explanation of batching and scheduling. Some ask if these tricks could work with other models or hardware. Some warn that batching can hurt the user experience if there are delays. Others discuss how prefix caching works, the trade-offs between speed and memory, and how this compares with larger engines like vLLM. Many are excited to see such a practical guide to how inference engines work under the hood.

Moving on, there’s a post comparing two popular tools for copying files over a network: rsync and rclone. The author wanted to copy large video project files quickly from a network drive to an external SSD. At first, they used rsync, which is single-threaded and only copies one file at a time. It took over eight minutes to copy 59 GB, using about 350 MB per second. With rclone and its multi-threaded option, the same data was copied in just over two minutes, using almost the full network speed of around 1 GB per second. The only tricky part was handling special files and matching rsync’s options. For checking which files changed, both tools took about the same time, but for copying, rclone was four times faster.

The main reason for rclone’s speed is that it can use many threads to copy files at once. In the comments, some users were surprised and thanked the author for sharing the tip. Others pointed out that rsync is old and was not designed for parallel transfers. A few mentioned that there are newer forks of rsync with multi-threading, but these are not yet standard. Some said rclone might not handle all features or permissions the same way, so users should check if it fits their needs. Others shared their own benchmarks and agreed that rclone is faster for big file transfers. Several thanked the author for clear examples and real-world tests.

Next, the open source flashcard app Anki is being handed over from its original creator, Damien Elmes, to a for-profit group called AnkiHub. The AnkiHub team says they are excited but nervous about this responsibility, and they promise to keep Anki’s spirit and values alive. AnkiHub is a small company started by students who love Anki, and they want to keep it open source. They say they will not follow bad business practices or try to make money at the cost of users. Instead, they want to make Anki more user-friendly and improve its design. They also want to make it easier for non-technical people to use add-ons, and make sure the project does not depend on just one person. They promise to be open about decisions and to involve the community.

The team says there is no financial trouble, and Anki and AnkiHub are already profitable, with no outside investors. The app will stay affordable, the main code will stay open source, and volunteer developers will still be important. Mobile apps will keep being updated, and AnkiDroid will remain separate and open source. The team hopes to improve onboarding for new users and fix old problems.

In the comments, many people are worried about this change. Some fear that turning Anki into a for-profit company might lead to “enshittification,” where the product gets worse for users. Others point to similar changes harming open source projects in the past. Several commenters say they are waiting to see if AnkiHub keeps its promises about openness and fair pricing. Some think it might be good, since more resources could mean faster updates and better features. Others hope add-ons and the open source code will not be restricted. There is praise for Damien’s work and hope that the new team will respect the community. A few people say they will back up their cards and watch carefully. Overall, the community is uncertain but paying close attention.

That’s all for today’s episode. We covered AI tools, open source changes, job threads, security issues, big number math, fast file transfers, and more. Thank you for listening to Hacker News Daily Podcast. See you next time!