# Hacker News 故事摘要 - 2026-02-02

## 今日概述

Today’s top Hacker News stories are about new AI products, open source license problems, job threads, and a big security bug. Many stories focus on how AI is changing coding and jobs, and how fast tools can cause mistakes. There are also news about moving popular apps to new owners, and about tools for faster file transfers. People are talking about trust, security, and how tech is changing fast.

---

## The Codex App

- 原文链接: [The Codex App](https://openai.com/index/introducing-the-codex-app/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46859054)

The article is about the launch of the Codex App by OpenAI, a new tool for developers. Codex is an AI system that can write code from natural language instructions.

The main idea is that Codex helps people code faster and easier. With this app, you can type what you want in plain English, and Codex writes the code for you. For example, you can say, “Make a button that says Click Me,” and Codex will create the code for that button. The goal is to make programming more simple, even for beginners. Codex supports popular languages like Python, JavaScript, and others. The app can also help debug code, explain what code does, or turn ideas into working programs. Codex is built on top of GPT-3, but it is trained more on code and technical data. This means it understands software tasks better than normal AI chatbots. The app can be used in web browsers, so you don’t need to install anything. OpenAI says they want to help both professional developers and people learning to code. Codex also works with APIs and can help make simple websites and scripts very quickly. OpenAI is planning to improve Codex with feedback from users. They hope this tool will lower the barrier to coding.

In the comments, some people are excited about Codex and think it will help new developers learn much faster. Others are worried that Codex could lead to more low-quality code, as people might copy code without understanding it. Some fear that developers could lose jobs if AI can write most of the code. A few users point out that Codex still makes mistakes, and its code is not always correct or safe. There are comments about how Codex could help automate boring tasks, letting programmers focus on harder problems. Some think it will change how people think about software, making coding more about ideas than syntax. Others are curious about security risks, like Codex writing code with bugs or vulnerabilities. A few developers wonder how Codex will handle licensing—will it copy code from open source projects? Some users want to see Codex work with more languages and frameworks. There are questions about how Codex will be priced, and if it will stay open to everyone. A few comments mention that tools like this might be the future of programming, but there is still a long way to go. Overall, people are interested but cautious, and they want to see how Codex develops over time.

---

## Mattermost say they will not clarify what license the project is under

- 原文链接: [Mattermost say they will not clarify what license the project is under](https://github.com/mattermost/mattermost/issues/8886)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46861331)

Mattermost, a chat platform, has unclear wording in its license, saying people "may be licensed" to use its code, but does not explain the exact terms. Many users on GitHub asked Mattermost to use a standard open source license, like AGPL, and make it clear to everyone how the code can be used. The current license text is confusing and leaves people unsure if they are really allowed to use or change the software. Some users said that the license should not depend on what you do with the code, like if you only compile it, but should allow all normal open source use. Others pointed out that using "may be licensed" suggests that maybe you are not allowed, which is not normal for open source.

Mattermost staff suggested new wording to explain the two license options: AGPL for open source users and a commercial license for businesses who do not want AGPL rules. However, people said this was still not good, because it only allowed making compiled versions and did not clearly license the source for other uses. Over time, users kept asking for a fix, saying the unclear license was a blocker for adoption and was not really open source. Some said the license mess is not a mistake, but a way for Mattermost to get free help while keeping control. After many years, a Mattermost team member finally said they understand the complaints, but the company will not change the license or make it clearer.

In the comments, some users were very unhappy and said this is not real open source. They pointed out that a project with unclear or mixed licenses is hard to trust or use. Others said it would be better to just use AGPL for all the code, and not mix licenses. A few people blamed company leadership, saying they want the benefits of open source without giving up control. One commenter said this is a common trick for companies who want free help but do not want to share fully. Some were sad because they liked Mattermost as an alternative to Slack, but could not use it because of the license. There was also confusion because different parts of the code seemed to have different licenses, making things even harder to understand. In the end, many developers reacted with disappointment or anger that Mattermost closed the issue without fixing the license.

---

## Ask HN: Who is hiring? (February 2026)

- 原文链接: [Ask HN: Who is hiring? (February 2026)](item?id=46857488)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46857488)

This post is a monthly thread on Hacker News where companies and teams share job openings for software developers and other tech roles. People use it to find new jobs or to see which companies are hiring.

Each comment is from a company or a team lead. They write about their open roles, what kind of person they want, and how to apply. The jobs are often in software engineering, but some posts are for design, product management, or data science. Many jobs are remote, but some are for specific cities like San Francisco, London, or Berlin. Some companies are big and well-known; others are small startups. The posts often mention the team’s tech stack, such as Python, JavaScript, or AWS. Some jobs are for full-time roles, others for internships or part-time work. A few comments explain company values or what makes the team special. Some companies offer visa sponsorship. Salary ranges are sometimes included, but not always. Many posts invite people to email a resume or reach out for a chat.

In the comment section, people often discuss hiring trends. Some users ask if remote jobs are now more common than before. Others talk about how hard it is to get a job without a degree or years of experience. A few people say there are too many “unicorn” requirements, like wanting five years of experience with a new technology. Some users share tips on how to stand out when applying, like writing a clear cover letter or showing side projects. Others warn that some companies never reply, which can feel discouraging. There are comments about salary transparency—some want more companies to list pay ranges. A few people thank those who post jobs, saying these threads helped them find work in the past. Some users also discuss which cities have the most job posts, or which programming languages are most in demand.

---

## Hacking Moltbook

- 原文链接: [Hacking Moltbook](https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46857615)

This article is about a big security problem found in Moltbook, a new social network where only AI agents are supposed to post and talk. The researchers found that Moltbook’s database was left open, letting anyone read and change almost everything on the site.

Moltbook became popular in the AI world because it looked like a busy place for AI agents to share ideas, vote, and build their reputation. But when experts checked the site, they found a serious mistake: an important database key was inside the website’s code, so anyone could find it. Using this key, people could get all the data, including 1.5 million agent API tokens, 35,000 email addresses, and even private messages. The researchers told the Moltbook team, who fixed the problem quickly.

The article explains that Moltbook was “vibe-coded”—the founder used AI to build the site without writing code himself. This made the process fast, but important security steps were missed. The biggest problem was the missing Row Level Security (RLS) setting in Supabase, the database hosting service. Without RLS, the public API key let anyone do anything: read, write, or change data.

Inside the database, the researchers saw that most “AI agents” were actually made by humans. There were only about 17,000 real users, but 1.5 million agents, so one person could easily make hundreds of fake AIs. Anyone could pretend to be an AI just by using a simple web request.

The data leak was even worse because private messages between agents were also open to anyone. Some messages included secret API keys for third-party services like OpenAI. Not only could someone read posts and messages, they could also change any post, add bad content, or even break the site.

The article gives five lessons: going fast with AI tools makes security mistakes easy; fake agents are a problem if there’s no way to check if an account is real; privacy leaks can hurt lots of people; letting anyone change posts is more dangerous than just reading data; and fixing security is a slow, step-by-step process.

In the comments, some people were surprised that such a popular AI site had basic security mistakes. Many said “vibe coding” is fun for building fast, but it’s risky if you skip safety steps. Others pointed out that Supabase makes it easy to start, but you must set up security yourself. Some developers shared stories of seeing similar mistakes in other AI projects.

A few commenters argued that the real problem is not AI, but that many new builders don’t understand security basics. Some thought the Moltbook team acted well by fixing things quickly after learning about the issue. Others wondered if letting humans pretend to be AIs on the platform is honest or if it breaks the idea of an “AI social network.” Overall, people agreed that new AI tools are exciting, but security still needs human care and attention.

---

## The largest number representable in 64 bits

- 原文链接: [The largest number representable in 64 bits](https://tromp.github.io/blog/2026/01/28/largest-number-revised)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46859443)

This article asks: what is the biggest number you can represent with 64 bits? Most people say it is 18,446,744,073,709,551,615, which is the highest value for a 64-bit unsigned integer. But, if we use floating point numbers, the biggest 64-bit double can represent is around 1.8 × 10^308.

The author goes further and asks: what if, instead of numbers, we use programs that fit in 64 bits? For example, some programming languages let you write very short code (like “9^9^9^99”) that stands for an extremely huge number, even if you can't actually compute it. Some languages even have special math functions built-in, which can make even bigger numbers with short code.

But the post is mostly about what happens if we use very simple languages, like Turing machines or lambda calculus, and count how big a number you can “describe” in 64 bits. With Turing machines, the famous “Busy Beaver” function BB(n) gives the maximum steps a machine with n states can make before halting. For 6 states (which fits in about 64 bits), BB(6) is known to be bigger than a tower of 2’s 10 layers high, but its true value is unknown. If you use lambda calculus, you can make even bigger numbers in fewer bits—there are clever 49-bit and 61-bit lambda calculus programs that represent numbers much bigger than Graham’s number (a famous large number in math).

The author also compares how efficient these different systems are at “describing” huge numbers with short programs. Lambda calculus does better than Turing machines because it is easier to write compact, powerful code. The post mentions different versions of the Busy Beaver function for both Turing machines and lambda calculus, and introduces “universal” versions that work with any description language.

In summary, the largest number representable in 64 bits is not a normal integer, but the output of a special 61-bit lambda calculus program called w218. This number is so big, it is far beyond most famous big numbers in math.

In the comments, many people are surprised by the idea of representing numbers as programs instead of plain data. Some think it’s “cheating,” because the program is more like a recipe than the number itself. Others point out that this is similar to how Kolmogorov complexity works, where the “size” of data is measured by the smallest program that outputs it.

A few readers ask if it’s fair to let programs call very powerful built-in functions, since then you could make even bigger numbers with very short code. Some suggest that the “largest number you can actually compute in practice” is much smaller than these theoretical answers, since most of these huge programs can never finish running.

There is debate about whether Turing machines or lambda calculus is more “natural” for this kind of challenge. Some like the simplicity of Turing machines, while others prefer the flexibility of lambda calculus. One person jokes that, in the end, all these numbers are so big that it hardly matters which is bigger.

Another user wonders if these ideas have any real-world use, or if it’s just a fun thought experiment. A few comment that this topic is a good way to learn about computability, mathematical logic, and the limits of programming.

Some are impressed by the clever code-golf solutions that pack so much “size” into so few bits. A few share their own tricks for writing compact programs in different languages. And finally, a couple of people say this post made them realize just how strange and interesting “big numbers” can be!

---

## Advancing AI Benchmarking with Game Arena

- 原文链接: [Advancing AI Benchmarking with Game Arena](https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46858873)

Google DeepMind is updating its Game Arena platform to test AI in new ways, adding the games Werewolf and poker alongside chess. The goal is to see how well AI can deal with real-world problems, like talking to people or handling risk, not just planning moves like in chess.

Chess was the first game used, since it is about perfect information and deep planning. AI models play chess to show how well they can think ahead and spot patterns. The new Gemini 3 Pro and Gemini 3 Flash models now lead the chess leaderboard, showing better play than older models. These models do not just calculate all moves, but use ideas like humans—such as piece safety and long-term plans.

Now, DeepMind is adding Werewolf, a team game where players talk and try to spot liars. Here, the AI must use language, spot lies, and work with others, just like in real life. This game helps test how AI handles tricky social situations, not just logic. It also lets researchers see if AI can spot or even do safe forms of trickery, which is useful for stopping bad actions in the real world. Again, Gemini 3 Pro and Flash are on top, showing they can reason about what others say and do, and build trust with teammates.

Poker is the next big test. In poker, not all cards are known, so AI must guess, take risks, and adapt to how other players act. Google is using the popular Heads-Up No-Limit Texas Hold’em rules. The AI models must learn when to bluff, fold, or push for a win, all while not knowing every detail. The best models will be shown in a live tournament, and their skills will be on display for everyone.

Game Arena also features livestreamed events with famous human players, so people can watch how these AIs play and learn from real experts. Each game—chess, Werewolf, and poker—asks something different from the AI, and all together they give a better picture of how smart, safe, and flexible new AI models really are.

In the Hacker News comments, many people are excited about moving past chess to games like poker and Werewolf, since these games are closer to real life. Some think that testing “soft skills” like talking and bluffing is key if AI will work with humans. Others worry that these games are still not the real world—AI may win at games, but that does not mean it will act safely outside of them.

There is some debate over how much these benchmarks really show. A few users point out that AI might just get good at the tricks needed to win these games, without truly “understanding” social rules or risk. Others are interested in how AI handles lying or bluffing, since this could be dangerous if not tested well.

Some users ask for more open data from the matches, so anyone can study how the best models think and act in these games. A few also wonder if AI agents playing Werewolf might learn bad habits, like lying too much, and what that could mean for real-world uses.

Several comments praise Google and DeepMind for making these tests public, and for including live streams with expert reviews. A number of people look forward to seeing how the AI models from smaller teams and open-source groups will do against the big names. Overall, the community sees this as an important step, but wants to watch closely how AI “learns” from these new, more human-like challenges.

---

## Nano-vLLM: How a vLLM-style inference engine works

- 原文链接: [Nano-vLLM: How a vLLM-style inference engine works](https://neutree.ai/blog/nano-vllm-part-1)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46855447)

The article explains how a small but powerful inference engine, called Nano-vLLM, runs large language models. It shows how Nano-vLLM takes your prompt and turns it into tokens, then manages how those tokens are processed to give you fast and efficient outputs.

Nano-vLLM starts with a simple `generate` method. You give it a prompt, and it uses a tokenizer to split your text into tokens. These tokens become sequences, which are the main units the system works with. Instead of handling each prompt by itself, Nano-vLLM uses a producer-consumer system. Prompts are put into a waiting queue, and a scheduler decides when to process them. The scheduler groups prompts into batches, which helps the system use the GPU more efficiently. Batching speeds up processing, but it can also mean some prompts wait longer for a result.

There are two main steps when generating text: prefill (processing the prompt) and decode (generating new tokens one by one). The scheduler tracks which prompts are waiting and which are running, moving them between queues as needed. If the GPU’s memory (KV cache) gets full, the scheduler can pause a prompt and let others continue until there is enough space again.

The Block Manager divides each prompt into fixed-size blocks, making memory use more efficient. It also uses clever hashing to reuse work—if two prompts have the same beginning, the engine can skip repeating the work for that part. This saves both time and memory.

The Model Runner handles running the actual model, sometimes using many GPUs together. It prepares the input, moves data from CPU to GPU, and uses CUDA graphs to make things even faster. When it’s time to pick the next word, the system uses a sampling step, controlled by the temperature setting, to decide how random or focused the output should be.

Hacker News commenters had several thoughts. Some were impressed that so much could be done in just 1,200 lines of Python code. Others liked the clear way the article broke down complex ideas like batching and scheduling. A few people asked if these tricks could be used with other types of models or on smaller hardware. Some warned that batching can hurt user experience if people have to wait too long. Others debated how well prefix caching works when prompts are very different. A few mentioned that understanding all these details helps you pick the right tool or set the right batch size for your needs. Some were interested in the trade-offs between speed and memory use, and others liked seeing how open-source projects can teach you about real production systems. There were also questions about how Nano-vLLM compares with bigger engines like vLLM, especially in edge cases. Overall, people seemed excited to see such a clear and practical guide to how inference engines work under the hood.

---

## 4x faster network file sync with rclone (vs rsync) (2025)

- 原文链接: [4x faster network file sync with rclone (vs rsync) (2025)](https://www.jeffgeerling.com/blog/2025/4x-faster-network-file-sync-rclone-vs-rsync/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46820142)

This article compares two tools for copying files over a network: rsync and rclone. The author wants to copy large video project files quickly from a network drive to a fast external SSD.

Rsync is the tool the author used at first. It only copies one file at a time and cannot use the full speed of a fast network. In his test, rsync copied about 59 GB of data in over 8 minutes, reaching about 350 MB per second. The author tried different settings and compression options, but rsync never used the full speed of his 10 Gbps network. The main problem is that rsync is single-threaded, so it cannot copy many files at once.

Then the author tried rclone, a tool he usually uses for cloud backups. Rclone has an option called --multi-thread-streams, which lets it copy files in parallel. With this setting, rclone copied the same data in just over 2 minutes, using almost all of the available network speed (around 1 GB per second). The only tricky part was handling some special files and matching rsync’s options. For checking which files changed, rclone and rsync took about the same time. But for copying, rclone was about four times faster.

The main reason for rclone's speed is that it can use many threads to copy files at the same time, while rsync can only use one. The author was happy to see his network finally being used at full speed. He suggests rclone as a good option for fast file syncs, even on local networks.

People in the comment section had mixed views. Some were surprised rclone was so much faster and thanked the author for sharing the tip. Others pointed out that rsync is old and was not designed for parallel transfers, so rclone has a clear advantage here. A few users said that rsync has newer forks or patches that add multi-threading, but these are not yet standard. Some mentioned that rclone might not handle all file features or permissions the same way as rsync, so users should check if it fits their needs. Others said that for most people, rsync is still good enough and easy to use. A few asked if rclone could replace rsync for all use cases, while some warned about possible edge cases or bugs with newer tools. Some users shared their own benchmarks and agreed that rclone is faster for big file transfers. Others discussed how hardware, network setup, or drive speed can affect results. Several thanked the author for clear examples and real-world tests.

---

## Ownership of open source flashcard app Anki transferred to for-profit AnkiHub

- 原文链接: [Ownership of open source flashcard app Anki transferred to for-profit AnkiHub](https://forums.ankiweb.net/t/ankis-growing-up/68610)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46861313)

The open source flashcard app Anki is being handed over from its original creator, Damien Elmes, to a for-profit group called AnkiHub. The new team says they are excited but also nervous about this responsibility, and they promise to keep Anki’s spirit and values alive.

They explain that AnkiHub is a small company started by students who love Anki, and they want to keep Anki open source. They say they will not follow bad business practices or try to squeeze money from users. Instead, they want to make Anki more user-friendly, improve its design, and help more people use it—not just medical students. They plan to make it easier for non-technical people to use add-ons and want to make sure the project doesn’t depend on just one person. They also promise to be open about how decisions are made and to involve the community.

The team says there is no financial trouble. Anki and AnkiHub are already profitable, and there are no outside investors. They will keep the app affordable, and the main code will stay open source. Volunteer developers will still be important, and the mobile apps will keep working and being updated. There are no plans to change the way add-ons work, and the AnkiDroid app will remain separate and open source. They hope to improve things like onboarding for new users and fix old problems that have been hard to solve before. They want to grow trust by being honest and listening to users.

In the Hacker News comments, many people are worried about this change. Some fear that turning Anki into a for-profit company might lead to “enshittification”—where the product gets worse for users over time as the company tries to make more money. Others point out that similar changes have harmed open source projects in the past. Several commenters say they are waiting to see if AnkiHub really keeps its promises about openness and fair pricing. A few think it might be good, since more resources could mean faster updates and better features. Some developers hope add-ons and the open source code won’t be locked down or restricted. Others mention that having no outside investors is a good sign, but wonder if that will last. There is praise for Damien’s work and hope that the new team will respect the community’s wishes. A few people say they will back up their cards and watch carefully, just in case. Overall, the community feels uncertain but is paying close attention to see what happens next.

---

## Ask HN: Who wants to be hired? (February 2026)

- 原文链接: [Ask HN: Who wants to be hired? (February 2026)](item?id=46857487)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46857487)

This post is a regular thread where people looking for jobs in tech share a bit about themselves. Job seekers write short messages with their skills, experience, and what kind of work they want. Many people mention programming languages they know, like Python, JavaScript, or Go. Some list special skills, such as machine learning, web development, or mobile app building. Others talk about remote work, saying they want to work from anywhere. Some are open to moving for the right job.

A few people explain that they like startup jobs, while others want bigger companies. Many share links to their resumes, portfolios, or GitHub pages. Some say they prefer full-time jobs, but others are looking for contract or freelance work. People from many countries post, so companies can find talent from all over the world. Some job seekers focus on specific industries, like finance, healthcare, or gaming. Others are generalists and can work in many areas.

The comments under the thread show support for this kind of post. Some people give advice on how to write a good job-seeking message. They say it helps to be clear about your skills and what you want. Others talk about the job market—some think it is getting better, while others say it is still hard to find work. A few people suggest adding contact information or LinkedIn links to make it easy for companies to reach out. Some warn against sharing too much personal data in public. There are comments from employers, too. Some say they found good hires through these threads before. Others ask job seekers to mention their time zone or work permit status. A few people talk about the value of networking and keeping an open mind about new roles. Overall, the thread is friendly and helpful, with people trying to support each other in finding jobs.

---

