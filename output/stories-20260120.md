# Hacker News 故事摘要 - 2026-01-20

## 今日概述

Today’s top Hacker News stories cover hidden science in monuments, new tools for AI safety and web crawling, and smart tips for faster databases. There are also posts about stock market risks, fast book searches, easy ways to test hardware drivers, and a new peer-to-peer network. Many stories focus on making technology safer, more open, and easier to use. If you like learning about clever ideas, practical tools, or future-proof tech, today’s stories have something for you.

---

## The 26,000-Year Astronomical Monument Hidden in Plain Sight

- 原文链接: [The 26,000-Year Astronomical Monument Hidden in Plain Sight](https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46695628)

The article talks about a hidden monument at the Hoover Dam that marks a special moment in Earth’s 26,000-year wobble, called axial precession. This terrazzo floor, found at Monument Plaza, acts like a giant celestial clock, showing the stars’ positions at the time the dam was finished.

The monument was designed in the 1930s by artist Oskar Hansen. Most people notice the big bronze statues and flagpole, but few know that the floor beneath is a detailed star map. The map uses the slow wobble of Earth’s axis, which changes the position of the North Star over thousands of years. Right now, Polaris is the North Star, but long ago it was Thuban, and in about 12,000 years, it will be Vega. The floor shows where these stars were—and will be—over time.

The article’s author wanted to learn how the monument works. He struggled to find information because the monument is not well-documented. With help from a historian, he found old plans and photos, and learned that the flagpole marks the center of the Earth’s wobble. The positions of planets and stars on the floor point to the dam’s completion date, making the monument a kind of time stamp for future generations. The author compares it to the 10,000 Year Clock project, which also tracks long-term cycles.

Key comments on Hacker News reflect a mix of surprise and admiration. Some readers are amazed that such a technical and long-term monument is mostly unknown and not explained to visitors. Others share stories of visiting Hoover Dam and walking over the plaza without knowing its meaning. A few people discuss axial precession and how hard it is to imagine such long cycles, while some mention similar monuments around the world. There are comments about the artist’s strange writing style, with some finding it confusing. Some readers wish that the dam’s tours would talk more about the monument, while others point out how rare it is to find public art that deals with such deep time. A few users express worry that people in the far future may not be able to read or understand the monument’s clues. Others praise the combination of art, science, and engineering at the plaza, saying it’s inspiring to see such big thinking made real.

---

## Running Claude Code dangerously (safely)

- 原文链接: [Running Claude Code dangerously (safely)](https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46690907)

This article talks about running Claude Code with a flag that skips asking for permissions, making it very powerful but also risky. The writer wanted a way to let Claude do anything it needs, but without risking their own computer.

Claude Code’s “--dangerously-skip-permissions” flag lets the AI install packages, change configs, and delete files without asking first. This is good for speed, but bad if something goes wrong. The author did not want to run this directly on their main computer, so they looked for safe ways to isolate it.

They first tried Docker, but needed Docker-in-Docker, which is unsafe and complicated. Other ideas like using a VM by hand or cloud VMs were either too slow, expensive, or hard to manage. Tools like firejail or Anthropic’s sandbox-runtime did not fit because they were either too limited or still had the same problems as Docker.

The writer decided to use Vagrant, which creates a full virtual machine with its own system. Vagrant is easy to set up, and you can destroy and rebuild the machine if something breaks. A small “Vagrantfile” shared in the article sets up the environment with all needed tools, and a shared folder keeps project files in sync.

Running Claude Code inside this VM lets it do anything, including installing system packages, running Docker containers, and testing web apps, without risking the main computer. If the AI messes up, you can just delete the VM and start over. Git keeps project files safe, and the system runs fast enough for normal use.

The main risks are if the AI deletes synced files (since the sync is two-way), or if a rare VM escape happens. However, for most people, this setup protects against accidents but not advanced attacks.

In the comments, many users agree that running dangerous AI code needs strong isolation. Some people share bad stories about running similar tools on their main system and losing important files. Others say that using Vagrant or even cloud VMs is a smart move. A few note that while Vagrant is older and less popular today, it is still great for real isolation.

Some users suggest using snapshots or one-way syncs for even more safety, so the AI cannot delete files on the host. Others worry about possible VM escape bugs, but most agree these are rare and not a big problem for normal use. There are also tips about fixing VirtualBox’s CPU bug and choosing other VM software if needed.

A few people think Docker can still work with careful setup, but most find it too complex. Many say that strong habits, like using git and regular backups, help reduce worries if something goes wrong. Some also point out that tools like Claude will only become more powerful, so these isolation tricks will be even more important in the future. Overall, readers think the author’s approach is practical, simple, and a good balance between speed and safety.

---

## Unconventional PostgreSQL Optimizations

- 原文链接: [Unconventional PostgreSQL Optimizations](https://hakibenita.com/postgresql-unconventional-optimizations)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46692116)

This article talks about less common ways to make PostgreSQL faster. It explains tricks that are not the usual “add an index” or “rewrite your query.” 

First, the author shows how PostgreSQL will do a full table scan, even when a query cannot possibly return results, because it does not use check constraints by default. By turning on a setting called `constraint_exclusion`, PostgreSQL can skip these scans, which is useful for ad-hoc queries in reporting tools where people can make small mistakes, like using the wrong case in a string. This can save time and computer resources.

Next, the article discusses how making indexes on the exact data you need can save lots of space and make queries faster. For example, if you only care about the date part of a timestamp, you can create an index just on that date, not the full date and time. This is called a function-based index and can be much smaller. But, it only works if everyone uses the same function in their queries. To help with this, you can use views or generated columns, so that the database always calculates the field the same way. Even better, PostgreSQL version 18 adds “virtual generated columns,” which do not use extra storage.

The author then talks about using hash indexes instead of B-Tree indexes for very large text fields, like URLs. Hash indexes are much smaller because they store only a hash of the value, not the value itself. PostgreSQL does not let you make a “unique” hash index directly, but you can use an “exclusion constraint” to do the same thing. This saves a lot of storage and can make lookups faster. However, these exclusion constraints cannot be used with foreign keys and have some limits with the `ON CONFLICT` clause when inserting data.

In the comments, many readers liked these creative tricks and thanked the author for sharing. Some said that these methods can be risky if you do not fully understand how PostgreSQL works, so you should test them carefully. Others pointed out that relying on everyone to use the same query pattern is hard in big teams, but they liked the idea of generated columns. A few warned that hash indexes may have problems with hash collisions, but the risk is low if your data is unique. There was also talk about the limits of exclusion constraints, with one reader suggesting that future PostgreSQL versions might make this easier. Some users shared their own experiences with reporting tools making big mistakes, and agreed that turning on `constraint_exclusion` can help. Others debated if the extra planning time is worth it for small queries, but most agreed it is helpful in data warehouses. A few readers said they had never heard of some of these features before and planned to try them out. Finally, some people mentioned that these tricks show how flexible PostgreSQL can be if you know where to look.

---

## Show HN: wxpath – Declarative web crawling in XPath

- 原文链接: [Show HN: wxpath – Declarative web crawling in XPath](https://github.com/rodricios/wxpath)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46618472)

This article talks about wxpath, a new Python tool for web crawling. wxpath lets you describe web crawls using XPath expressions, instead of writing loops or lots of code.

With wxpath, you can start at a URL and use special commands like url(...) and ///url(...) to move through web pages and pick out data. For example, one command can start at a Wikipedia page, find all links to other wiki articles, visit those links, and grab the title and short description from each page. wxpath does this work at the same time (concurrently) and can show you results as soon as they are found. It supports both async (for speed) and blocking (for simple scripts) ways to run crawls. wxpath tries to be polite—it uses custom headers and respects robots.txt by default. It can save results in JSON, or keep them in a local database using sqlite or redis, so you can pause and continue big crawls. There’s also a system for adding hooks, so you can change how crawling or data extraction works, for example, to only keep English pages. You can use wxpath as a Python library or from the command line, and it has options for depth, concurrency, headers, and more. The tool is still in early development, so some features like proxy rotation and JavaScript support are missing, and users should be careful to avoid crawling too much at once.

In the comments, some people are excited because wxpath makes crawling much simpler, letting you do in one line what used to take many lines of code. Others compare it to tools like Scrapy, saying wxpath is less flexible but easier for small jobs. Someone likes the idea of using XPath directly, since XPath is powerful and well-known. A few users warn that web pages change often, so XPath patterns can break. Some are worried about crawl “explosions” if you don’t set max depth. One commenter wishes for browser support to handle JavaScript-heavy sites. Others ask if wxpath can avoid being blocked, or if it has good support for politeness and caching. A few want more examples or tutorials to help new users. There is also some talk about how wxpath can help with research, because it’s so fast to set up. Overall, people like how wxpath lowers the barrier to web crawling, but agree it’s important to use it carefully and not overload websites.

---

## Nvidia Stock Crash Prediction

- 原文链接: [Nvidia Stock Crash Prediction](https://entropicthoughts.com/nvidia-stock-crash-prediction)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46693205)

This article looks at the chance of Nvidia’s stock price dropping below $100 at any point in 2026, even though it’s now about $184. The author tries to answer a prediction contest question and uses math, market data, and code to make a guess.

First, the article explains that stock prices can move up or down, and these moves have both a trend (return) and randomness (volatility). In the short term, randomness is stronger, but over longer times, the trend is more important. The author notes that Nvidia’s stock doesn’t behave like a simple random walk, because its return and volatility change over time. He points out that using just past price movements isn’t enough, since market volatility isn’t constant.

Instead, the article uses information from options trading—people who buy and sell the right to buy Nvidia stock at certain prices in the future. The prices of these options can be used to figure out how much the market expects Nvidia’s price to move. The author uses a simple mathematical model (the binomial model) and some code to match option prices to a measure called “implied volatility,” which tells how wild the market expects Nvidia’s price to be.

By looking at the prices for options that only pay out if Nvidia falls close to $100, the author estimates a daily volatility of about 3.1%. He then runs a simulation using this volatility to see how likely it is for Nvidia to drop below $100 in 2026. At first, the math says the chance is about 24%, but this is too high because the model assumes everyone is risk-neutral (doesn't mind risk), which isn’t true in reality. So, he adjusts this number using a method from the Bank of England and ends up with roughly a 10% chance. The author admits this is a rough estimate, but it’s much higher than the zero percent many might guess.

In the Hacker News comments, some readers agree with the use of option prices as a way to measure risk, saying it’s a clever and practical approach. Others warn that options markets can be thin or illiquid for far-out prices like $100, so the numbers might not be reliable. A few commenters point out that big tech stocks like Nvidia can be more volatile than people expect, especially if there are surprises in the economy or technology. Some users say that the 10% chance might still be too high, while others think it’s reasonable, given how fast markets can change.

There are also comments about the math and code in the article. Some developers like the detailed code samples and say this helps them understand option pricing better. Others discuss the limits of the binomial model and suggest that more complex models could give different answers. A few readers ask if the author considered dividends or possible stock splits.

Finally, several users argue about the usefulness of prediction markets and if they are better or worse than expert guesses. Some think the market is smart and prices in all known risks, while others believe market prices are not always right. Overall, the discussion shows that while math and markets help, predicting big stock moves is always tricky and uncertain.

---

## Fast Concordance: Instant concordance on a corpus of >1,200 books

- 原文链接: [Fast Concordance: Instant concordance on a corpus of >1,200 books](https://iafisher.com/concordance/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46652569)

This article talks about a new online tool that lets you search for any word or phrase in more than 1,200 classic books, and see where and how it is used, instantly. The tool is called Fast Concordance, and it uses books from the Standard Ebooks project, all of which are in the public domain.

A concordance is a list of where words appear in texts, often used by people who study literature or language. Usually, it takes a lot of time to make a concordance, especially for so many books. This tool makes it very quick. If you type a word, it shows you every place that word appears, along with some words before and after it. This helps you understand how the word is used in different situations. The tool works fast, even though it searches a very big set of books. It does not need to load the whole text for each search, which saves time. Instead, it uses special data structures and indexes to jump directly to the right spots. The website is simple to use. You type a word or phrase, and results show up in less than a second. This is helpful for students, writers, or anyone curious about language.

In the comments, many people say they like the tool and find it useful for research or writing. Some users talk about other concordance tools they have used before, but say this one is faster and easier. A few people ask about the technical details, like how the search is so fast, and the creator explains more about the indexing method. Others suggest extra features, such as being able to search for word patterns or use regular expressions. Some wonder if the tool could be expanded to more books in the future. A few users worry about copyright if newer books are included, but agree that public domain books are safe. One commenter mentions how this could help non-native English speakers learn how words are used. Overall, the comments are positive and show that many people are interested in this kind of tool.

---

## Linux kernel framework for PCIe device emulation, in userspace

- 原文链接: [Linux kernel framework for PCIe device emulation, in userspace](https://github.com/cakehonolulu/pciem)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46689065)

This project is about a Linux framework called PCIem, which lets people make and test virtual PCIe devices in userspace, without needing real hardware. PCIem helps developers build and debug PCIe drivers by creating fake PCIe cards that look real to the Linux kernel.

The framework works by connecting a userspace program (the "shim") to the Linux kernel through a special device file. This userspace program controls how the fake PCIe card behaves. PCIem supports important PCIe features like BAR (base address register) mapping, interrupts (including legacy IRQ, MSI, and MSI-X), and DMA (direct memory access), even supporting peer-to-peer DMA between devices. The system is modular, so you can add different PCIe capabilities as needed.

One example is a virtual graphics card made with PCIem and QEMU. This card can run games like DOOM by software rendering frames in QEMU and sending them to the host using DMA. The same setup can handle simple OpenGL games, showing that PCIem can be used for quite complex device emulation and driver testing. All of this is done without needing to buy or use real PCIe hardware, which saves time and money for both developers and hobbyists.

PCIem’s license is dual MIT/GPLv2 for the main parts, which gives users flexibility depending on their project needs. The project has detailed documentation and a blog post for people who want to learn more or try it themselves.

In the Hacker News comments, many users are excited about how PCIem can speed up driver development and testing. Some developers mention that working with real PCIe cards is expensive and hard, so this virtual system could help smaller teams and open source projects. Others ask about performance, wondering if PCIem is fast enough for real-world testing or only for basic use cases. A few commenters discuss possible uses beyond development, like security research or teaching how PCIe works.

Some users are curious about how PCIem compares to existing tools like QEMU’s native PCIe device emulation or vfio-pci. There are also questions about how safe it is to use such a framework, and if it could be used to fuzz or stress-test drivers. A few people point out that setting up PCIem still needs some technical knowledge, so it may not be for complete beginners. Overall, the community seems impressed by how PCIem makes PCIe driver work more open and accessible, while also recognizing that it is a tool for advanced users.

---

## The Zen of Reticulum

- 原文链接: [The Zen of Reticulum](https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46691660)

The article explains Reticulum, a network protocol that does not use servers or “the cloud.” It says most people think about the internet as a big cloud with powerful servers in control, but Reticulum works differently—it is peer-to-peer, with no center and no authority.

Reticulum wants true decentralization, where no one can control, block, or spy on the network. Instead of using IP addresses (which show your location), it uses cryptographic identity hashes, so your identity is not tied to where you are. Every part of the network is equal—there are no special nodes. Each message is sent directly to a person or device, not to a place. Security is built-in: every link is assumed to be unsafe, so everything is encrypted by default. Trust is not about authority or companies; it is about cryptographic proof—either you have the right key, or you don’t.

The article also talks about the value of scarcity. Modern networks waste bandwidth, but Reticulum is built to work at very low speeds, like 5 bits per second, and over any medium—including radios or wires. This makes developers think carefully about what they send, and helps everyone share the network better. Communication can be slow and asynchronous (store and forward); you send a message, and it arrives when possible. You don’t need to be always online or expect instant replies.

Reticulum lets you own your network. You don’t need to rent from big companies, and you can use cheap hardware to run your own infrastructure. If the internet breaks or is censored, your Reticulum network can keep going, even in hard situations. Your identity is portable; you can move anywhere, change how you connect (WiFi, radio, etc.), but your identity stays the same.

The protocol is designed with ethics in mind. Its license forbids using the software for harm, like building weapons or systems that hurt people. The protocol itself is public domain, free for anyone, but the official code is protected by its license. The goal is to keep the network for good, not for control or profit.

When you build software for Reticulum, you do not care what network you’re on. You send a message to a hash, and it gets there, no matter the hardware or connection type. There is no DNS or central naming system; names are hashes, and trust is built by sharing those hashes and keys directly between people.

In the comment section, many readers liked Reticulum’s strong stance on decentralization and privacy. Some praised the focus on true peer-to-peer networking, saying the internet needs more projects like this. Others were interested in how Reticulum could help during internet outages or censorship, especially in difficult times or places. A few were excited by the technical ideas, like running a network over radio or other non-traditional links, and thought the protocol could be a good fit for mesh networks or emergency communication.

Some commenters had questions about practical use. They asked about how easy it is to set up, if normal users could handle it, and how well it works in the real world. A few worried that the learning curve or the idea of hashes instead of names might confuse people. Others wondered about performance and if the protocol could handle big networks or lots of data.

There were concerns, too. Some people felt that the “no harm” license, while noble, might be hard to enforce or could limit adoption, since anyone could just copy the public domain protocol and write their own code. Others pointed out that real-world decentralization is hard, and that even peer-to-peer networks have weak points if not enough people run nodes.

A few readers compared Reticulum to other projects like Tor, Briar, or IPFS, discussing the pros and cons of each. Some were hopeful that Reticulum’s ideas might push the internet to be more open and resilient, even if it never becomes mainstream. Overall, the comments showed a wide mix of optimism, skepticism, curiosity, and respect for the project’s bold goals.

---

