# Hacker News 故事摘要 - 2025-08-15

## 今日概述

Today’s top Hacker News stories talk about new Git features for large files, how AI could change work, and ways to save money on cloud servers. There’s a clever hack for faster gym entry, rare occult books now online, and startup tips from Paul Graham. Other stories include a surprising OpenBSD benchmark, new AI tools for hardware coding, problems with Apple’s text engine, and open-source firmware for new AMD CPUs. If you like software, AI, saving money, or tech history, you’ll find something interesting today.

---

## The future of large files in Git is Git

- 原文链接: [The future of large files in Git is Git](https://tylercipriani.com/blog/2025/08/15/git-lfs/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44916783)

This article talks about how Git is getting better at handling large files, and why this might mean we won’t need Git LFS much longer. Right now, large files make Git slow and use a lot of storage, so people use Git LFS as a workaround, but it has problems of its own.

Git LFS stores big files outside your main repo, so clones are faster and smaller, but it brings extra costs and setup steps. The article explains that Git has added something called partial clone, which lets you avoid downloading big files when you first clone a repo. You can use a filter to skip files over a certain size, and Git will only download them if you actually need them. For example, cloning a repo with a single large file using partial clone was 97% faster and took up much less space compared to a normal clone.

There are some downsides: if you need the big files later, Git has to fetch them from the server, which can add a delay. But this is similar to how Git LFS works. The article points out that, unlike Git LFS, partial clone doesn’t lock you into one vendor, isn’t as expensive, and doesn’t require everyone on your team to install extra tools. Once you switch to Git LFS, it’s hard to undo, and your team might get confusing files if they don’t have LFS set up.

The article also talks about a new Git feature called large object promisors. This is still being developed, but the idea is to let Git servers store big files in a special place, making things easier and cheaper for both users and hosting companies. In the future, this could make it possible to push even bigger files to places like GitHub, without the old headaches.

For now, Git LFS is still needed for big files, but the Git project is working on better solutions. Soon, using large files in Git should be much simpler.

In the comment section, some people are happy to see these changes, saying partial clone is a big step forward. Others point out that there are still problems: not all Git hosts or tools support partial clone fully yet, and some workflows might break. A few users share stories about migrating away from Git LFS and how painful it was, while others say LFS was never a good fit for their projects. There’s debate about whether it’s ever a good idea to put large files in Git at all, with some saying it’s better to keep big binaries elsewhere. Some users are excited about large object promisors, but want to see real-world support before switching. A few worry about compatibility with older tools and the learning curve for new features. Others are just glad that Git is finally tackling these long-standing issues. Overall, most agree that these updates are good news, but say it will take time before everyone can drop Git LFS for good.

---

## AI Is Different

- 原文链接: [AI Is Different](https://www.antirez.com/news/155)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44892283)

This article talks about how AI is not like past technologies and could change the world in new ways. The writer says AI can now do things that seemed impossible a few years ago, like understanding language, writing code, and even finding bugs that skilled programmers missed.

AI models, like large language models, are still mysterious. Even top experts have trouble predicting what will happen next with them. Sometimes they overhype, and sometimes they underestimate how fast AI is moving. Before the current "transformer" models, AI was already getting better every year, and there’s no strong sign that progress will stop soon. Maybe AI will slow down, but if that happens, it will likely lead to more research and new ideas.

The author warns that if AI keeps improving and can work without much help from humans, the results will not be like older tech revolutions. Investors seem to think AI will just bring more business, like the internet or smartphones did. But the writer says this might not be true.

If AI replaces many workers, our economy will face big problems. Companies might stop paying for outside services if their own AI can do the work. It’s hard to imagine a world where only a few giant companies control all AI. Either AI will become something anyone can use, or governments might step in to change the rules.

The future could see less economic growth, and maybe we’ll need a new kind of economy. Markets do not want to face this idea, and even with uncertainty and wars, stock prices keep rising. But the author says stocks don’t matter much in the big picture. Even systems that lasted for centuries were changed forever when new knowledge arrived. AI could be that kind of change.

In the Hacker News comments, some people agree that AI is moving very fast and the future is hard to guess. Others say we’ve seen hype cycles before, and maybe AI will slow down like other technologies did. A few point out that powerful AI could make life better by taking over boring jobs, but some worry it could cause big problems for workers.

Some commenters think it’s not clear if AI will become a public good or stay under the control of a few big companies. Others say governments will probably have to create new rules to deal with AI’s impact. A few people wonder if past tech changes, like the industrial revolution, can still help us understand what’s coming, but most agree that AI is different in important ways.

Many are excited by what AI can do, but some are more worried about risks and the chance of making mistakes. A few talk about how hard it is to predict the future, and that we should be ready for surprises. People also discuss if the stock market’s optimism is real or just wishful thinking. Overall, the comments show a mix of hope, worry, and curiosity about what AI will mean for all of us.

---

## Show HN: Edka – Kubernetes clusters on your own Hetzner account

- 原文链接: [Show HN: Edka – Kubernetes clusters on your own Hetzner account](https://edka.io)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44915164)

This article talks about Edka, a service that helps you run Kubernetes clusters on your own Hetzner account. It says you can cut your cloud costs by up to 70% compared to using AWS or GCP, and you keep full control over your servers.

Edka lets you set up a full Kubernetes infrastructure in less than two minutes. You pay Hetzner directly for the servers, which usually costs much less than big cloud providers. Edka adds a control panel for easy management, upgrades, scaling, and backups. You pay Edka a flat fee for each cluster, but you can run one cluster for free. If you want to stop using Edka, you can detach it with one click—your servers and apps keep running, because you own everything. Edka also supports GitOps workflows, one-click add-ons, and built-in monitoring, making it easy to manage your clusters like a platform-as-a-service (PaaS) but without losing flexibility.

The article gives examples of real companies using Edka. Aicole, a French startup, moved from Azure to Edka and saved 64% on costs, while still keeping good performance and reliability. Another company, TROI Ticketing Solution, saved 72% on costs and made it easier for their developers to deploy apps.

People in the comment section had mixed opinions. Some thought Edka was a great idea, especially for people wanting to avoid high AWS or GCP prices. They liked that you keep control over your own infrastructure and can leave Edka anytime. Others worried about using Hetzner, since it’s not as well-known as AWS or GCP, and wondered about support and uptime. Some users said they liked the simple pricing and the one-free-cluster offer, while others asked about security and how hard it is to set up everything by yourself. A few people pointed out that Edka makes Kubernetes easier for small teams or startups, but some thought Kubernetes is still too complex for most small projects. Others shared tips about running clusters on Hetzner, mentioning good performance but also some problems with networking. Some users asked if Edka supports features like managed databases or easy backups, and a few wanted to know more about migration tools. Overall, the conversation showed interest in saving costs and owning your infrastructure, but also some caution about moving away from the big cloud providers.

---

## I accidentally became PureGym’s unofficial Apple Wallet developer

- 原文链接: [I accidentally became PureGym’s unofficial Apple Wallet developer](https://drobinin.com/posts/how-i-accidentally-became-puregyms-unofficial-apple-wallet-developer/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44910865)

This article tells the story of a developer who was tired of PureGym’s slow app and made his own Apple Wallet pass to get into the gym faster. He was frustrated that it took 47 seconds and several annoying steps just to enter the gym, while his old PIN code still worked after eight years but the app’s QR code updated every minute.

He started by trying to screenshot the QR code and add it to Apple Wallet, but that didn’t work because the codes are dynamic. He found out the API was not very secure—his 8-digit PIN, unchanged for years, was also his API password. Using tools like mitmproxy, he watched how the app fetched fresh QR codes every minute. He learned that Apple Wallet passes (built using PassKit) are more powerful than most people think—they can update themselves and pop up on your lock screen near the gym.

He built a backend in Swift using Vapor to generate and update the Wallet pass, letting his phone or Apple Watch always have the latest QR code. He scraped gym locations so the pass would appear at any PureGym. Now, instead of 47 seconds, it takes 3 seconds to get in—just a tap of his watch. Over a year, he saves about 4 hours. He also pulled in gym capacity data for his Home Assistant dashboard to see when the gym is busy.

He knows he’s breaking the rules a bit but only uses it for himself, doesn’t share the service, and is ready for it to break if PureGym changes things. He jokes that maybe PureGym should hire him, and says users care more about fast, simple experiences than company plans or roadmaps.

In the comment section, many people praised the cleverness and fun of the project. Some thought it was sad that a single person could make such an improvement so quickly, while a big company hadn’t fixed the slow experience. Others pointed out the weak security, joking about the unchanging PIN being the real risk. A few warned that reverse engineering APIs and using them this way might break terms of service, and could even get the developer’s gym account banned if PureGym cared.

Some readers shared their own pain with slow or bloated gym apps and wished more companies would use Apple Wallet or similar tech. Others wondered if this kind of hacking is good or bad for the industry—should companies be more open to letting users build what they need? A few said they’d love to see this made into an official feature, but agreed copyright and legal issues make that tough. There were also comments about Apple’s complicated PassKit system and how few people use it, saying the technical walkthrough was helpful.

Overall, people liked the story and the approach—solving a small daily annoyance with smart engineering. Some hope companies will notice how much users care about simple, fast solutions, even for tiny things like gym entry.

---

## Occult books digitized and put online by Amsterdam’s Ritman Library

- 原文链接: [Occult books digitized and put online by Amsterdam’s Ritman Library](https://www.openculture.com/2025/08/2178-occult-books-now-digitized-put-online.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44914061)

Amsterdam’s Ritman Library has put 2,178 old occult books online, thanks to help from Dan Brown, the author of “The Da Vinci Code.” These books cover topics like alchemy, astrology, magic, and more, and most were written before 1900. The library started this project in 2018 to make its rare collection available to everyone, calling it “Hermetically Open.” The books come in many languages, mostly Latin, but also German, Dutch, and French, so people who know these languages will have an easier time reading. Still, English readers can find some books if they search for works published in London or Cambridge.

Some examples of these books include strange titles from the 1600s, and many are full of old language, odd spellings, and strange ideas. If you expect simple “magic recipes,” you might be surprised, because these books are often about matching plants, animals, and the stars, or about long talks on numbers, symbols, and even early ideas about psychology. Many famous writers and thinkers from history, like Aleister Crowley, Madame Blavatsky, and even Isaac Newton, are linked with these ideas.

The library does not only focus on magic; it also has books on philosophy, religion, medicine, and science from the time when these subjects mixed with occult thinking. For example, one famous “Cambridge Platonist,” Henry More, tried to combine Plato’s views with Christianity and early science. Back then, it was hard to separate science from magic or religion.

In the comment section, one user made a joke about a well-known YouTube occultist “feeling a disturbance in the Force” because so many occult PDFs are now free. Others on Hacker News discussed how important it is to save rare books and make them open for everyone, even if the topics seem strange or outdated today. Some people were excited about reading old magical texts for fun, while others talked about the history of science and how “occult” ideas once helped shape real science. A few users wondered about the language barriers and wished for more translations into English. Some also warned that reading about old magic can be confusing if you expect it to be like stories in movies or books; the real texts are much more complex and sometimes hard to understand. Overall, most people saw the project as a great way to share rare knowledge and help us learn about the history of ideas.

---

## Do Things That Don't Scale (2013)

- 原文链接: [Do Things That Don't Scale (2013)](https://paulgraham.com/ds.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44913359)

This article is about how startups should do things that don’t scale at the beginning, even if it seems slow or messy. Paul Graham explains that startups don’t just “take off” on their own—founders must push hard to get things going.

He says you should recruit users one by one. Stripe, a payments company, did this by setting up new users’ accounts directly, instead of waiting for people to sign up. Airbnb’s founders went door-to-door in New York to help people list their apartments. These actions seem small and manual, but they are necessary to start growing. Many founders feel shy or lazy, and they think that small numbers of users are not worth the effort. But growing 10% each week adds up fast—if you keep at it, you’ll have thousands or millions of users over time.

Graham says most startups are very fragile at first. Even big companies like Microsoft and Airbnb started out small and unsure. Founders should not judge their early progress by comparing themselves to giant companies. Instead, they should ask, “How big could this be if we do the right things?” The “right things” often look unimportant, like sending handwritten notes or helping one user at a time.

He gives examples from other startups too. Wufoo sent handwritten thank-you notes to every new user. Pinterest’s founder went to design conferences to find his first fans. Facebook started by only serving Harvard students, then expanded slowly. Hardware startups sometimes build their first products by hand before they can afford a factory.

Graham says you should over-deliver for your first users. Make them feel special. This is easier for small startups than for big companies, so founders should use their small size as an advantage. Engaging directly with users gives you the best feedback. Sometimes, you can even “fake” automation by doing things manually behind the scenes until you can build the real system.

He warns against trying to launch big right away, or making big partnerships too soon. Most success comes from slow, steady growth and making users happy, not from a splashy launch.

In the Hacker News comments, many readers agree that doing things that don’t scale is key for early startups. Some share their own stories about helping every user personally or building things by hand. Others warn that this advice is not new, but it’s still hard for new founders to accept. A few worry that “doing things that don’t scale” can be misunderstood—founders might spend too much time on manual tasks and not enough on building better products.

Some point out that this advice fits best for consumer or small-business startups, but not all businesses can use it—especially those selling to big enterprises. A commenter notes that some founders try to look like big companies too soon, missing the chance to be personal and flexible. Others say that over-delivering for users helps create fans who tell others about your product.

There are also stories from founders who started by doing things manually, and how it taught them what their users really wanted. Some commenters discuss how hard it is to know when to switch from manual work to automation. One person says the biggest risk is giving up too early, before your efforts have a chance to pay off. A few readers suggest that founders should also look for signs that their market is too limited or their efforts are not working, so they can change direction if needed.

Overall, most people think that doing things that don’t scale is great advice—if you mix it with learning from your users and staying flexible about your approach.

---

## OpenBSD is so fast, I had to modify the program slightly to measure itself

- 原文链接: [OpenBSD is so fast, I had to modify the program slightly to measure itself](https://flak.tedunangst.com/post/is-OpenBSD-10x-faster-than-Linux)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44915824)

This article talks about a small benchmark test where OpenBSD runs a program much faster than Linux. The author was surprised because, usually, people say OpenBSD is slower, not faster.

The test program creates a new thread, and then both threads each make 256 network sockets. The program just tries to measure how long it takes to do this. On Linux, the code takes about 0.017 to 0.026 seconds. On OpenBSD, it runs in about 0.002 to 0.006 seconds. That means OpenBSD is about 10 times faster, at least in this test. The machines are not exactly the same, but are similar enough. The author had to change the code a little because OpenBSD was so fast, the timer could not measure such a small value. The program sometimes fails with “Too many open files,” so the author increases the file limit to 1024. The author also points out that the speed difference is not about network code. There is a hint in the code, but the main point is that OpenBSD is unexpectedly fast at making sockets with threads in this test. The author finds this funny because usually people show OpenBSD as the slow one.

In the comments, some people say the test isn’t very fair because the machines are not exactly the same. Others explain that Linux has a security feature called “seccomp” or “user namespaces” that might slow things down. Some users think maybe Linux checks more things when making sockets, so it takes longer. A few people note that OpenBSD is known for simple and clean code, which can sometimes make simple tasks faster. Another commenter says that, in real-world use, Linux is usually faster, but small tests like this can show strange results. Some people want to see more benchmarks with different tasks before trusting the result. One user jokes that benchmarks are always strange, and people can always find one that makes their favorite system look good. Others ask if this test matters for real programs, since most software does not create hundreds of sockets at once. Finally, a few people are happy to see OpenBSD win for once and say it is a good reminder that performance depends on the task.

---

## Launch HN: Embedder (YC S25) – Claude code for embedded software

- 原文链接: [Launch HN: Embedder (YC S25) – Claude code for embedded software](item?id=44915206)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44915206)

Embedder is a new tool that helps people write code for embedded devices using Claude, an AI language model. The team launched it as part of Y Combinator’s Summer 2025 batch and shared it on Hacker News to get feedback.

The main idea is to make coding for devices like microcontrollers faster and easier. Usually, writing this kind of code is slow because you need to look up lots of details and work with tricky hardware. Embedder lets you ask Claude to write code, answer questions, and help with debugging. You can upload your own code, give context about your hardware, and even ask for help with things like drivers or protocols. The tool supports popular boards like Arduino and ESP32, and you can use it in your browser.

The founders say their tool can save hours of searching online or reading data sheets. They want to help both beginners and experts by making embedded work less frustrating. They also offer a free version and a paid plan for more usage. Some people are already using it to finish projects faster, and the team is working on adding more features, like support for new chips.

In the comment section, some people think Embedder is a great idea and could help hobbyists and students start with embedded programming. Others are curious if Claude really understands hardware details or can debug difficult real-world problems. A few users ask about privacy, since uploading code to a cloud tool might be risky for private projects. Some engineers prefer to read data sheets themselves, saying that learning is part of the fun. Others wish the tool could support more exotic boards or languages. There are also questions about how well the AI handles errors or edge cases, and if it can explain solutions in simple terms. Overall, many commenters are interested to see how the tool grows and if it can really change how people work with embedded devices.

---

## TextKit 2 – The Promised Land

- 原文链接: [TextKit 2 – The Promised Land](https://blog.krzyzanowskim.com/2025/08/14/textkit-2-the-promised-land/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44917450)

TextKit 2 is Apple’s new text layout engine for iOS and macOS, meant to replace the older TextKit 1. The article’s author has spent several years using TextKit 2, even building a custom text view with it, and now shares both praise and frustration about the new system.

TextKit 2 promised an easier, faster, and more flexible way to handle text in apps. Its architecture is well-designed, offering good abstraction and letting developers work with only the parts they need. The API is modern and supports progressive complexity, which means you can start simple and add more features as you go. However, the implementation is lacking. In reality, although there are interfaces for custom storage and layout, only Apple’s built-in classes actually work. For example, you must use NSTextContentStorage; trying anything else will fail. This limits flexibility and means that old problems from NSTextStorage are still present in TextKit 2.

There are also many bugs. Some have been fixed, but others remain open for years, and bug reports often receive no response. The author especially dislikes the way TextKit 2 handles the “extra line fragment” at the end of documents, which often results in broken layouts.

A big feature of TextKit 2 is the “viewport” approach. Instead of laying out the whole document at once, it focuses only on the text you see on screen, which should make scrolling smoother and faster. But this causes new problems: the total height of the document is only an estimate, and it keeps changing as you scroll. This makes the scroll bar jump around and feel unstable. The author tried many tricks to work around it, but none worked well. Even Apple’s own TextEdit app suffers from these glitches.

After years of trying, the author feels the system is too hard to use for real-world apps. The design is good, but the details are frustrating, and he now thinks TextKit 2 might not be the best choice for text editing UIs.

In the comments, some people agree and share their own struggles. Many say they also found TextKit 2 confusing, buggy, or poorly documented. A few mention that while the idea of a viewport is smart, it’s hard to get right, and Apple’s own apps don’t always work smoothly with it.

One developer says they avoid TextKit 2 and stick to older tools, even if they are not perfect. Another wonders if Apple made the API too complex for normal developers, and wishes for better guides or real-world examples. Someone else points out that Apple often designs great frameworks but sometimes doesn’t finish them or fix reported problems.

On the other hand, a few people say that TextKit 2 is still new, and hope it will improve over time. They remind others that new Apple APIs often need a few years to mature, and early bugs are common. Some suggest that for most basic uses, TextKit 2 is fine, but it breaks down for custom or advanced text editors.

A couple of commenters ask whether open-source alternatives might be better for big text projects. Others say they appreciate the author’s honesty, because most online posts only talk about the good parts.

Overall, the community sees TextKit 2 as a step forward in theory, but not yet ready to be the “promised land” for all text editing needs. Many hope for better tools and fixes from Apple in the future.

---

## Porting Gigabyte MZ33-AR1 Server Board with AMD Turin CPU to Coreboot

- 原文链接: [Porting Gigabyte MZ33-AR1 Server Board with AMD Turin CPU to Coreboot](https://blog.3mdeb.com/2025/2025-08-07-gigabyte_mz33_ar1_part1/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44916900)

This article explains how developers are trying to run coreboot, an open-source firmware, on the Gigabyte MZ33-AR1 server board with a new AMD Turin CPU. AMD released code for their Turin CPUs to help the open-source community, so the project’s first step is getting Turin support into coreboot.

The first main goal was to create a basic skeleton for the Turin system-on-chip (SoC) in coreboot. Developers started by copying and editing code from the older AMD Genoa SoC, since Turin is similar but has key differences (like fewer USB ports and new memory settings). They had to compare source files and AMD’s documentation, make changes for the new hardware, and figure out how PCI domains and addresses work for Turin. This process included help from other coreboot developers and lots of troubleshooting.

Next, they focused on the Platform Security Processor (PSP) firmware. PSP is a small computer inside AMD chips that helps start the CPU and handle security. They needed to gather special firmware blobs from AMD’s GitHub and from the vendor’s BIOS image. Tools like PSPTool and UEFITool helped extract these blobs, but the tools needed fixes and improvements to handle the new Turin images.

The team also set up mainboard code for the MZ33-AR1. This code connects the board to the new SoC code and sets up features like the serial console. They prepared the files and settings needed for a simple boot, making sure the board could start and show messages on the serial port. They found workarounds to build a bootable image even when the public PSP blobs didn’t work yet. This included copying address settings from the vendor BIOS so coreboot would load correctly.

The article walks through the exact steps for extracting blobs, setting up files, and building the firmware. It ends by showing the first successful boot messages, though with some errors, and explains that support for Turin CPUs in coreboot is still experimental.

In the Hacker News comments, some people are excited about more open-source firmware for powerful AMD servers, saying it’s a big win for transparency and security. Others point out the process is still complex and requires vendor blobs, so it’s not fully open yet. A few commenters worry about the PSP, which is closed-source and could be a security risk, even if the rest of the firmware is open. Some developers appreciate the detailed technical steps, saying guides like this help more people learn about firmware hacking.

There’s discussion about the usefulness of tools like PSPTool and the challenges of reverse-engineering vendor firmware images. Some users mention that open firmware is important for server owners who want better control and security, especially in sensitive environments. Others ask if AMD will ever allow a truly open boot process, or if some parts will always need closed blobs. Finally, a few people praise the teamwork between open-source developers and hardware companies, but remind everyone this is early work and not ready for production servers yet.

---

