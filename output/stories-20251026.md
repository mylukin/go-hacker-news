# Hacker News 故事摘要 - 2025-10-26

## 今日概述

Today’s top Hacker News stories cover old and new tech. There is a look back at Cold War bunkers and electron microscopes, plus new projects like a homemade operating system and a modern scripting language. Other stories talk about tricky bugs in machine learning and PyTorch, changes in a popular coding contest, and problems with unencrypted satellite data. There is also news on Alzheimer’s research and issues with the latest AI hardware. Most stories highlight big changes, clever engineering, and the risks of new technology.

---

## NORAD's Cheyenne Mountain Combat Center, C.1966

- 原文链接: [NORAD's Cheyenne Mountain Combat Center, C.1966](https://flashbak.com/norad-cheyenne-mountain-combat-center-478804/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45652218)

This article is about the building and use of the NORAD Cheyenne Mountain Combat Center in the 1960s, which was a secret bunker for North American air defense during the Cold War. It explains why the center was needed, how it was built, and what technology it used to watch for attacks.

The first operations center was just a small office and even used an old bathroom, which was not safe or big enough. As threats from the Soviet Union grew, especially with faster missiles, the U.S. Air Force knew they needed a bigger, safer place underground. Work on the Cheyenne Mountain bunker began in 1961 and finished in 1964. The bunker was made to keep working even if a nuclear bomb hit nearby. It could run for 30 days without outside help, with its own power, water, and food.

Inside, there were 11 separate buildings on big steel springs to stop shock from blasts. The staff used computers, like the Philco 212, and big screens to watch for missiles, planes, and satellites. The bunker had special radio antennas on the mountain, thick cement walls, and lots of steel for protection. There was also a special camera called the Baker-Nunn to track satellites in space with high accuracy. Both U.S. and Canadian officers worked together in the center, sharing command. The article shows that the center was built to be almost impossible to destroy and to keep people and equipment safe from all kinds of attacks.

The top comments on Hacker News reflect on how impressive the technology and engineering were for the time. Some people are amazed that a Cold War bunker could run for a month without outside help, and they compare it to modern data centers or even sci-fi movies. Others point out how much computing has changed since then—the Philco 212 computers were huge and slow compared to today’s smartphones, but they were cutting edge back then. A few commenters talk about the fear and tension of the Cold War, saying the bunker was a sign of just how real the threat felt.

Some users wonder about the cost and whether such bunkers are still needed with new types of threats today, like cyber attacks instead of bombs. Others share personal stories, like family members who worked at Cheyenne Mountain or visited it. A few mention the famous role of the bunker in movies and TV shows, like “WarGames” and “Stargate,” showing its place in popular culture. There’s also discussion about safety features, like the springs under the buildings, and respect for the engineers who planned for every possible danger. Some commenters even joke about how much more comfortable the bunker was than their own offices. Overall, people are both impressed and a little nostalgic about this unique piece of Cold War history.

---

## Show HN: MyraOS – My 32-bit operating system in C and ASM (Hack Club project)

- 原文链接: [Show HN: MyraOS – My 32-bit operating system in C and ASM (Hack Club project)](https://github.com/dvir-biton/MyraOS)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45715055)

MyraOS is a new operating system made from scratch in C and assembly, made for x86 computers and shared as a Hack Club project. The goal was to make a real Unix-like OS that can run on real devices, not just an experiment.

The OS has many features: it runs in protected mode, with memory management, paging, and both user and kernel modes. There is support for things like drivers (for keyboard, mouse, disk, display), an ext2 filesystem, and even a user interface with windows, buttons, and icons. The OS can run real programs using an ELF loader, and it even comes with a port of the classic game Doom built in and ready to play. You can try MyraOS using QEMU, a virtual machine tool, with simple commands for Windows, Linux, or macOS. The creator asks for feedback and feature requests, and says they spent a lot of time building this project.

In the comments, many people say it is impressive to write an OS from scratch, especially with so many working features. Some ask about performance and if it can run on real hardware, not just virtual machines. Others are curious how the Doom port works, and if it means the OS has good graphics and sound support. A few people discuss the learning value of building an OS, saying projects like this are great for understanding how computers work at a low level. There are questions on what was hardest—memory, drivers, or process handling. Some think the project could use more documentation or easier build steps for beginners. Others share links to their own or similar OS projects and encourage the creator to keep going. A few warn that OS development is hard to maintain alone and suggest finding more contributors. There is a lot of praise for the clear goals and working features, and people like that it is not just a “toy” OS. Some hope the creator will write blog posts about the process, since it could help others learn. Overall, people are supportive and excited to see new OS projects like MyraOS.

---

## Wren: A classy little scripting language

- 原文链接: [Wren: A classy little scripting language](https://wren.io/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45671475)

Wren is a small scripting language made for people who want something simple, fast, and easy to use in other programs. The article explains that Wren is inspired by languages like Smalltalk, Lua, and Erlang, but uses modern and familiar syntax.

Wren’s main points are its small size—the core code is easy to read and not complicated. It is fast because it compiles code in one step and runs tight bytecode. Wren is class-based, so you use classes and objects like in other big languages. It supports concurrency with something called fibers, letting you run many tasks at the same time. Wren is made for embedding, which means you can add it to your apps without needing big extra libraries. The language works with C and C++ and is easy to build. The website gives guides for syntax, lists, maps, variables, classes, functions, and more. There’s also a place to try Wren in your browser, and clear instructions for getting started. The code examples look simple, showing how to print text, make classes, and use fibers for tasks.

In the Hacker News comments, some people really like Wren’s small size and clear code—they say it’s easy to read and understand. Others think fibers are a cool way to do many things at once, and they like that Wren is easy to add to other programs. A few users compare Wren to Lua, saying Lua is still more popular for embedding, but Wren’s class-based style feels more modern. Some developers wish Wren had a bigger standard library or more tools, but others like it being minimal. There are people who ask about real-world uses, and some share stories of using Wren in games or small apps. A few say the syntax is friendly for beginners, while others wonder if it will grow a bigger community. Some users are excited to try Wren in the browser, and others thank the creators for good documentation. A few have questions about performance, but most agree Wren is fast enough for scripting jobs. Overall, the comments show interest, some questions, and a mix of hope and caution about Wren’s future.

---

## The bug that taught me more about PyTorch than years of using it

- 原文链接: [The bug that taught me more about PyTorch than years of using it](https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45684253)

The article is about a strange bug in PyTorch that stopped a neural network from learning on an Apple Silicon Mac. The writer tried to fix the problem by changing hyperparameters and checking their code, but nothing worked.

The bug was not in the model or the data—it was deep inside PyTorch’s backend. The loss during training would stop improving because the encoder weights of the model never changed. The writer found that gradients were flowing and the optimizer was working for the decoder, but not for the encoder. Using the Adam optimizer, the encoder weights never updated, but with SGD they did. This made the writer look closer at how Adam works and check PyTorch's internal state tensors.

The main cause: PyTorch’s MPS (Apple Silicon GPU) backend had a bug in how it handled non-contiguous tensors with the `addcmul_` and `addcdiv_` operations. When a tensor is non-contiguous (not stored in a straight line in memory), these operations would silently fail on MPS. For the model’s encoder, weights were initialized by transposing the decoder, making them non-contiguous. Adam's state tensors (`exp_avg`, `exp_avg_sq`) copied this layout. So, when Adam tried to update these tensors on MPS, nothing happened—the weights just stayed the same.

The fix was to make tensors contiguous at initialization or upgrade to PyTorch 2.4 or macOS 15, where the bug is fixed. The article explains how PyTorch picks different "kernels" (low-level code) for different devices and how these kernels can behave differently. It also shows how to check tensor properties like `.is_contiguous()` and why memory layout matters for performance and correctness.

In the comments, many developers shared similar stories about debugging PyTorch or machine learning bugs. Some said they always blame their own code first, just like the writer did. Others noted that device-specific bugs are hard to find and that Apple’s MPS backend is still maturing. Some users said this is why they prefer to use only well-tested backends like CUDA. There was debate about whether PyTorch should throw errors instead of silently failing, with most agreeing that silent failures are the worst.

A few commenters found the deep dive into PyTorch internals very helpful, saying that it’s rare to see such clear explanations of backend mechanics. Others pointed out that debugging tools and error messages could be better for these types of bugs. Some suggested always making tensors contiguous to avoid trouble, while others warned that this can slow down code if used carelessly.

Several people praised the way the article showed each debugging step and thought process. They said it helped them feel less alone when stuck on hard bugs. Others mentioned that reading real-world debugging stories teaches more than official documentation. Some developers were surprised that random tensor operations in PyTorch can still fail in the same way on Apple Silicon, even in newer versions. A few users added that this kind of bug shows why open-source software and good bug reports are important. Overall, the comments agreed that the article was both useful and encouraging for anyone working with machine learning frameworks.

---

## Advent of Code 2025: Number of puzzles reduce from 25 to 12 for the first time

- 原文链接: [Advent of Code 2025: Number of puzzles reduce from 25 to 12 for the first time](https://adventofcode.com/2025/about#faq_num_days)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45710006)

Advent of Code 2025 will have only 12 puzzles instead of the usual 25 for the first time. The event is still made by Eric Wastl and will keep most of its old traditions, but this is the biggest change to the format in ten years.

The event is an online programming contest with small puzzles for all skill levels. You can solve puzzles in any programming language. Many people use Advent of Code for learning, interview practice, company training, or just for fun. You do not need a deep computer science background—basic programming and problem-solving skills are enough. Problems are designed to run quickly on old computers, so you do not need fancy hardware. If you want to support the event, you can share it or donate through AoC++. Tips are given for solving puzzles: check examples, make your own test cases, ask friends, or try again later. There is a subreddit for hints.

Some questions are answered in the FAQ. The site has a high-contrast mode if the text is hard to read. All puzzles unlock at midnight EST/UTC-5, fitting Eric’s schedule. The global leaderboard is gone because it caused stress and even led to bad behavior like DDoS attacks. Now, you can only compete through private leaderboards with friends. You should follow the rules of your private leaderboard, and asking for help or using AI depends on those group rules. The creator does not want people to send in puzzle ideas or copy the site’s content. Advent of Code is free to play, but you cannot redistribute puzzles or inputs.

People on Hacker News have many opinions about the change. Some users are sad to see fewer puzzles, saying it was a nice December tradition. Others think 12 puzzles is better; it is easier to finish, and more people will complete the event. A few users mention that making 25 puzzles every year must be very tiring for Eric, and they support the change to help him avoid burnout. Some are worried that the event will lose its “advent calendar” feel, but others argue that quality is more important than quantity. There are comments from people who never finished all 25 puzzles, so this new format feels more reasonable for busy people. A few wish the old global leaderboard would come back but agree that cheating and stress made it hard to keep. Some users like that private leaderboards and friendly competition are still possible. Others say they will miss the daily December routine but are grateful the event continues at all. Many agree that it is better to have fewer puzzles than to risk losing Advent of Code entirely.

---

## System.LongBool

- 原文链接: [System.LongBool](https://docwiki.embarcadero.com/Libraries/Sydney/en/System.LongBool)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45664139)

This article explains what System.LongBool is in RAD Studio, a set of tools for building apps. System.LongBool is a special type used in the programming languages Delphi and C++ to show true or false values. Unlike a normal Boolean, which may use just 1 byte, LongBool always uses 4 bytes (32 bits) in memory. In code, if the value is 0, it means false. If it is anything else, it means true. This can help when working with other programming languages or when talking to the operating system, because some systems expect Boolean values to be 4 bytes. LongBool is not the only Boolean type in RAD Studio. There are also ByteBool and WordBool, which use 1 and 2 bytes. The main reason to have all these types is so that your code works well with different libraries and systems. The article also points you to other types like System.Boolean for more standard use.

People in the comments talk about why these different Boolean types exist. Some say that using LongBool can avoid bugs when working with Windows APIs, because Windows sometimes expects a 4-byte Boolean. Others warn that LongBool can be confusing, especially if you mix it with other types, because only 0 is false and any other number is true—so mistakes can happen if you are not careful. Some users share stories about old bugs caused by mixing Boolean types, or when code expected only 0 and 1, but got another number. A few developers point out that the differences between ByteBool, WordBool, and LongBool are mostly about making C++ and Delphi code work together smoothly. One comment says that in modern code, you should try to use standard Boolean types unless you know you need compatibility. Another comment reminds everyone to check the documentation when writing code that calls system libraries, to avoid surprises. Some users find it strange that such small details can break programs, but others say it is normal when working close to the hardware or different compilers. Overall, the comments show that while LongBool is simple, using the right type matters a lot in real projects.

---

## Alzheimer's disrupts circadian rhythms of plaque-clearing brain cells

- 原文链接: [Alzheimer's disrupts circadian rhythms of plaque-clearing brain cells](https://medicine.washu.edu/news/alzheimers-disrupts-circadian-rhythms-of-plaque-clearing-brain-cells/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45713738)

This article talks about new research showing how Alzheimer’s disease changes the daily rhythms of certain brain cells that help clear away harmful plaques. The study used mice to see how Alzheimer’s affects genes in microglia and astrocytes—cells that clean the brain.

Normally, our body runs on a circadian rhythm, which is like an internal clock controlling when genes turn on and off. Alzheimer’s patients often have trouble sleeping and get confused at certain times of day, and this study helps explain why. The researchers found that in mice with Alzheimer’s, the usual timing of hundreds of genes in these cleaning cells gets mixed up. This is different from what happens with normal aging.

Some of the affected genes are linked to Alzheimer’s risk, and about half of these are controlled by this internal clock. The research suggests that when the clock in these brain cells is disrupted, the cells don’t clear away amyloid—an Alzheimer’s-related protein—as well as they should. This can make the disease worse. The study also found that the presence of amyloid causes some genes to start following new, abnormal rhythms, especially those involved in brain inflammation.

The scientists think that if we can find ways to fix or control these rhythms in brain cells, we might be able to slow down or stop Alzheimer’s. They hope to test ways to make the internal clock stronger or weaker in specific cells to improve brain health.

In the comments, some users were excited about the idea of fixing circadian rhythms as a possible treatment for Alzheimer’s. Others pointed out that sleep problems often come before memory loss, so this research makes sense. A few commenters wondered if better sleep habits could help protect against Alzheimer’s. Some were cautious, saying that results in mice do not always apply to humans. Others discussed how difficult it can be to measure and change circadian rhythms in real patients. There were also comments about the importance of microglia and astrocytes, with some users sharing their own experiences caring for family members with Alzheimer’s. A few people hoped for more research into how diet, exercise, and light exposure might help the brain’s clock. Some users felt hopeful, while others said it will still take a long time to turn these findings into real treatments.

---

## Nvidia DGX Spark: When benchmark numbers meet production reality

- 原文链接: [Nvidia DGX Spark: When benchmark numbers meet production reality](https://publish.obsidian.md/aixplore/Practical+Applications/dgx-lab-benchmarks-vs-reality-day-4)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45713835)

This article looks at real-world machine learning (ML) work on the NVIDIA DGX Spark system, comparing NVIDIA’s official benchmark numbers to actual use in production. The writer spent six days training and fine-tuning large models, then checked if the hardware really worked as well as NVIDIA claimed.

NVIDIA’s benchmarks say DGX Spark is very fast—tens of thousands of tokens per second for training and high speeds for inference. The writer tested these claims using different models, fine-tuning methods, and workloads. For training, the numbers matched up: models trained quickly, loss curves looked good, and the claimed speed was real. Inference (making predictions) was also fast, especially when using smaller models, and 4-bit quantization (making models smaller and faster) did not hurt quality.

But, the writer found several problems not shown in the benchmarks. The biggest issue: GPU inference (using the graphics card to make predictions) was broken. Models trained fine, but when trying to use the GPU to generate answers, the output was empty or full of errors. Running inference on the CPU worked, but it was much slower. Training on the GPU also had problems: memory fragmentation caused the system to freeze after a few hours, which meant work could be lost unless the job was split into short sessions with frequent checkpoints and manual memory clearing. Some software, like llama.cpp, did not work at all on this hardware and setup.

The root causes are the “bleeding edge” combination of ARM64 architecture, the new Blackwell GB10 GPU, and the latest CUDA version. Many machine learning tools are not fully ready for this setup, drivers are new, and some bugs or missing features mean production use is risky. The writer points out that NVIDIA’s public benchmarks likely used carefully chosen setups and did not mention these real-world issues.

In short, training is fast and works with workarounds, CPU inference is slow but reliable, but GPU inference is not ready for production. Anyone using DGX Spark now must be an expert, use special tricks for stability, and should not expect everything to “just work” like the marketing says.

In the Hacker News comments, several readers agreed that hardware benchmarks often hide real-world pain points. Some said they had similar problems with new GPUs and drivers, especially on non-x86 systems, and shared tips like splitting jobs into smaller parts or using older, more stable hardware for important work. Others noted that NVIDIA’s marketing often focuses on best-case numbers and that production ML always requires more testing and manual work than people expect. A few pointed out that edge-case issues are common with new tech, and that it usually takes 6–12 months before things become stable enough for easy use. Some users asked if these problems could be fixed soon with better drivers or updates, while others wondered if alternative hardware or cloud solutions would be wiser for now. One commenter praised the thorough documentation and wished more people wrote up “what went wrong” stories. Overall, readers agreed that “bleeding edge” hardware is powerful but risky, and that honest reviews like this help the community avoid wasted time.

---

## Making the Electron Microscope

- 原文链接: [Making the Electron Microscope](https://www.asimov.press/p/electron-microscope)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45713253)

This article is about the history and invention of the electron microscope, a tool that lets scientists see things much smaller than what normal light microscopes can show. It explains why scientists needed a new way to see tiny things, because light microscopes could not show details smaller than about 200 nanometers—too large to see viruses, proteins, or the inside of cells.

The story starts with early microscopes and their limits, then moves to key discoveries about how light works and why its wavelength sets a limit on what we can see. Scientists tried X-rays to see smaller things, but those had problems too, as they couldn’t focus X-rays easily and needed special crystals. The big breakthrough came in the 1930s, when Ernst Ruska and Max Knoll in Germany built the first electron microscope by using magnetic fields to focus electron beams, instead of glass lenses to focus light. This allowed much higher magnification because electrons have much shorter wavelengths than light.

At first, the new electron microscopes were not very powerful, but over time, scientists improved the design. They learned how to make thin samples, use special stains, and even freeze samples to see them better—leading to cryo-electron microscopy, which can now show single atoms in proteins. The electron microscope helped scientists study viruses, like the ones that cause polio and COVID-19, and it played a role in creating vaccines. The story also talks about other important inventors, like Reinhold Rüdenberg, whose son’s sickness inspired him to design his own electron microscope, and Ladislaus Marton, who took the first biological electron images.

The article shows how making the electron microscope took teamwork, new physics, and clever engineering. It also explains some problems with electron microscopes: they are very big and expensive, need special rooms, and can only see dead or frozen things, not living cells in motion. Even so, the electron microscope has changed biology, letting scientists see how life works at the smallest level.

In the comments, many readers praise the article’s clear and detailed storytelling. Some are amazed by how many people contributed to the invention, not just one “lone genius.” A few readers highlight the role that industry (like Siemens and Zeiss) played in turning experiments into real machines. Others mention that the story shows how advances in one field (like physics) can change another (like biology).

Some users share memories of using electron microscopes in their own jobs, talking about how hard it can be to prepare samples and operate the machines. A few point out that while electron microscopy is powerful, it still cannot replace light microscopes for watching live cells or fast changes. One commenter wonders if new technology, like AI or smaller, cheaper machines, might someday make electron microscopy more common in schools or smaller labs.

Others discuss the human side of the story—how war and politics shaped who got credit or could keep working, especially with Rüdenberg’s troubles in Nazi Germany. Some argue over who should be called the “true” inventor, but most agree that invention is often messy and shared. Finally, several readers say the article inspired them to learn more about the people behind the science, showing that discovery is often driven by personal struggles, teamwork, and even luck.

---

## Eavesdropping on Internal Networks via Unencrypted Satellites

- 原文链接: [Eavesdropping on Internal Networks via Unencrypted Satellites](https://satcom.sysnet.ucsd.edu/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45650167)

Researchers used a regular satellite dish to listen to signals from geostationary satellites and found a lot of sensitive data being sent without encryption. This data included phone calls, SMS, internet traffic, military and government information, in-flight Wi-Fi activity, and even banking or power company records.

They picked up unencrypted mobile phone traffic, including calls and user IDs, being sent from telecom networks to remote cell towers. Some military and police systems also sent out unprotected data, like ship tracking and surveillance info. Airplanes using in-flight Wi-Fi were leaking passengers’ web browsing and crew data. Some VoIP services sent call audio and metadata in the open. Banks and stores sent login details, emails, and inventory lists without protection. Even power grids and oil pipelines sent control messages “in the clear.”

The researchers say there is no one group in charge of encrypting satellite links, so fixing the problem is slow. When they told companies about leaks, some—like T-Mobile and Walmart—fixed the issue. But many others have not responded yet, or are still being contacted. As a result, some details are not public yet.

For everyday users, there’s no way to know if your data goes through these unprotected links. Most web traffic is safe because of HTTPS, but calls and texts in some places are at risk. Users can try using VPNs or encrypted apps like Signal for better safety. Companies using satellites are told to always use encryption, treating satellite links as if they are public Wi-Fi.

The study used a cheap satellite dish and TV tuner, plus custom software to pick up and decode the signals. The team could see about 14% of global satellite traffic from one spot in California. The main reason for not using encryption is cost—encryption takes up more bandwidth and needs better hardware. Some companies also think the risk is low or do not realize their data is exposed.

In the Hacker News comments, many people were shocked that such important data is still sent without encryption in 2025. Some wondered why organizations use satellites for sensitive data at all, or why they don’t treat satellite links as risky. Others pointed out that adding encryption can be hard for older systems and slow connections, but still shouldn’t be skipped. A few said that security experts have warned about this problem for years, but companies often ignore the advice until something bad happens. Some commenters shared stories about working with satellite networks and seeing similar issues. Others debated whether new systems like Starlink have the same risks, but most agreed that all sensitive data should be encrypted. Some users worried about privacy, and a few asked if this eavesdropping is legal or ethical. Many were impressed by how simple the equipment was, showing that anyone could listen in if they tried. Overall, the community agreed that the industry needs to take satellite security much more seriously.

---

