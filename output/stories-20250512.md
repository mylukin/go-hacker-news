# Hacker News 故事摘要 - 2025-05-12

## 今日概述

Today's top stories on Hacker News focus on innovation, tech ethics, and community projects. A developer worked on disabling Windows Defender, raising security debates. INTELLECT-2, a new AI model, uses decentralized learning for better reasoning. Scraperr, a web scraping tool, sparked talks on ethical practices. A minimal LSP client in Clojure caught attention for its simplicity. The Continuous Thought Machine (CTM) mimics human thinking in AI. Discussions also covered AI's societal impact and the Absolute Zero Reasoner's self-learning approach. Lastly, CoMaps, a fork of Organic Maps, aims for better community governance.

---

## I ruined my vacation by reverse engineering WSC

- 原文链接: [I ruined my vacation by reverse engineering WSC](https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43959403)

The blog post tells a story about a developer who spent his vacation in Seoul working on a project to disable Windows Defender using the Windows Security Center (WSC) API. This wasn't the first time he worked on such a project. A year prior, he created a tool called "no-defender" that disabled Windows Defender by tricking it into thinking another antivirus was installed, but it was taken down due to legal issues. During his vacation, a friend asked if he could create a cleaner version of the project without using third-party antivirus software. Despite not having the right computer gear with him, the developer dived into a complex process of reverse engineering, dealing with technical challenges and using remote access to a friend's PC to test his code.

The comments section on Hacker News reveals a mix of opinions. Some users suggest alternative ways to disable Windows Defender, such as renaming directories or using group policies. Others discuss the challenges of managing Windows Defender on newer operating systems, like Windows 11, where updates can undo user changes. A few commenters express frustration with Windows' security features, arguing that they are overly intrusive and resource-intensive. Meanwhile, some people highlight the potential security risks of disabling such features, pointing out that a balance is needed between security and system performance. The conversation also touches on the broader issue of user control over their own devices, with some users advocating for more freedom to customize their systems, while others emphasize the importance of security measures to protect less knowledgeable users.

---

## Intellect-2 Release: The First 32B Model Trained Through Globally Distributed RL

- 原文链接: [Intellect-2 Release: The First 32B Model Trained Through Globally Distributed RL](https://www.primeintellect.ai/blog/intellect-2-release)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43958898)

The article discusses INTELLECT-2, a new model with 32 billion parameters, trained using a decentralized approach called globally distributed reinforcement learning (RL). Unlike traditional centralized methods, this approach uses a network of diverse, permissionless computers working asynchronously. Key components developed for this include PRIME-RL, a framework for decentralized training; TOPLOC, which verifies outputs from untrusted sources; and SHARDCAST, which distributes model updates efficiently. The model aims to improve reasoning skills by combining new training techniques and infrastructure.

The article highlights how INTELLECT-2 uses a unique RL training method across a global network, which is a significant shift from centralized training requiring large GPU clusters. It introduces infrastructure components like PRIME-RL for decentralized training and SHARDCAST for distributing model updates. Training involved 285,000 tasks, and new techniques were used to stabilize training, such as two-sided GRPO clipping and advanced data filtering. Experiments showed improvements in tasks like math and coding, although further enhancements need better base models or datasets. The project seeks to foster open research and decentralize training to make AI development more accessible.

In the comments, some readers noted the interesting choice of the project's name and logo, referencing a well-known sci-fi novel. Others debated the potential of using such decentralized systems as a form of "proof of work" for cryptocurrencies, though opinions varied on its practicality. Some commenters were skeptical about the project's impact, suggesting it merely fine-tuned an existing model with little improvement. Others praised the infrastructure as the main achievement, likening it to selling tools during a gold rush. Concerns about privacy in decentralized networks were raised, suggesting the need for secure encryption methods. Overall, the project sparked discussions on its technical merits and implications for AI and distributed computing.

---

## Scraperr – A Self Hosted Webscraper

- 原文链接: [Scraperr – A Self Hosted Webscraper](https://github.com/jaypyles/Scraperr)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43955842)

The article is about Scraperr, a tool for self-hosted web scraping. It's a project available on GitHub and allows users to extract data from websites. 

Scraperr is designed to be run on your own server, meaning you have more control over how it works. It uses technologies like Selenium to navigate and scrape web content. The tool supports various selectors, like XPath, to precisely target the data you want to extract. One of its goals is to make web scraping more accessible by being straightforward to set up and use. The creator mentions plans to update the tool, possibly switching to Playwright for better performance and features. Ethical use is encouraged, such as respecting a site's robots.txt file and following terms of service.

The Hacker News comments reflect a mix of opinions. Some users emphasize the importance of ethical scraping practices, like including a contact method in the user agent string and respecting robots.txt files. Others discuss the challenges of dealing with aggressive scrapers that ignore these guidelines, leading to IP bans. Some commenters debate the ethics of using robots.txt to favor certain search engines while blocking others. There's also discussion about the technical aspects of web scraping, such as managing long page load times and dealing with CAPTCHA challenges. Many users share their experiences with different scraping tools, including Playwright and Selenium, and discuss strategies to improve scraping efficiency and avoid detection. Overall, the comments highlight the complexities and ethical considerations involved in web scraping.

---

## LSP client in Clojure in 200 lines of code

- 原文链接: [LSP client in Clojure in 200 lines of code](https://vlaaad.github.io/lsp-client-in-200-lines-of-code)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43955397)

The article discusses how the author created a minimal LSP (Language Server Protocol) client in Clojure using only 200 lines of code. LSP is a standard that helps text editors communicate with language servers, which understand programming languages. This setup simplifies integration between different editors and languages. The author explains the implementation, focusing on the base communication layer, JSON-RPC protocol, and a simple API to interact with language servers. The article ends with an attempt to build a command-line linter using this client, revealing some challenges with language server implementations.

Now, let's dive into the comments. One user is intrigued by the use of Clojure in Defold, a game development tool, and wants to explore this further. Another user comments on Clojure's popularity, saying it's not dying but isn't as hyped as it once was. They mention that Jank, a Clojure variant, might gain more attention. Some users discuss the verbosity and style of the code, with one saying it resembles Java more than idiomatic Clojure. Another user defends the code, stating it uses Clojure's features well and suggests that using core.async might be unnecessary.

There is also a debate on the readability and structure of the code. One user finds the code hard to read due to long functions, while another defends the design choices, saying they could easily test and understand the program's behavior using Clojure's tools. Finally, a comment mentions the trade-offs of using Clojure, such as startup time and the dependency on a large standard library, which might not be apparent from the "200 lines of code" claim.

---

## Continuous Thought Machines

- 原文链接: [Continuous Thought Machines](https://pub.sakana.ai/ctm/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43959071)

The article "Continuous Thought Machines" discusses a new AI model called the Continuous Thought Machine (CTM), which aims to bridge the gap between modern AI's efficiency and the biological plausibility of neuron timing in brains. Current AI systems often ignore the timing and synchronization used by neurons, which are crucial for biological intelligence. The CTM integrates neural timing, allowing it to process data in a way that is more similar to human cognition, potentially leading to more flexible and adaptable AI systems.

The CTM introduces three key ideas: a decoupled internal dimension for sequential thought, neuron-level models with private weights for each neuron, and using neural synchronization as a representation for solving tasks. This architecture allows the CTM to dynamically process data, creating richer internal representations without relying on large data inputs. Experiments with the CTM show promising results in tasks like image classification, maze solving, and parity tasks, demonstrating its ability to generalize and perform adaptive computation.

In the Hacker News comments, some users expressed concerns about the CTM's lack of acknowledgment of existing research on biologically plausible networks, like spiking neural networks. Others highlighted the potential confusion caused by the terminology used in the paper. Some commenters appreciated the exploration of continuous processing and neural synchronization, while others debated the practicality and novelty of the CTM approach. There was also discussion about the challenges of simulating time dynamics in neural networks and the potential of such models to advance AI towards more human-like intelligence. Overall, while the CTM sparked interest, it also raised questions about its alignment with existing neuroscience-inspired research.

---

## Avoiding AI is hard – but our freedom to opt out must be protected

- 原文链接: [Avoiding AI is hard – but our freedom to opt out must be protected](https://theconversation.com/avoiding-ai-is-hard-but-our-freedom-to-opt-out-must-be-protected-255873)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43958382)

The article discusses the growing presence of artificial intelligence (AI) in daily life and the difficulty of opting out of its influence. It highlights how AI is involved in various areas like job applications, healthcare, and finance, often making critical decisions without human input. The piece argues that people should have the right to live without AI's influence, as its pervasive use could lead to social and economic exclusion for those who opt out.

AI is now integral to many essential systems, such as healthcare, government services, and even our social media feeds. This integration creates challenges for those who wish to avoid AI, as they might have to forgo many modern conveniences and services. The article also points out the biases present in AI systems, like discrimination in hiring or credit scoring, which can lead to unfair treatment of certain groups. The author draws an analogy with "The Sorcerer's Apprentice," suggesting that AI, like magic, can be beneficial but also dangerous if not controlled properly. The piece argues for the need for transparency and accountability in AI systems and emphasizes the importance of digital literacy to help people understand and challenge AI decisions.

In the comment section, users express various opinions. Some point out that automated resume screening isn't new; it's just becoming more widespread with AI. Others worry about AI making decisions in sensitive areas like insurance claims or healthcare, suggesting that there should always be human oversight. Some commenters believe that AI could improve decision-making compared to human biases, while others argue that it is harder to hold algorithms accountable for mistakes. A few users mention the need for transparency and the ability to challenge AI decisions, similar to appealing a court judgment. Another perspective says opting out of AI is practically impossible since AI is embedded in almost every aspect of modern life, from online searches to email filtering. There is also a discussion about the potential societal impact of AI, with some expressing concern over privacy and the power of large corporations in controlling AI technologies.

---

## Absolute Zero Reasoner

- 原文链接: [Absolute Zero Reasoner](https://andrewzh112.github.io/absolute-zero-reasoner/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43922341)

The Absolute Zero Reasoner (AZR) is a new approach to training AI models without using human-curated data. Instead, the model creates its own tasks, solves them, and learns from the process. This is called the Absolute Zero Paradigm. The model acts in two roles: it proposes tasks and then tries to solve them. By doing this, it learns continuously without needing data from humans. The AZR focuses on three reasoning modes: deduction, abduction, and induction. It starts with simple tasks and gradually moves to more complex ones through self-play.

The AZR shows strong performance on code and math tasks, even outperforming some models trained with lots of human data. This is especially noticeable in larger models. The system can improve its skills in different domains, suggesting that it can generalize well. However, it sometimes creates errors in reasoning, which highlights the need for future improvements in safety.

In the comments, some users are skeptical. They feel the AZR relies on over-hyped marketing and isn't much different from existing methods. One user notes that the approach feels like adversarial training, but without new data. Another points out that starting with a pre-trained language model contradicts the "zero data" claim. Some find the concept interesting, as it could help in areas with less available data. Others worry that focusing only on code that compiles might lead to poor coding practices.

Overall, while the Absolute Zero Reasoner presents a novel method for AI training, it raises questions about its practical applications and the need for careful monitoring to ensure safe and useful outcomes.

---

## Embeddings are underrated

- 原文链接: [Embeddings are underrated](https://technicalwriting.dev/ml/embeddings/overview.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43963868)

The article discusses how embeddings, a machine learning technology, could greatly influence technical writing. Unlike text generation models like GPT, embeddings allow writers to find connections between texts on a large scale. Embeddings transform input text into arrays of numbers, enabling the comparison of different texts. This article explains how embeddings work, with an example using Google's Gemini to create embeddings. It highlights that embeddings are cost-effective and potentially environmentally friendly, though the latter is uncertain. The article also notes that different models produce varying sizes of embeddings, affecting their interchangeability.

In the Hacker News comments, the article's author, Kayce Basques, clarifies that he aims to introduce technical writers to embeddings, which are already well-known among machine learning experts. Some readers felt the article lacked clarity on what embeddings are and how they apply to technical writing. Others suggested providing more concrete examples to engage technical writers better. Commenters discussed the potential applications of embeddings, like improving search functions and categorizing text. Some users shared their experiences with embeddings in projects, noting the benefits of integrating them client-side. There were also technical discussions about the nature of embeddings compared to hash functions and the complexity of high-dimensional spaces where embeddings operate.

---

## A community-led fork of Organic Maps

- 原文链接: [A community-led fork of Organic Maps](https://www.comaps.app/news/2025-05-12/3/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=43961908)

The article talks about a new community-led project called CoMaps, which is a fork of the existing Organic Maps. This fork aims to be more transparent, community-driven, not-for-profit, completely open source, and privacy-focused. The project is rapidly progressing, with the community involved in decision-making and naming the project. They also invite people to join development, governance, and promotion efforts.

The current situation arose because of unresolved disagreements among Organic Maps' shareholders, Viktor and Roman, making the project's future uncertain. The community decided to fork the project because of concerns about governance and the potential commercial sale of Organic Maps without community input. The situation mirrors previous issues where open-source projects faced similar challenges due to governance models.

In the comments, some people express concerns about forking causing community fragmentation, while others argue it's necessary if governance is poor. Some believe that forking is challenging, citing examples like WordPress, where forking didn't happen despite governance issues. Others highlight technical differences, noting that applications like Organic Maps are easier to fork due to their nature. There are also discussions about the importance of financial transparency, the difficulties of maintaining open-source projects, and the impact on user experience. Some users prefer the offline functionality of Organic Maps and express concerns about search capabilities, while others are hopeful that CoMaps will address these issues. Overall, the community seems supportive of the fork, hoping it leads to a better, more community-focused project.

---

