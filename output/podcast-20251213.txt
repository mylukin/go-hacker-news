Hello everyone, this is the 2025-12-13 episode of Hacker News Daily Podcast. Today, we bring you stories about VPNs and their real server locations, lessons from Twilio Segment on switching from microservices back to a monolith, efforts to recover Anthony Bourdain’s lost li.sts, playing with Markov text generators, learning the Gleam language during Advent of Code, the early history of computer games, fake social media accounts in elections, a university’s $266 million software upgrade, best practices for writing release notes, and the art of building small HTML tools.

Let’s start with VPNs. A recent article explored whether VPN services really send your internet traffic through the countries they claim. The author studied 20 well-known VPN providers and found that most do not route your data through the promised countries—instead, they use “virtual” locations. Out of 20, 17 VPNs sent traffic out from a different country than listed. Many VPNs advertise more than 100 country options, but in reality, your traffic often passes through data centers in the US or Europe. After checking over 150,000 VPN exit IPs, the study found 38 countries that were only “virtual”—the VPN said your traffic was there, but it never actually was. Only three providers—Mullvad, IVPN, and Windscribe—matched their claimed locations with reality.

Why do VPNs do this? Sometimes for technical or legal reasons—it’s easier or safer to use data centers in stable countries than in risky places. But VPNs often do not tell users about these virtual locations. Old IP databases can also be wrong, trusting whatever the VPN says. The main advice: if you need your traffic to exit in a real location for privacy or legal reasons, choose a VPN known for honesty about server locations.

On Hacker News, commenters were not surprised. Running real servers worldwide is hard and costly, and some argued that virtual locations can be better for speed and reliability. Others worried about honesty—users might choose a location for safety and get in trouble if their data exits somewhere else. People shared tips for checking real exit points, and some praised providers like Mullvad and IVPN for transparency. There was a call for VPNs to clearly mark which locations are virtual, and some said regulations may be needed. Many agreed: always read privacy policies and test before trusting a VPN.

Next, a story from Twilio Segment about software architecture. They started with microservices, splitting their main product into many small parts so teams could work separately and problems could be isolated. But as the system grew, the number of services and code repositories became too much. The main product needed to send user events to over 100 partner APIs. At first, one queue handled all events, but slow partners could block everything. To fix this, they made one service and queue per partner. This helped at first, but soon there were more than 140 services to manage.

With so many services, the team spent too much time fixing tests, updating shared code, and scaling each service. Different services used different library versions, and the system became messy. The team decided to merge everything back into one big service—a monolith. Now, with a single codebase and standard library versions, productivity improved, and releases were faster. There were risks: a single bug could crash everything, and some things were less efficient, but overall the trade-offs were worth it.

On Hacker News, many agreed with the move. Microservices can add too much complexity for small teams. Some said microservices only work well if you pay for great tools and automation—something only big companies can afford. Others said the lesson is to build what fits your team, not just follow trends. There was also talk about how team size and company culture affect these choices. Most agreed: there is no perfect answer. Choose what works for your needs.

In another story, a writer tried to recover old, lost lists by Anthony Bourdain from a service called li.st. Using internet archives like Common Crawl, and a small Python script, they searched for and restored as much of Bourdain’s content as possible. Images from the original posts were gone and could not be recovered, but the text was kept as Bourdain wrote it. The lists covered many topics—favorite views, foods, books, and even closed bars in New York. The project became a kind of digital archaeology and was shared on GitHub in hopes others could help fill in the gaps.

Comments were full of praise for the project and for Bourdain’s writing. Some loved seeing his words again, while others discussed the technical challenges of recovering old web data, especially images. Suggestions included searching other archives or contacting former li.st staff. Some worried about copyright, while others were inspired to try recovering lost content from other sites. Many felt efforts like this are important to save early internet culture, and some gave tips for better web archiving in the future.

Next, we have a story about a programmer who used 24 years of his own blog posts—about 200,000 words—to train a simple Markov model and generate new, random-sounding text. He built a program called “Mark V. Shaney Junior,” based on a classic idea from the 1980s. The Markov model looks at groups of three words and predicts what comes next, making text that sometimes makes sense, but often mixes unrelated ideas. Using a higher order makes the text more natural, but can lead to copying whole parts of the original.

Hacker News users remembered the original Mark V. Shaney and shared their own stories of early text generators. Some were impressed by how well the simple code worked, others pointed out the limits—Markov models do not understand language, only patterns. People suggested trying bigger datasets or higher-order models, and joked about the strange sentences produced. There was debate about privacy and copyright if the model copies too much from the original. Overall, users liked the playful, experimental attitude and wanted to try it themselves.

Now to programming languages—one developer shared their experience using Gleam for Advent of Code, which this year had only 12 days. They found Gleam’s clean syntax, strong compiler, and helpful error messages made it enjoyable. The functional style, with pipelines and data transformations, fit well for the puzzles. Debugging was easy with the echo function, but the lack of string interpolation was missed. Built-in list functions, option types, and fold_until helped avoid bugs and made the code simpler.

There were some pain points: basic file I/O and regular expressions require extra packages, pattern matching on lists is limited, and big integers behave differently on Erlang and JavaScript. For some puzzles, the author had to call outside tools. Comments were mostly positive—people liked Gleam’s focus on safety and friendly syntax, though some worried about missing standard features for bigger projects. There was debate on whether functional programming is better for puzzles or production code. Many said Advent of Code is a great way to try new languages.

Let’s turn to games. An article looked at the early history of computer games, focusing on adventure games. In the late 1970s and early 1980s, people got games by copying from friends, typing them in from magazines, or buying them on cassette or disk. Most early games were text-based, like quizzes, simulations, and especially Star Trek games. As computers improved, games became more graphical.

The story of "Adventure"—the first big adventure game—stands out. Will Crowther made it using real caves and Dungeons & Dragons ideas, then Don Woods added more puzzles. Adventure spread quickly, inspiring others. Scott Adams sold "Adventureland" for home computers, and with his wife Alexis, made many more adventure games. "Zork," created by MIT students, was more complex and helped start Infocom. The first graphical adventure was "Mystery House" by Ken and Roberta Williams, who later made King’s Quest. The article notes that women like Alexis Adams and Roberta Williams played a big role in early gaming.

On Hacker News, readers shared memories of these games and the magic they felt. Some praised the open culture of sharing code, and others noted that the hard puzzles made finishing a game feel special. There was talk about learning to code by changing games, and how modern games may have lost some of the wonder. The gender gap in early games was discussed, with thanks for highlighting women’s contributions. Some wondered if new games today can inspire people the same way.

Moving to social media, another article explained how fake accounts are used to influence elections. Fake accounts on popular sites can be bought for a few dollars each, and bundles of hundreds or thousands are easy to get. Some are simple bots, others are made to look real, with photos and long histories. They can spread false news, argue with real users, or make ideas look more popular than they are. More complex fake accounts cost more—sometimes $15 or more. The article gave real examples from elections, and said that companies selling these accounts offer services to manage them. Social media platforms try to fight back, but fake accounts keep coming.

On Hacker News, people worried about how easy and cheap it is to buy fake accounts for politics. Some said platforms are not doing enough, while others pointed out that real people also spread false news. There was debate about whether fake accounts really change election results or just add noise. Some suggested more rules or better detection, others doubted if anything can truly stop the problem. Many agreed people should be careful about what they read and share.

Next up, Washington University in St. Louis spent $266 million on a new Workday software system, after student protests led them to share the details. The project replaced 80 old systems from the 1990s and took at least seven years. The costs included $81 million for finance and HR, $98.9 million for a new student app, and more for planning and support. The project moved almost everything into Workday, a cloud-based service. Another school, University of Washington, spent $340 million for a similar project, which also had problems.

Hacker News commenters were shocked by the cost. Some said big software projects at schools almost always go over budget and take too long. Others wondered if it was better than fixing old systems or building something simpler. Some shared stories of similar expensive projects at their own schools. Others defended the upgrade, saying old systems really were falling apart. Many felt universities should use money more carefully since students and families end up paying.

A user on Hacker News asked how to write release notes for different groups—developers, business users, and end users. Answers included writing one set of notes with clear sections, or making different versions for each group. Some use tools to filter notes by role. Many said notes should be short and easy to understand—focus on what changed, what’s new, and any important fixes. Some teams automate the process or send updates by email or in-app messages.

Commenters said it’s hard to keep everyone happy, and some people just ignore release notes. Still, many think they’re important for trust and transparency. Some teams do not have a perfect system and keep trying new ideas. The main advice: use clear, simple language, and ask users what kind of updates they want.

Finally, an article shared lessons from building small HTML tools—simple web apps made from just one HTML file, with inline JavaScript and CSS. The author has built over 150 such tools, often with help from LLMs like ChatGPT. The idea is to avoid frameworks and build steps, making tools easy to copy, share, and host—like on GitHub Pages. Many tools use copy-paste for input and output, and store state in the URL or localStorage. Some fetch data from APIs, or even run Python or C code in the browser using Pyodide or WebAssembly.

Readers loved the simplicity and the “no build step” approach. Some felt nostalgic for the early web, and many praised how easy it is for beginners. There were suggestions to be careful with API keys in localStorage, and debate about using CDNs. The remixing idea—copying old tools to make new ones—was popular. Many felt inspired to try building their own HTML tools and appreciated the practical advice.

That’s all for today’s episode of Hacker News Daily Podcast. Thank you for listening. We hope these stories spark new ideas and help you stay curious in the world of technology. See you next time.