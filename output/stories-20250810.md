# Hacker News 故事摘要 - 2025-08-10

## 今日概述

Today’s top Hacker News stories talk about fast changes in technology, new AI models, and debates about privacy laws. There are also stories about language, web design, and fun tech projects. Many comments compare today’s rapid tech growth to the past and discuss both the good and bad effects. If you like stories about how tech affects daily life, new tools for developers, or language history, today’s stories have something for you.

---

## 1910: The year the modern world lost its mind

- 原文链接: [1910: The year the modern world lost its mind](https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44858154)

This article looks at the year 1910, a time when new technology changed life very quickly and made many people feel anxious and lost. The author explains how inventions like the car, airplane, and bicycle shook up daily life, making cities noisier and faster, and people unsure about the future.

If you lived in 1875 and woke up in 1905, you would see electric lights, skyscrapers, cars, and even cameras and music players—things that did not exist before. The number of cars grew fast; France went from 3,000 cars in 1900 to 100,000 by 1914, and Ford made over 300,000 Model T cars in just one year. This new speed made some people excited, but others worried it was not normal or healthy. Critics even said "automobilism" was an illness, and doctors warned about strange new health problems from riding bicycles.

Women on bicycles were especially shocking to some people, because they broke old rules about how women should act and dress. Novels and news stories joked or warned that these new machines turned people into "machines" themselves, moving too fast for nature.

All this change led to new kinds of stress. Many white-collar workers, like office staff, teachers, and engineers, started to suffer from "nervous exhaustion" or "neurasthenia." Hospitals saw a huge rise in mental health cases. Most of these people were not farm workers, but city people with "brain work" jobs, often feeling overworked and tired.

Artists also felt the change. Composer Stravinsky shocked crowds with his wild music, and painters like Kandinsky and Picasso made strange, new art that did not look like real life. Cameras made it easy to copy real things, so painters started to paint feelings and ideas instead. At first, critics hated this art, calling it a sign of sickness or city vice.

The article also talks about two famous thinkers, Weber and Freud. Weber said modern work habits came from Protestant religion, helping people save money and work hard. Freud saw that modern life forced people to hide their real feelings, which could make them sick or anxious. Both men saw that new technology and ways of life could be both good and bad for people.

In the Hacker News comments, some people agree that today feels a lot like 1910, with technology moving too fast and making life stressful. Others say every generation feels this way with big changes, so maybe it is normal. Some readers like how the article connects art history with tech history, while a few argue that life is actually better now, because we have more comfort and safety. One commenter notes that worries about "speed" in 1910 sound like today's worries about smartphones and social media. Another points out that, just as in 1910, new tech can make some jobs disappear and create new ones. A few people say the article is too dramatic—change can be hard, but people always adapt. Others like the reminder that mental health is not a new problem, but has been around for a long time. Some share stories of their own family members feeling lost during big changes in history. Overall, the comments show both worry and hope: change is scary, but it can also lead to new art, ideas, and ways of living.

---

## GPT-OSS vs. Qwen3 and a detailed look how things evolved since GPT-2

- 原文链接: [GPT-OSS vs. Qwen3 and a detailed look how things evolved since GPT-2](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44855690)

OpenAI just released two new large language models with open weights: gpt-oss-20b and gpt-oss-120b. This is the first time OpenAI has shared large, open-weight models since GPT-2 in 2019, and these new models can run locally with some hardware and clever optimizations.

The article explains how LLMs have changed since GPT-2. Both gpt-oss and GPT-2 use the transformer architecture, but many small parts have improved. Dropout, once common to stop overfitting, is now rarely used because LLMs see huge datasets only once during training. GPT-2 used absolute positional embeddings, but now most models use RoPE (rotary position embeddings) for better learning of word order. Activation functions changed from GELU to Swish, as Swish is a bit faster without hurting accuracy.

Modern models use gated feedforward layers (like SwiGLU) instead of simple ones; this gives better performance with fewer parameters. GPT-oss uses a “mixture-of-experts” (MoE) setup, with many expert layers but only a few active at a time, making the model both bigger and more efficient. Grouped Query Attention (GQA) is another trick: it lets multiple attention heads share keys and values, reducing memory use. Sliding-window attention is used in every second layer, so the model only looks at a small part of the context sometimes, saving even more resources.

For normalization, RMSNorm is now used instead of LayerNorm. RMSNorm is simpler and cheaper to compute but works just as well. The article shows that starting with GPT-2 is still a good way to learn about LLMs, because it is simple and clear.

When comparing gpt-oss to Qwen3 (another top open-weight model), there are differences. Qwen3 is deeper (more layers), while gpt-oss is wider (bigger layers). Wider models are usually faster at inference, while deeper ones can be harder to train but may learn different things. The MoE setup also differs: gpt-oss has fewer but larger experts; Qwen3 has more, smaller experts. GPT-oss uses sliding window attention, but Qwen3 does not, and gpt-oss adds bias units to attention weights, a detail not seen since GPT-2.

Another new thing is “attention sinks”: learned bias units that help the model remember important tokens in long contexts. Both models have an open license, but only the weights and inference code are open, not the training data or code.

GPT-oss was trained mostly on English text, with a focus on STEM, code, and general knowledge, and it uses both supervised and reinforcement learning. One special feature is that users can control reasoning effort (low, medium, high) in prompts, which changes answer length and depth. This helps save resources for simple tasks and only uses more compute when needed.

For hardware, OpenAI used a new quantization scheme (MXFP4) so the models can run on fewer GPUs. The 20B model fits on newer consumer GPUs, and the 120B model can run on a single H100 or MI300X. Without quantization, much more memory is needed.

Some early tests show gpt-oss is powerful, sometimes matching OpenAI’s best closed models in benchmarks. But it can “hallucinate” (make things up) more than other models, maybe because it was trained more for reasoning than memorizing facts. As tool use (like using search engines) gets better, this may not matter as much.

Now, let’s look at what Hacker News readers thought. Many are happy to see OpenAI release open-weight models again, and they like that these models can run locally. Some say this is a big step for privacy and for people who want control over their AI tools. Others are more cautious: they point out that “open-weight” is not the same as “open-source,” since the training code and data are not public.

A few comment that gpt-oss still needs powerful hardware, so it’s not really for everyone yet. Some are interested in the quantization tricks, saying that making big models run on a single GPU is very useful. Several readers note that the architecture changes (like RMSNorm, MoE, GQA) are now standard and not unique to OpenAI, but it’s still good to see OpenAI adopting them.

Some users compare gpt-oss to Qwen3 and other models, saying the choice between “deeper” and “wider” models is interesting, and they are curious how this will affect future models. There is debate about the hallucination problem: some think it’s a big issue, others say it can be fixed by better training or tool integration.

People also mention that open-weight models are great for research, but wish OpenAI also released the base models and training code like other projects do. Several are excited about the “reasoning effort” feature, saying it’s a clever way to balance cost and quality, especially for businesses.

A few are skeptical, saying benchmarks don’t always show how good a model is in real life, and more time is needed to test these models. Some see this as good competition for Meta, Google, and Alibaba, hoping it will push the whole field forward. Finally, many agree that having more strong models with open weights is good for everyone—developers, researchers, and users who care about privacy and control.

---

## Try and

- 原文链接: [Try and](https://ygdp.yale.edu/phenomena/try-and)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44855079)

The article talks about the phrase “try and” in English, like in “try and eat the salad,” and how it is used in North America and other English dialects. It explains that while “try to” is seen as more correct, “try and” is common and has a long history, going back to at least the 1500s.

The article gives examples of how “try” can be used with nouns (“try the salad”), with “to” plus a verb (“try to eat the salad”), or with “-ing” (“try adding vinegar”). But with “try and,” you get sentences like “try and eat the salad,” which means almost the same as “try to eat the salad,” though some grammar books say it is wrong. The phrase “try and” is more common in British English, but it is also used in American and Canadian English. Some studies even say “try and” might be older than “try to,” but others think they developed at the same time.

The article goes into grammar details. For example, “try and” does not act like normal coordination (“and”) in sentences. You cannot switch the order of the verbs (“kill mosquitoes and try” sounds wrong), and you cannot use “both” (“both try and kill mosquitoes” is not allowed). Also, both verbs must be in the basic form: “I will try and finish the assignment” is okay, but “I tried and finished” is not. However, in some dialects, like in Northeastern Canada or South Africa, different forms like “tries and does” or “tries and find” can be used. You cannot separate “try” and “and” with words like “always” or “not.” For example, “try always and tell the truth” is not correct. Also, you cannot leave out the second verb (“I’ll try and” is not okay).

Other verbs can use “and” this way too, like “be sure and,” “mind and,” or “remember and.” There’s also “come and” or “go and,” as in “come and pick me up,” but these have slightly different grammar rules.

People in the comments noticed how often “try and” is used in speech, even though it’s not usually taught in school as correct. Some said it sounds more casual and friendly, while “try to” feels more formal. Others pointed out that “try and” is common in British TV and books, but not as much in American writing. A few commenters were surprised to learn “try and” is so old, as they thought it was a new or lazy way to speak. Some developers joked about how language rules change, like code style guides do. One person mentioned that language learners sometimes get confused by “try and,” since it does not always follow standard grammar rules. Others shared examples from their own dialects, saying they use “try and” every day without thinking. Some grammar fans debated if “try and” really means the same as “try to,” or if it suggests a slight difference in meaning or intention. A few teachers said they correct students for using “try and,” but now wonder if they should stop. Lastly, some commenters liked seeing real examples and history, saying it helps them understand why English is sometimes so strange.

---

## One Million Screenshots

- 原文链接: [One Million Screenshots](https://onemillionscreenshots.com/?q=random)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44858067)

The article talks about "One Million Screenshots," a website that shows screenshots of many different homepages from across the web. The site lets you zoom in and out, browse random websites, and see what popular websites look like.

The main idea is to give people a way to explore and compare website designs. On the site, you can search for a specific website, or just look at random ones to get ideas. There is a map-like interface, made using the Leaflet JavaScript library, that lets you move around and zoom in on the screenshots—similar to how you might use Google Maps but for website homepages. The site uses a tool called Urlbox to take and manage these screenshots. There are links for more information, such as an FAQ, a way to suggest new sites, and documentation for a Screenshot API if you want to get screenshots for your own projects.

Many people might use this for inspiration, research, or just out of curiosity. The screenshots are all of homepages only, not subpages, and the goal seems to be to show what is popular or interesting online.

In the Hacker News comments, some people are impressed by how fun and simple the idea is. They like that you can quickly see how web design has changed over time and across different sites. Some users mention that the zooming and browsing experience feels smooth and engaging. A few people ask how the screenshots are kept up to date, since websites change often. Others wonder about copyright issues with showing so many website images at once. Some users see it as a useful tool for web designers looking for ideas. Others point out that it can be a fun way to discover new sites, or to see how famous websites looked in the past compared to now. A few commenters suggest features they would like, such as sorting by region or type of site. Some worry about the privacy of smaller sites, but others think that homepages are already public. Overall, the project is seen as a simple but clever way to explore the web visually.

---

## Fight Chat Control

- 原文链接: [Fight Chat Control](https://fightchatcontrol.eu/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44856426)

The article talks about a new EU law called “Chat Control,” which would make all private messages, photos, and files sent online in Europe get scanned by software. The goal is to find illegal content, but the plan includes scanning even encrypted chats, which worries many people.

The law would affect everyone in the EU. Every message, photo, or file you send—even if you use end-to-end encryption—would be checked. This would happen with no need for suspicion or a court order. Supporters say this will help find child abuse material, but the article argues it is mass surveillance that weakens everyone’s privacy and security. If encryption is broken or weakened, not just police but anyone—including hackers—could access private information like bank details or health records. The article says this goes against basic privacy rights in the EU’s Charter. It also points out that automatic scanning tools often make mistakes, misunderstanding innocent content as illegal. This can lead to innocent people being accused or investigated by mistake.

Child safety groups and the United Nations have warned that such wide surveillance does not stop abuse and might make things worse, because it makes everyone’s data less secure. Another worry is that, if the EU makes this law, other countries with fewer freedoms could copy it to watch their own people. The article notes that politicians want to make themselves exempt from this scanning, but not regular people.

Right now, 15 EU countries support this law, 3 are against, and 9 are undecided. Countries like Austria, the Netherlands, and Poland oppose it, saying it breaks privacy rules. Countries like France, Denmark, and Italy support it. Germany is undecided, and its decision could decide the law’s future.

Many Hacker News commenters strongly oppose Chat Control. They say scanning everyone’s messages is like treating all citizens as suspects. Some are shocked that encrypted messages would be scanned, calling it a huge risk for security. Others point out that mass surveillance rarely stops real criminals, who can always find other ways to hide. A few say that once this kind of law exists, governments may use it for more than just child protection—like political control. Some think this could set a bad example for other countries, leading to less freedom everywhere. A few commenters wonder if there is any way to stop the law, or if people will have to move to other chat apps or even other countries. Others argue that protecting children is important, but believe there are better, more focused ways to do it without watching everyone. There are also voices worried about false positives and the harm to people wrongly accused. Some are surprised that politicians would exclude themselves from the rules, calling it unfair. A few commenters ask if the law could even be enforced on open-source or peer-to-peer platforms. Overall, most feel this law is a big step backwards for privacy and digital freedom in Europe.

---

## Show HN: Bolt – A super-fast, statically-typed scripting language written in C

- 原文链接: [Show HN: Bolt – A super-fast, statically-typed scripting language written in C](https://github.com/Beariish/bolt)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44856935)

Bolt is a new scripting language made for speed and safety, written in C and meant to be easy to add into other programs. The project is small, open source, and aims to help with real-time applications where performance matters.

Bolt is statically typed, which means it checks types before running your code. This helps catch errors early. It is also “embed-first,” so it is simple for C programs to include Bolt and run scripts. The language is lightweight and quick to compile, able to process thousands of lines in just a second. Benchmark results show Bolt is faster than many other scripting languages in the same class. The code base is small and tries to avoid extra features to keep things simple. Bolt uses only the standard C library and can be built as a static library for x64 systems. There is an example in the docs showing how to add Bolt to a C project with just a few lines of code. The language has a rich type system and can handle errors clearly. There are guides, examples, and benchmarks to help people learn Bolt or check its speed. At the moment, Bolt is not stable, so users may find bugs, but the author welcomes reports and fixes.

In the Hacker News comments, some people are excited to see a new fast scripting language that is easy to embed. They like that Bolt is statically typed and written in C, as this could make it good for games, tools, or embedded devices. Others point out there are already many scripting languages, like Lua, Wren, or Zig, and ask what makes Bolt different. Some worry about long-term support and if the project will be maintained, since small languages sometimes fade away. A few users ask about language features, such as memory safety, threading, and how Bolt compares in speed to Lua or Python. There are questions about how hard it is to add new features or if Bolt will stay “minimal.” Some like the clear error handling and the focus on catching mistakes before running code. Others are waiting to see more real-world examples before trying it. A few suggest that, even if it is not yet stable, Bolt could be a good learning tool for people who want to see how interpreters work. Overall, there is interest, but people want to see how Bolt grows and if it finds a place among other scripting languages.

---

## Show HN: Engineering.fyi – Search across tech engineering blogs in one place

- 原文链接: [Show HN: Engineering.fyi – Search across tech engineering blogs in one place](https://engineering.fyi/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44855157)

Engineering.fyi is a new website that lets you search and read engineering blog posts from many top tech companies all in one place. The site gathers articles about software, AI, cloud, databases, and other tech topics, so users can keep up with the latest trends and tools across the industry.

The main idea is to save time for engineers and tech fans by having one spot to find news, tutorials, and updates from companies like Airbnb, Google, Meta, Shopify, Microsoft, and others. The homepage lists recent articles, showing the title, author, reading time, and if code is included. Topics cover things like Kubernetes upgrades, new AI models, code review tools, and open-source libraries. There are posts for all skill levels, from beginner to advanced, and many include example code or guides. For example, you can learn how Airbnb manages large-scale upgrades, or how Google’s new AI models help with video and text processing. There are also updates about open-source projects, podcasts, and new features in tools like Firebase and PyTorch. Some posts talk about AI helping with code reviews or building apps without writing code, making tech more accessible. The site includes both deep technical articles and short updates, so users can pick what interests them.

In the Hacker News comments, some users liked the simple idea and found it helpful to have many engineering blogs in one place. A few said it’s similar to older blog aggregators but focused more clearly on tech companies. Some people wanted more filtering options, like by company, topic, or skill level, and asked about adding RSS feeds for their own reading tools. Others wondered how often the site updates or how it chooses which blogs to include. A few users shared that they already follow these blogs directly, but liked the extra convenience. There were some questions about copyright, since the site links to posts but shows summaries. One commenter worried about missing out on smaller, less-known engineering blogs that might not be included yet. Others suggested features like newsletters, dark mode, or search by tags. Overall, most agreed this tool is useful for busy developers who want an easy way to stay up to date with the tech world.

---

## Diffusion language models are super data learners

- 原文链接: [Diffusion language models are super data learners](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44856101)

A new research article talks about “diffusion language models” and how they can learn from very small amounts of data. The main idea is that these models work differently from standard language models like GPT.

Standard language models use transformers, which read lots of text and predict the next word. Diffusion language models, instead, use a process similar to how images are generated in AI art—starting from noise and slowly making sense out of it. This approach lets the model learn patterns from just a few examples, which is called “super data learning.” The article explains that, in tests, these models learned new tasks with much less data than older models needed. They also performed better in low-data situations, like when learning a new language or a rare word. The authors give technical details about the training process and compare results with big transformer models. They show examples where the diffusion model guesses missing words or sentences, and it does this very well even with little training. The researchers think this could help build smart AI for languages or topics where there is not much text online.

In the comments, some people are excited and think this is a big step forward, especially for small companies or projects with little data. Others ask if these models are slower than normal transformers, since diffusion models often take more steps to make a result. A few users wonder if this will help with fairness and bias, because models could be trained on more balanced or local data. Some say this kind of model might be good for edge devices, where you can’t store big datasets. But a few people are not convinced yet—they want to see real-world tests, not just research papers. One commenter asks if this idea could be used for code or music, not just text. Another points out that new models often look good in labs, but don’t always work well in practice. Overall, commenters agree the idea is interesting and want to see more open-source projects using it.

---

## Creating the Longest Possible Ski Jump in "The Games: Winter Challenge"

- 原文链接: [Creating the Longest Possible Ski Jump in "The Games: Winter Challenge"](https://mrwint.github.io/winter/writeup/writeup2.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44825347)

This article explains how someone tried to make the longest ski jump in the old PC game "The Games: Winter Challenge." The author wanted to see how far a ski jump could really go and started by removing the game's copy protection, which secretly limited jump distance. Without these limits, longer jumps were possible.

Instead of just playing the game over and over, the author took a technical route. They opened the game’s code and learned how the ski jump mechanics and physics worked. The game records every jump in replay files, which have two main parts: the starting state and the player’s input for each frame. By changing these files, the author could create new, perfect jumps.

The article goes deep into how the game’s physics work. The jump has four parts: going down the ramp, taking off, flying, and landing. The game uses simple 3D physics, with fixed-point math instead of floating-point, because of old hardware limits. The author rebuilt the whole jump simulation in Rust to test and optimize jumps outside the game.

To find the best jump, the author wrote a program to try many input combinations, but the search space was huge. To make this faster, they focused only on the best strategies: staying in the center for less drag, perfect ski angles during flight, jumping at just the right time, and landing as late as possible. The author also found a trick called the “wiggle technique,” where moving left and right quickly down the ramp adds speed, similar to "strafe running" in old shooter games.

A key insight was that sometimes slowing down just before landing could mean staying in the air one more frame, adding extra distance. The author found the best result by combining all these tricks. The final jump reached 113.8 meters, close to the theoretical max.

In the comments, many readers praised the deep technical analysis and the creative problem-solving. Some enjoyed the nostalgia and were surprised by the complexity of old games. A few pointed out that brute-forcing every option would take too long, so using game knowledge to cut down choices was smart. Others discussed similar tricks in other games, like “strafe running” in Quake, or the famous backwards jump in Mario.

Some readers wondered if there might still be undiscovered bugs or glitches that could give even longer jumps. A few asked about the tools used for reverse engineering or how fixed-point math works. Others appreciated that the author shared their code and replays, inviting the community to try breaking the record. A couple of people shared stories of trying to beat their own childhood game records with modern tools.

Overall, the discussion mixed admiration, technical talk, and memories of playing classic games. Many liked that the project was more about understanding and enjoying the game’s design than just chasing a high score.

---

## Reflections on Soviet Amateur Photography

- 原文链接: [Reflections on Soviet Amateur Photography](https://www.publicbooks.org/strangers-in-the-family-album-reflections-on-soviet-amateur-photography/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=44832060)

This article looks at how photography was used by ordinary people in the Soviet Union, and how family photo albums can show both personal and political history. It reviews a book called "In Visible Presence," which studies Soviet family photos and the stories around them.

The Soviet government encouraged people to use cameras, but wanted photos to have "social significance"—not just pictures of family and friends. However, even simple family photos are important because they show who people are and what they value. The book’s authors, Sarkisova and Shevchenko, found that Soviet family albums often included strangers, not just family. For example, group portraits at places like Red Square would have many people who didn’t know each other, but each person got a copy and kept it as a memory.

The research was done by talking to families in different Russian cities and looking at over 50 photo collections. These albums told stories not just about home life, but about the bigger Soviet community. The photos helped people remember the Soviet past, but also showed things that were hidden or not talked about—like fear, silence, or things people forgot on purpose.

The book is praised for showing how photos have different meanings depending on who looks at them and when. It also talks about how the meaning of photos can change over time, and how they can be used by the state for political reasons. The authors show that silence and forgetting are important themes—sometimes people chose not to talk about certain photos, or didn’t even realize what was missing.

The book is special because it lets readers follow their own path through the stories and photos, and it even includes features like a ribbon marker to help you come back to it. The authors kept working on the book for many years, watching how politics in Russia changed and how that affected what people remembered or wanted to forget.

Top Hacker News comments had many different views. Some people were surprised that Soviet family albums included strangers and saw this as a unique part of Soviet culture. Others compared Soviet photo albums with those in their own countries, noting similarities and differences. A few developers talked about how digital photos today make it easier to share and store images, but also easier to lose old memories. Some readers found the idea of silence and forgetting in family albums very moving, while others thought it was common in many cultures, not just the Soviet one.

There was discussion about how governments use personal photos for propaganda and how people resist by keeping their own private history. A few comments focused on the design of the book, saying that the ribbon marker and inclusion of many photos made it feel personal and valuable. Some users wondered if future historians will study our digital photo collections in the same way, and if we are losing something by taking so many photos that we never print or look at again.

Overall, the comments showed respect for the research and for the idea that even simple family photos can have deep meaning—both for individual families and for understanding history.

---

