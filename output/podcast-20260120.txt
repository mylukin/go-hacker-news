Hello everyone, this is the 2026-01-20 episode of Hacker News Daily Podcast. Today, we have a rich mix of stories, from hidden monuments and deep time, to the latest in AI safety, database tricks, and peer-to-peer networking. Let’s get started.

Our first story takes us to the Hoover Dam, where a little-known monument hides in plain sight. While most visitors see the large bronze statues and flagpole at Monument Plaza, few notice the terrazzo floor beneath their feet. This floor is actually a giant star map, designed in the 1930s by artist Oskar Hansen. It marks a special point in Earth’s 26,000-year axial precession, the slow wobble of our planet’s axis. The map shows the positions of stars like Polaris, Thuban, and Vega at the time the dam was finished, and even points to where these stars will be in the far future.

The article’s author wanted to understand the monument, but found little information. With a historian’s help, he discovered old plans showing that the flagpole marks the center of the Earth’s wobble. The floor’s star and planet positions act as a time stamp for future generations, similar to the 10,000 Year Clock project.

Hacker News users were surprised and impressed. Some had visited Hoover Dam but never noticed the star map. Others discussed the science of axial precession and how hard it is to imagine such long cycles. A few wished the monument’s meaning was better explained on tours, while others worried that people in the future might not understand it. Many praised the combination of art, science, and engineering, calling it inspiring and rare.

Next, let’s talk about AI safety. One article explored how to run Claude Code with a special flag called “--dangerously-skip-permissions.” This flag lets the AI install packages, change configs, and even delete files without asking. While this makes things fast, it is also risky. The author did not want to put their own computer in danger, so they looked for ways to isolate the AI.

After trying Docker and other options, the writer settled on Vagrant, which creates a full virtual machine. It’s easy to set up, and you can destroy and rebuild the VM if something breaks. Files are kept safe using git, and the system runs fast enough for normal work.

The comments were full of advice and caution. Many agreed that running unknown AI code needs strong isolation. Some shared stories of losing files by running risky tools on their main system. Most thought Vagrant was a good choice, even if it’s an older tool. Some suggested extra safety tricks, like one-way file syncs or snapshots. A few worried about rare VM escape bugs, but most said this setup is safe for most people. Overall, readers found the author’s method practical and simple, and said isolation will be even more important as AI gets more powerful.

Our next story covers less common ways to make PostgreSQL faster. The article explains tricks beyond just adding indexes or changing queries. For example, turning on “constraint_exclusion” lets PostgreSQL skip full table scans when queries cannot return results, which is helpful for reporting tools. The author also suggests making function-based indexes, like indexing just the date part of a timestamp if you do not need the time. This saves space and speeds up queries, but only works if everyone uses the same function in their queries. PostgreSQL 18’s new virtual generated columns make this even easier.

Another trick is using hash indexes for very large text fields. Hash indexes are smaller because they store only a hash, but they cannot be used as unique indexes directly. Instead, you can use exclusion constraints, though there are some limits, such as not working well with foreign keys.

In the comments, readers liked these creative tips. Some warned that you need to understand PostgreSQL well before trying them, and that keeping everyone on the same query pattern is hard in big teams. There were also discussions about hash collisions and the limits of exclusion constraints. Many agreed constraint_exclusion helps in big data warehouses, and some were excited to try out features they had not heard of before.

Moving on, we look at wxpath, a new Python tool for web crawling. Instead of writing lots of code, you can use XPath expressions to describe your crawl. wxpath lets you move from page to page, pick out data, and run crawls concurrently. It can save results in JSON or a local database, and supports both async and blocking modes. It tries to be polite by using custom headers and following robots.txt. You can use it as a Python library or from the command line, and there are options for depth, concurrency, and more. The tool is still early, so things like proxy rotation and JavaScript support are missing.

Hacker News users liked how wxpath makes crawling easy, often in just one line. Some compared it to Scrapy, saying wxpath is simpler but less flexible. Others warned that web pages change, so XPath patterns can break. There was talk about crawl “explosions” if you do not set a max depth, and some requests for more tutorials and browser support. Overall, people liked how wxpath lowers the barrier to web crawling, but said to use it with care.

Now to the world of finance. An article examined the chance that Nvidia’s stock price will fall below $100 in 2026, even though it is now around $184. The author used math, market data, and code to make an estimate. Instead of just looking at past prices, the author used options trading data to get a measure called “implied volatility.” Using a binomial model, he simulated possible price paths and found a 24% chance, but then adjusted for market risk, ending up with about a 10% chance. He noted this is much higher than zero, which many might guess.

In the comments, some agreed with using options prices to measure risk, while others pointed out that the options market can be thin for such far-off prices. There was debate over the 10% number, with some thinking it is too high, others saying it’s about right given how fast markets can change. Readers liked the detailed code samples and discussed the limits of the binomial model. Some asked about dividends and stock splits. There was also a lively debate about whether prediction markets are better than expert guesses. In the end, everyone agreed that forecasting big stock moves is always uncertain.

Next, we have a tool for book lovers and language learners. Fast Concordance is a new online tool that lets you search for any word or phrase in more than 1,200 public domain books from the Standard Ebooks project. A concordance shows every place a word appears, along with a few words before and after, to help you understand its use. This tool is very fast, using special indexes to jump right to the results, and works in less than a second. It is simple to use—just type a word or phrase and see the results.

The comments were positive. People found it useful for research or writing, and some said it was faster and easier than other concordance tools. There were requests for more features, like word pattern searches or regular expressions. Some asked about the technical details, and the creator explained more about the indexing method. Others wondered if the tool could be expanded to more books, and some saw it as helpful for language learners. Overall, users liked the tool and its speed.

Now, let’s look at PCIem, a Linux framework for making and testing virtual PCIe devices in userspace. PCIem lets developers build fake PCIe cards that look real to the Linux kernel. You can test and debug drivers without real hardware, saving time and money. It supports key PCIe features like BAR mapping, interrupts, and DMA, even peer-to-peer DMA. One example is a virtual graphics card made with PCIem and QEMU that can run games like DOOM by software rendering frames and sending them to the host.

PCIem’s license is dual MIT/GPLv2, giving flexibility for different projects. There is detailed documentation and a blog post for those who want to try it.

In the comments, users were excited about how PCIem could make driver development and testing easier and cheaper. Some compared it to QEMU’s own PCIe device emulation or vfio-pci, and asked about performance. Others saw uses in security research or teaching. A few noted that PCIem still needs some technical skill to use, but overall, the community was impressed by how it opens up PCIe driver work, especially for smaller teams and open source projects.

Our last story is about Reticulum, a network protocol with no servers or “cloud.” Reticulum is fully peer-to-peer, with no center and no authority. Instead of IP addresses, it uses cryptographic identity hashes. Security is built in, with all links encrypted, and messages can travel over any medium—even very slow links like 5 bits per second radio.

Reticulum is designed to be owned by its users, with no need to rent from big companies. The protocol is public domain, but the official code is licensed to forbid harmful use. The goal is to keep the network for good, not for profit or control. There is no DNS or central naming; names are just hashes. Trust is built by sharing keys directly between people.

The comments showed strong interest in real peer-to-peer networking and privacy. Some liked the focus on working even during internet outages or censorship. Others were curious about technical details, like setup and performance. There were worries about the “no harm” license and about real-world adoption, since decentralization is always hard. Some compared Reticulum to Tor, Briar, or IPFS. Most agreed that while the project may not go mainstream, its ideas could help push the internet toward more openness and resilience.

That’s all for today’s episode. We hope you enjoyed these stories and the thoughtful discussions from the Hacker News community. Thanks for listening, and we’ll see you next time.