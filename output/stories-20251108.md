# Hacker News 故事摘要 - 2025-11-08

## 今日概述

Today’s top Hacker News stories cover new tech tools, security, and AI. There is a fast web framework called Marko, research showing AI tests are often weak, and a simple writing system for old laptops. Other stories talk about a font made by averaging many fonts, Cloudflare dealing with fake top domains from a botnet, and a book about how programming languages control code. There are also discussions on old programming ideas, compiler lessons, human touch research, and a near-miss between planes. Topics are mix of coding, AI, design, and safety.

---

## Marko – A declarative, HTML‑based language that makes building web apps fun

- 原文链接: [Marko – A declarative, HTML‑based language that makes building web apps fun](https://markojs.com/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45858905)

Marko is a web framework that lets you build web apps using a language based on HTML. It is made to be easy for people who already know HTML, CSS, and JavaScript.

Marko extends normal HTML to add features for making dynamic and reactive websites. You can use simple HTML with some extra code to handle things like buttons, counters, and updates without much extra work. Marko focuses on being fast: it streams content so users see pages quickly, even before all JavaScript is loaded. It only sends the code needed for each part of the page, which helps your app load faster and use less data. Marko’s syntax is simple and close to HTML, so you do not need to learn a lot of new things. You can use Marko for small templates or big, complex web apps. The framework is used by large sites like eBay, so it is proven at scale. Marko supports features like file-based routing, TypeScript, and has many built-in tags to make development easier. The documentation and tutorials help you get started quickly, and there is a playground to try things out online. Marko is open source and part of the OpenJS Foundation.

In the Hacker News comments, some people like that Marko uses HTML as the base, making it simple for new developers. Others say the streaming and fast loading are very good for modern web apps. Some users mention that Marko is less popular than frameworks like React or Vue, so finding help or new team members can be harder. A few users who have used Marko in big projects say it is reliable and scales well. Others wonder if Marko’s approach is different enough to stand out in a crowded field. There is discussion about how Marko’s fine-grained bundling helps performance, with some developers wishing other frameworks did this too. Some people mention that the syntax is easier than JSX, while a few think learning yet another way to write components is confusing. Users also discuss the community: some find it friendly and helpful, but others say the ecosystem is still small. A few wish Marko had more plugins and third-party tools. Some point out that Marko’s tight HTML integration makes it a good fit for teams who already know web basics. Overall, people seem interested but divided on if they would switch from their current tools.

---

## Study identifies weaknesses in how AI systems are evaluated

- 原文链接: [Study identifies weaknesses in how AI systems are evaluated](https://www.oii.ox.ac.uk/news-events/study-identifies-weaknesses-in-how-ai-systems-are-evaluated/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45856804)

A new study from Oxford and other top universities shows that many tests for large language models (LLMs) are not very strong or clear. The researchers looked at 445 different AI benchmarks used to check how good and safe AI systems are.

The study found that only a small number of these benchmarks use proper statistical methods, so some results could be just luck. Many benchmarks test things like “reasoning” or “harmlessness,” but do not explain what those words mean. This can make it hard to know if the AI is really getting better or just getting lucky with the test. For example, some tests ask for answers in a special format, so if an AI gets the right answer but writes it the wrong way, it looks worse than it is. Another problem is that AIs can do well on simple math or medical questions, but fail if the question is changed a little, showing they may not really understand. Also, scoring high on a test, like a medical exam, does not mean the AI can do a real doctor’s job.

The researchers say these problems can be fixed. They suggest that benchmarks should have clear definitions and test real skills. Tests should use good statistics to show if results are real or just chance. They also made a checklist to help people build better AI tests. This checklist is free for anyone to use.

In the Hacker News comments, some people agree that AI tests are too easy or do not match real-world needs. Others point out that companies want good scores for marketing, so they may pick easy tests. Some worry that regulators may use weak tests to make rules about AI, which could be risky. A few readers remember times in other fields, like medicine or education, when unclear tests caused problems. Some think we need more open, shared tests, not private ones made by big companies. Others ask who should decide what a “good” test is, and if it is even possible when ideas like “reasoning” are hard to measure. Many agree that better methods and more honesty are needed, but they also note that no test will ever be perfect for all situations. Some suggest learning from old fields like psychology, where test design is very careful and slow. Overall, readers feel this study is important and hope it leads to stronger, clearer ways to test AI.

---

## WriterdeckOS

- 原文链接: [WriterdeckOS](https://writerdeckos.com)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45858945)

WriterdeckOS is a special operating system that turns old laptops or Chromebooks into simple writing devices. The goal is to help people write without any distractions—no internet, no apps, no games, and no notifications.

WriterdeckOS is based on Debian Linux and uses a basic text editor called Tilde. When you start the laptop, it goes straight to this editor. There is no desktop, no browser, and just the tools you need to write. The system is very light, so it works well on old or cheap laptops. The install process will erase everything on the laptop, so you should back up your files before you start. You install it by putting the ISO file on a USB stick, booting from the USB, and following the instructions. When it’s done, you use the username “author” and password “password” to log in. Saving your work is manual—there is no autosave. You can change color themes, turn on word wrap, and use basic commands to move or delete files. Advanced users can switch text editors, change keyboard layouts, connect to Wi-Fi for syncing, or even encrypt the whole drive. There are guides for these options, but most people will just use it for writing.

In the Hacker News comments, some people really like the idea of a distraction-free writing tool. They say modern computers are full of things that make it hard to focus, so writerdeckOS is helpful. Others remember old hardware like the AlphaSmart and say this is a good way to use old laptops for something simple. Some users worry that erasing the whole laptop is risky, and wish there was a way to try it without wiping all data. A few comments suggest using just a “live USB” mode so you don’t need to install it. Some people want autosave, since losing work is frustrating. Others think you could just use Linux with a basic text editor and turn off Wi-Fi, but supporters say writerdeckOS is easier for beginners. There are also questions about battery life and support for different laptops, since some Chromebooks may not work. A few users share tips for similar setups, or ask if the system could support more formats. Overall, the comments show both excitement for simple tools and caution about using a system that erases everything.

---

## Avería: The Average Font (2011)

- 原文链接: [Avería: The Average Font (2011)](http://iotic.com/averia/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45859243)

This article is about Avería, a font made by averaging all the fonts on one person’s computer. The creator is not a type designer, but is interested in both typography and creative programming.

To make Avería, the author first overlaid each letter from many fonts at low opacity, using tools like ImageMagick and PHP, to see what the “average” letter would look like. This produced blurry letters, but some shapes, like the baseline, were clear. The author noticed that some letters, like lowercase “g,” showed two very different common forms. Wanting a font that kept sharper edges, the author tried to turn the blurry images into clear, single-color shapes by using thresholds, but this didn’t work well for all letters.

The author explored more technical ways to average shapes, reading about morphing between shapes and thinking about how font features (like serifs or stems) could be matched. This became complicated, so the author thought of a simpler method: breaking each letter’s outline into many points and just averaging the positions of matching points from all the fonts. This worked well and was simple.

After a month of work, the Avería font was ready. The name comes from the Spanish word for “breakdown,” which is related to the word “average.” The font is released under a free license and comes in several styles—Regular, Bold, Light, Italic, Serif, and Sans, using hundreds of fonts as the base.

The author thanks friends for design advice and encourages people to use the font freely in any project, commercial or not. There are also versions made from open fonts, and collections for easy installation.

In the Hacker News comments, some people think the project is a fun and creative way to explore what “average” really means in design. Others point out how hard it is to average fonts well, especially when letters have different common shapes—like the “g” or “a.” A few commenters mention other tools that can interpolate between fonts, such as Superpolator or FontForge, and discuss their own experiences with font design.

Some users are curious about how the font looks in real use, asking for examples or sharing links to try Avería. Others talk about possible uses, like in art projects or as a unique alternative to standard fonts. A few note that the font, since it is an average, might look odd or “uncanny” compared to traditional fonts, but that is also what makes it interesting.

One commenter wonders if this idea could work for other types of design, not just fonts. Another person says they appreciate that the font is open and free to use. There are also technical discussions about the challenges in matching points on letters and how averaging works when shapes are very different. Overall, people are impressed by the project’s mix of creativity, programming, and design.

---

## Cloudflare scrubs Aisuru botnet from top domains list

- 原文链接: [Cloudflare scrubs Aisuru botnet from top domains list](https://krebsonsecurity.com/2025/11/cloudflare-scrubs-aisuru-botnet-from-top-domains-list/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45857836)

Cloudflare noticed that strange domains linked to the Aisuru botnet were showing up above big names like Amazon and Google on their list of most-visited websites. These Aisuru domains appeared because the botnet, made from many hacked smart devices, was sending massive numbers of requests to Cloudflare’s DNS servers, making their domains look popular. Aisuru has grown fast since 2024 and can launch huge DDoS attacks, even reaching 30 terabits per second. 

The botnet used to send requests through Google’s DNS, but switched to Cloudflare’s 1.1.1.1 in October. This made Aisuru’s command domains show up in Cloudflare’s public rankings. Sometimes, these domains even included a real street address, or copied names of well-known cloud providers, which made people worry about privacy and confusion.

Cloudflare reacted by hiding parts of these malicious domain names in their rankings and added a warning to explain that not all top domains are safe or human-visited. CEO Matthew Prince said their system simply counts DNS queries, so the botnet could easily push its domains to the top by flooding the service. Cloudflare plans to improve the ranking to spot and filter out bot traffic better.

Renee Burton from Infoblox explained that the rankings can be misleading due to technical reasons like caching, server settings, and how traffic is shared between many systems. She said it’s not easy to show only real user data. Alex Greenland from Epi agreed, but felt Cloudflare’s rankings failed to keep out obvious bot traffic, which could hurt trust in their list. He said there should be two lists: one for real users, and one for all traffic, including bots.

Greenland also warned that many services trust Cloudflare’s top domains as safe, so letting malicious domains into these lists could cause wider security problems. For example, TRANCO, a respected list of top domains, uses Cloudflare’s data, so errors could spread.

Recently, Cloudflare started hiding Aisuru domains more completely, but data files still show some at the top. Most of the requests came from the US, matching earlier reports that many infected devices are on US networks. The botnet controls hundreds of servers, mainly using the old .su domain (from the Soviet Union days), which is often abused by cybercriminals. Experts suggest blocking .su domains or watching for them on your network as a simple way to spot Aisuru’s activity.

In the Hacker News comments, some readers agreed with Cloudflare’s choice to hide the botnet domains, saying it protects privacy and keeps people from trusting bad sites. Others worried this is just “security by obscurity,” and hiding data might make the rankings less useful or transparent. A few people argued that Cloudflare’s list was never meant to be a trust list, just a raw count of DNS requests, so the real issue is how people use the data. Some said services like TRANCO need to be more careful and not treat top-N lists as automatically safe. 

There was also debate about how hard it is to filter out bot traffic, with some commenters pointing out technical limits, like how DNS queries don’t show if the user is human or a bot. A few developers shared tips for blocking or alerting on .su domains, while others talked about the risks of IoT devices with default passwords being used in big attacks. Some readers said Cloudflare’s actions were too little, too late, and called for better industry standards to check domain popularity and safety. Overall, the discussion showed concern about the power of botnets, the trustworthiness of “top domain” lists, and the need for better ways to detect and block bad actors.

---

## Control structures in programming languages: from goto to algebraic effects

- 原文链接: [Control structures in programming languages: from goto to algebraic effects](http://xavierleroy.org/control-structures/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45798068)

This article is about a new book that explains how programming languages let us control the flow of our code, from simple “goto” commands to modern ideas like algebraic effects. The book covers the history of control structures, showing how they started in early languages and changed over time.

First, it talks about early programming languages like Fortran and Algol, where “goto” and basic loops were common. Then it explains how structured programming made code easier to read and less likely to have bugs by using things like “if,” “while,” and “for.” The book moves on to advanced ideas for imperative languages, such as generators, coroutines, and ways to invert control (like callbacks and iterators). There are code examples in Java, Python, OCaml, C++, and Haskell to help readers see how these ideas work.

Next, the book looks at control in functional languages. It explains things like continuations, which let you save the state of a program and jump back to it later. There are chapters about using “continuation-passing style” (CPS), special control operators like “callcc,” and even more powerful tools called algebraic effects and effect handlers. The book also covers exceptions, monads, and how these control structures make programs more flexible.

Finally, the book discusses how to think about and prove things about code that uses these advanced features. It introduces type and effect systems, Hoare logic, and separation logic, which help programmers understand and reason about programs with complex control flows.

Hacker News readers had many thoughts about this book. Some were excited to see a clear, detailed history of control structures and liked that the book included both theory and code examples. A few said this book would be helpful for anyone learning about programming languages, not just experts. Others discussed their own experiences with “goto”—some said it caused problems in the past, while others argued that “goto” can be useful if used carefully. There were comments about continuations and algebraic effects being hard to understand, but also powerful once you get used to them. A few readers pointed out that not many real-world languages have full support for algebraic effects yet, but they hope more languages will add them soon. Some people liked seeing code in different languages, saying it made the ideas easier to compare. Others wondered if all these advanced control features are really needed for most programming tasks, or if they just make code harder to understand. Finally, several readers thanked the author for sharing the book preview for free and said they looked forward to reading the whole thing.

---

## An Algebraic Language for the Manipulation of Symbolic Expressions (1958) [pdf]

- 原文链接: [An Algebraic Language for the Manipulation of Symbolic Expressions (1958) [pdf]](https://softwarepreservation.computerhistory.org/LISP/MIT/AIM-001.pdf)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45857045)

This article is about an early design for a new programming language by John McCarthy in 1958. The language lets computers work with symbolic expressions, such as math formulas or logic statements.

The main ideas are about how to handle lists and symbols inside a computer. The language is not finished, but it is already more flexible than older languages for doing things like algebra, theorem proving, and writing compilers. It uses lists to store expressions, where each list element is stored in a computer word, and you can have lists inside lists (sub-lists). This makes it easy to build and change complex data, like trees of possible actions or math formulas.

The language uses algebraic notation, which means you can write expressions without always naming the results in the middle. This makes the code simpler and closer to how people write math. You can use recursion, so a procedure can call itself, and temporary results are handled automatically with lists, without the programmer needing to manage memory.

Conditional expressions (if-then-else) are built-in, which helps reduce the need for lots of small "goto" jumps. The language allows functions and predicates (tests) to be used as arguments, making powerful tools for transforming data.

There are different types of data: numbers, whole words, true/false values, addresses, and functions. Statements can assign values, control the flow with "go" (like goto), set up arrays, and call subroutines. You can also declare properties for symbols, use compound statements, and define loops.

Lists can be erased, copied, searched, and mapped over with functions. When reading input, expressions are broken into parts and stored as lists. There are also examples for doing algebra, like differentiating a formula.

In the Hacker News comments, many people are amazed at how modern this old paper feels. Some say this is the start of Lisp, one of the most important programming languages for AI and symbolic math. Commenters note how the basic ideas here, like lists and recursion, are still core in many languages today.

A few users point out that the language is very close to how Lisp ended up, especially the use of "cons," "car," and "cdr" for list processing. Others highlight that McCarthy's focus on symbolic computation was different from most programming at the time, which was mostly about numbers.

Some commenters share that the explanation of how to store lists in memory is helpful, especially the use of bits in a machine word. Others mention that this paper shows the value of reading old research, as the problems and ideas are still important.

A few people discuss the challenges of working with early computers and how clever these early language designs had to be. Some are surprised by how much was already figured out in the 1950s, and they praise McCarthy for his clear thinking.

Overall, readers feel inspired by the history and by seeing that many programming ideas we use now were already present in these early days.

---

## My first fifteen compilers (2019)

- 原文链接: [My first fifteen compilers (2019)](https://blog.sigplan.org/2019/07/09/my-first-fifteen-compilers/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45776368)

This article is about building compilers by breaking the process into many small, simple steps. The author shares their experience in a compilers course where they wrote fifteen compilers in fifteen weeks, each one adding a little more complexity.

The main idea is the “nanopass” approach. In this method, a compiler is made up of many small passes. Each pass takes a program in one form and changes it a bit, either transforming it, optimizing it, or analyzing it. Over time, the program becomes more and more like the final output, which is usually machine code. The author’s course started with a simple compiler that just changed one kind of assembly to another. Each week, they added a new pass, making the input language a bit more like a real programming language. At the end, the author had a compiler with 43 small passes that could turn a large part of the Scheme language into x86-64 assembly.

The nanopass idea is supported by open-source tools that make it easier to write and organize these little passes. The author compares this to how parser combinator libraries let you build up complex parsers out of small pieces. The approach is modular: each part is simple, easy to test, and easy to understand.

The course also used a “back-to-front” strategy. Instead of starting with the front end (parsing the programming language), students started with the backend (producing machine code) and worked backwards. This meant that after each week, students had a working compiler that produced real executable programs. This was motivating, because you could see and run what you built right away.

This way of building compilers is rare, but the author argues it makes compiler writing less scary and more fun. It is similar to another approach, where you first build a simple compiler for a small part of the language, then keep adding features. Both methods let you always have something that works, which helps you keep going.

The author says learning compilers this way gave them the confidence to work on real-world compilers later, like the Rust compiler at Mozilla. The skills didn’t all transfer directly—Rust’s compiler is much more complex—but the experience made compilers feel possible.

In the comments, many readers said they wish more courses and books used the nanopass or small-step approach. Some people liked how this method gives quick feedback and helps you avoid getting lost in a big, scary project. Others pointed out that in industry, compilers often have fewer passes for speed reasons, but they agreed that for learning, many small passes are easier to manage. Some readers shared their own experiences with similar teaching methods and said it helped them a lot. A few people wondered if this approach could work for other big projects, not just compilers. There were also comments about how the open-source tools mentioned in the article are useful for learning, but maybe less common in production code. Some users worried that too many passes could slow things down in a real-world compiler, but others replied that modular code is easier to maintain and debug. Finally, several readers said this article made them want to try writing a simple compiler for fun.

---

## Humans have remote touch 'seventh sense' like sandpipers

- 原文链接: [Humans have remote touch 'seventh sense' like sandpipers](https://techxplore.com/news/2025-11-humans-remote-seventh-sandpipers.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45859928)

Scientists found that humans can sense hidden objects in sand before touching them, much like how sandpiper birds find food under the sand. In the study, people moved their fingers through sand to find a cube buried below, using only small changes in how the sand felt.

The research shows human hands are more sensitive than we thought. People could notice tiny movements in sand when there was a hidden object, even before their fingers touched it. This is similar to how some birds use their beaks to feel things in the sand. The team also tested a robot arm with a special touch sensor. The robot could sometimes sense objects from a greater distance, but it made more mistakes, giving more false results than humans.

Humans had a success rate of about 70%, while the robot had only 40% precision. Both humans and robots worked close to the best possible levels predicted by science. The study changes how we think about the sense of touch, showing it can work at a distance, not just by direct contact. The findings could help make better robots or tools that need to sense things under sand, like for archaeology or space exploration.

Experts involved say this is the first time remote touch was tested in humans. They believe this new knowledge could help design assistive devices and smarter robots. The work also shows how mixing psychology and robotics leads to new ideas and better technology.

In the comments, many people were surprised by how sensitive the human sense of touch can be. Some shared stories about feeling objects under loose soil or sand, saying they always thought it was just luck. Others talked about how similar tricks are used in search and rescue or even cooking, like feeling for bones in fish.

A few readers wondered if this skill could be improved with practice, or if some people are naturally better at it. One comment asked about the limits—how deep or far away an object could be before it becomes impossible to sense. Some users discussed the robot results and thought robots would soon catch up with humans as sensors get better.

Another group of comments focused on uses for this skill, suggesting ideas like safer landmine detection, or using it in medicine for finding things in the body. Some were excited about using this for robots on Mars or the ocean floor. A few doubted the practical value, saying most people never need to find things buried in sand, but others replied that even small improvements in touch could be helpful in many fields.

Overall, the discussion showed a mix of excitement, curiosity, and practical thinking about where this research could lead.

---

## Near mid-air collision at LAX between American Airlines and ITA [video]

- 原文链接: [Near mid-air collision at LAX between American Airlines and ITA [video]](https://www.youtube.com/watch?v=-j76cp7bETw)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=45859216)

A near mid-air crash happened at Los Angeles Airport (LAX) when one plane made a wrong turn after takeoff and almost hit another plane. The video shows that the ITA pilot turned left instead of following the planned path, coming very close to an American Airlines plane, but both pilots and air traffic controllers acted quickly to stop an accident.

The video explains that at LAX, planes have to follow strict routes after takeoff because it is a very busy airport. The error started when the ITA crew possibly programmed their flight computer wrong or misunderstood their departure instructions. The American Airlines pilot saw the problem and calmly told air traffic control, who then gave new orders to both planes. Both crews responded quickly and avoided each other. The video also points out that these types of mistakes at LAX, especially with the “West Configuration” of runways, have happened before. The video creator notes this is not an official replay, but a recreated radar view for learning.

Many comments praise the American Airlines pilot for staying calm and professional, saying they “owned the situation.” Others thank the air traffic controllers for their fast work, even though their job is very hard and often underpaid. Some pilots in the comments share that these mistakes can happen when programming the flight computers, especially when there is a runway change or confusing instructions. They say it is important to double-check everything and work together as a team.

Some people joke about how pilots never want to be featured in videos like this, while others make light of the “possible pilot deviation” warning, which means a pilot may have broken a rule. A few users ask if LAX needs better training or to review its current procedures, since similar incidents keep happening. One user explains that in the U.S., pilots often have to mix different navigation instructions, so reading the full chart and instructions is very important. Another person says this is why humans are still needed in airplanes—to see and fix problems quickly, not just rely on computers.

A few people mention that their airlines require triple-checking the route, and some suggest making a documentary to bring more attention to these close calls. One user says that even with all the technology, clear talking and teamwork are the keys to safety. Many agree that everyone did their job well this time, but it was a close call, and everyone should always pay attention to avoid these mistakes in the future.

---

