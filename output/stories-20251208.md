# Hacker News 故事摘要 - 2025-12-08

## 今日概述

Today’s top Hacker News stories cover data safety problems in distributed systems, Japan’s earthquake response, new tools for GPU debugging, and running VPNs on Kindles. There are stories on North Korea’s hidden fiber network, IBM buying Confluent, and the risks in Nvidia’s AI chip business. AI coding tools and smarter coding agents are making software faster to build. Many enjoyed clever tiny code demos in graphics programming. If you like tech, safety, AI, or hardware, today’s stories have something for you.

---

## Jepsen: NATS 2.12.1

- 原文链接: [Jepsen: NATS 2.12.1](https://jepsen.io/analyses/nats-2.12.1)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46196105)

This article is about Jepsen’s analysis of NATS JetStream 2.12.1, a popular system for sending and storing messages across a cluster of computers. Jepsen tested how well NATS JetStream keeps data safe when things go wrong, like power failures or file corruption.

The main points of the article show that while NATS JetStream promises that messages are safe after you get an acknowledgment, this is not always true in practice. Jepsen found that if data files get corrupted or are partly deleted on just a few nodes, many messages can disappear—even if those messages were already acknowledged. The same problem happens if several nodes lose power or crash around the same time: you can lose many recent messages, even though the system told you they were saved.

One big reason for this is that NATS does not write messages to disk right away by default. Instead, it saves up messages and writes them every two minutes. If a crash happens before the next disk write, all the recent messages can be lost. This is called a “lazy fsync” policy, and it is risky for data safety. Some other systems, like MongoDB or Zookeeper, write to disk before confirming a message is saved. NATS allows you to change this, but it comes with a performance cost.

Another finding is that sometimes, even a single node crash—if combined with other problems like network delays—can cause not just message loss but also “split-brain.” This means different parts of the cluster disagree on what messages exist, leading to confusion and more data loss.

Jepsen also tested older versions and found that, in version 2.10.22, a crash could erase whole streams of data. This bug was fixed in version 2.10.23. The article highlights several open issues where NATS still loses data or deletes streams after certain faults, and these are being investigated by the NATS team.

The article points out that while NATS documentation claims the system is always available and fully consistent, this is not possible because of the CAP theorem. In reality, NATS behaves like other systems using Raft: it works if a majority of nodes are up, but can lose data in rare but possible cases.

From the comments, some readers are surprised and concerned that NATS, a widely used system, has these data loss problems by default. They appreciate Jepsen’s deep testing and clear reporting of real-world faults. Some people note that most users may not realize they need to change the disk flush setting to protect their data, and that the documentation should warn about this more clearly.

Others say that “lazy fsync” is common for better speed, but users should be aware of the risk, especially in systems where data loss is unacceptable. A few argue that no distributed system can be perfect, and trade-offs between speed and safety are normal. Some discuss how other systems like Kafka or etcd handle similar issues, showing this is a common challenge.

There are also comments about the value of Jepsen’s approach—actually breaking things to see what fails—versus just trusting what the docs say. Some users share their own stories of data loss in production and stress that strong defaults and clear warnings are very important.

A few people defend NATS, saying that with careful setup, the risks can be managed, and that the system is still very useful for many cases. Others wish for better tools or settings to make safe operation easier out of the box.

Overall, the comments show a mix of concern, respect for the testing, and a reminder that all distributed systems have complex trade-offs. People agree that more clear documentation and better defaults would help users avoid surprises.

---

## Strong earthquake hits northern Japan, tsunami warning issued

- 原文链接: [Strong earthquake hits northern Japan, tsunami warning issued](https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46192846)

A strong earthquake hit northern Japan, with a magnitude of 7.5, late on Monday night. The quake caused tsunami warnings and advisories, but these were later lifted after only small tsunami waves were seen.

The earthquake’s center was near Aomori Prefecture, about 54 kilometers deep. People in Hachinohe city felt very strong shaking, and six people were hurt by falling or moving objects. Tsunami warnings went out for parts of Iwate, Hokkaido, and Aomori. The largest tsunami reported was 70 centimeters at Kuji Port. After three hours, the warning was downgraded, and by the next morning, all tsunami advisories were gone.

The quake caused "long-period ground motions," which can be dangerous for high-rise buildings because they shake for a long time. In Rokkasho, these motions made it hard for people in tall buildings to stand.

Officials warned that a much bigger "mega quake" could happen soon in the same area, possibly causing larger tsunamis. They asked people to check evacuation routes and emergency supplies, and to stay alert for the next week. Some towns ordered people to evacuate right after the quake.

Train lines, including the Shinkansen, stopped between Fukushima and Shin-Aomori while tracks were checked for damage. Some highways and airports also stopped or checked services, but most traffic returned to normal by Tuesday. Airlines said they expected to fly as usual.

Nuclear power plants in the area were checked, and no problems were found. Treated water releases at Fukushima Daiichi were paused as a safety step, but there was no danger reported.

The government quickly set up a crisis team and asked local officials to help with evacuations and information. Rescue and relief efforts were made a top priority. Experts said the quake was big and happened at a plate boundary, and warned people to keep away from the coast.

Hacker News comments showed many people were worried about the risk of aftershocks or a bigger quake. Some praised Japan’s early warning systems and strict building rules, saying these saved lives. Others remembered the 2011 earthquake and tsunami, comparing the response and sharing concern for nuclear safety. A few users discussed the impact on travel and business, while others asked about how Japanese cities prepare for disasters. Some commenters expressed relief that the tsunami was small, and others shared stories of living through earthquakes in Japan. People also talked about how quickly trains and services stopped, showing Japan’s focus on safety. A few questioned if the mega quake warning system causes too much fear, while others thought it was better to be safe.

---

## AMD GPU Debugger

- 原文链接: [AMD GPU Debugger](https://thegeeko.me/blog/amd-gpu-debugging/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46193931)

This article is about building a real debugger for AMD GPUs, similar to the ones we have for CPUs, so developers can pause and inspect code running on the GPU. The author explains that GPU debugging is hard because GPUs work with many threads at once and don’t have easy tools for stopping and checking what’s happening.

First, the author tried talking directly to the GPU by creating a simple program that runs a shader without using Vulkan. They used low-level libraries to open the GPU device, allocate memory buffers, and send commands by hand. The process involved setting up memory for code and commands, mapping this memory for both CPU and GPU, and then loading compiled shader code into it. Commands need to be packed in a special format (PM4 packets) and sent to the GPU.

To make the GPU stop and let the CPU inspect things, the author used special registers called TBA and TMA that set up a “trap handler.” This handler runs when the GPU hits a breakpoint or trap. But writing to these registers from user space is tricky, so the author used a debug interface in Linux called debugfs, and used tricks to set the right values for all possible “contexts” (VMIDs).

The trap handler itself is a small program that saves the GPU’s state into memory, signals the CPU, and waits for the CPU to say it can continue. The CPU can then pause, read the saved state, and even halt or resume the GPU by writing to another register. Restoring the GPU’s state is complex, but the article shows how to do it step by step.

For compiling real code, the author used RADV’s “null_winsys” mode, which lets you compile SPIR-V shaders without running them, so you can feed the GPU machine code directly. The author also explains how to add real debugger features, like stepping through code, setting breakpoints, watching memory addresses, and mapping code back to source lines. There are plans to improve variable names and types, and maybe to better integrate with Vulkan for a more user-friendly debugger. At the end, the article shares some bonus code for walking GPU memory pages.

In the comments, many people are impressed by the technical depth and call this work “awesome” or even “legendary.” Some say that GPU debugging is a huge pain point and hope this project becomes a real tool. Others point out that AMD’s stack is very complex and hard to document, so they admire the author’s persistence. A few ask if this could ever work on Nvidia GPUs or Intel GPUs, and some discuss the differences in hardware support for debugging. There are also people who warn that this kind of low-level work is risky and can crash your system, but say it’s great for learning. Some users wish GPU vendors made these features easier to use, and a few share stories about how hard it is to debug shaders today. Others ask practical questions about how breakpoints work or how safe it is to set trap handlers on all VMIDs. A few wonder if this could be turned into a plugin for existing tools, while others discuss possible security risks of exposing these debug features to user programs. Overall, the tone is very positive, with many people thanking the author for sharing such detailed knowledge.

---

## Let's put Tailscale on a jailbroken Kindle

- 原文链接: [Let's put Tailscale on a jailbroken Kindle](https://tailscale.com/blog/tailscale-jailbroken-kindle)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46194337)

This article talks about how to run Tailscale, a VPN tool, on a jailbroken Kindle e-reader. The author explains why someone might want to do this and gives a step-by-step guide on how to get it working.

First, jailbreaking a Kindle means removing the software limits set by Amazon. This lets you use custom apps, change screensavers, and read more kinds of ebooks, not just Amazon’s. Jailbreaking is popular for many devices, not just Kindles. The process depends on your Kindle’s version, and it can be risky—you might break your device or lose your warranty.

The article explains that Tailscale is not required, but it makes using a jailbroken Kindle much better. With Tailscale, your Kindle gets a fixed IP address. You can connect to it easily using SSH, and send files straight to it with Taildrop. This helps move ebooks or documents without plugging in a USB cable.

To get started, you need to check your Kindle’s version and find the right jailbreak method, like “AdBreak” or “WinterBreak.” After jailbreaking, you install a launcher (KUAL) and tools (MRPI) to run other apps. Then, you follow extra steps to add Tailscale. This includes copying files to your Kindle, getting a special key from Tailscale, and setting up where Taildrop files should go.

Once everything is set up, you can manage files, install more apps, and even use a Bluetooth keyboard. The biggest benefit is sending books or files from any device straight to your Kindle, which saves time and makes it more useful.

In the Hacker News comments, some people are excited about running Tailscale on odd devices and love the idea of making Kindles more open. Others share their own experiences jailbreaking Kindles for better reading apps or to get rid of ads. Some users warn about the risks, like bricking devices or getting locked out after updates.

A few commenters say they wish Amazon would make Kindles more open by default. Some think the process is too technical for most people, but like that guides exist for those who want to try. One person points out that installing Linux tools on a Kindle is fun, but the hardware is slow, so it’s best for simple things.

Others discuss the ethics of jailbreaking, saying owners should have more control over the devices they buy. Some are worried about security when installing custom software. People also ask if it’s really worth all the trouble, or if just using a different e-reader is better.

Overall, the thread shows a split between people who love tinkering and those who prefer easier, safer solutions. But many agree that opening up devices can make them a lot more fun and useful.

---

## Hunting for North Korean Fiber Optic Cables

- 原文链接: [Hunting for North Korean Fiber Optic Cables](https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46194384)

This article looks at how fiber optic cables are set up inside North Korea, using clues from public reports, photos, and network tests. The writer started this search after seeing a PowerPoint from North Korea showing a fiber line across the country, which raised questions about where these cables really go.

There is very little public information about North Korea’s network, but some outside sources help. A report from 38North said that in 2017, a Russian company connected a fiber cable to North Korea over the Russia–Korea Friendship Bridge. A photo near the border shows something in the grass by the railway, which might be buried fiber cable. Other reports say the first lines in North Korea were built in the 1990s, joining cities like Pyongyang, Hamhung, Sinuiju, and Nampo. North Korea’s “Kwangmyong” network, which is their national intranet, also uses fiber and links all provinces.

Maps and cell tower data suggest that main fiber lines follow big roads and railways, mostly along the east coast. The Pyongra railway line, for example, connects Pyongyang to cities like Wonsan, Hamhung, Chongjin, and Rajin. In some places, photos show utility boxes and paths along the tracks, hinting at buried cables. Where the railway goes through mountains, it’s guessed the cables use highways instead.

North Korea also gets internet from China, entering at Sinuiju. Most servers are probably in Pyongyang, but it's not clear. Tests with traceroute show that the Russian connection has low delay near the border, while the Chinese path takes longer, maybe because the network is more complex inside the country.

The main lesson is that North Korea’s fiber network probably follows railways and highways, with connections from both Russia and China, but many details are still guesses.

In the comments, some people are impressed by the detective work and use of open sources to map the cables. Others point out that the article relies on many assumptions, so the real network may be different. A few note that using old reports and photos is risky because North Korea may have changed things since then. Some wonder why North Korea would put so much effort into a fiber network when most citizens cannot access the global internet. Others add that even a closed network is useful for government, military, and local communication. A few think the author should be careful sharing this kind of research, as it could be seen as sensitive. There’s also talk about whether the US or other countries try to tap these cables, or if North Korea has protections in place. A few readers wish for more technical details, like how data is split between the Russian and Chinese routes. Finally, some thank the author for sharing rare information about such a secretive place, even if much remains uncertain.

---

## IBM to acquire Confluent

- 原文链接: [IBM to acquire Confluent](https://www.confluent.io/blog/ibm-to-acquire-confluent/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46192130)

IBM will buy Confluent, a company known for real-time data streaming and for building on Apache Kafka. The deal is for $31 per share in cash, and Confluent will keep its own brand inside IBM after the purchase.

The article says this move will help IBM and Confluent offer better tools for companies to use their data in the cloud and for AI. The Confluent CEO, Jay Kreps, thanked his team and said the company’s mission will not change. He believes that working with IBM will help Confluent’s technology reach more businesses around the world. IBM already has experience with open-source companies, like when it bought Red Hat and HashiCorp. Jay Kreps says this history makes IBM a good partner. The letter also tells Confluent employees that their jobs, pay, and daily work will stay the same until the deal is fully done, which could take until mid-2026. The CEO promises to be open with the team about any changes. The article ends with legal details and tells shareholders and investors to look out for more official documents about the deal.

In the comments, many people are worried that IBM will slow down Confluent’s growth or make it less exciting. Some say that when IBM bought Red Hat, things changed, and they fear the same will happen to Confluent. Others think that Confluent’s technology will now be harder for small companies to use, or that IBM will make it more expensive. A few users say this is a smart move for IBM, because streaming data is important for AI and modern businesses. Some are happy for the Confluent team, saying they worked hard and deserve the reward. A few point out that big companies often do not manage new teams well after buying them. Some wonder if open-source projects like Kafka will change because of this deal. Others remember that IBM has a mixed record with past tech buys, sometimes making the products worse. Still, a few comments hope that with more resources, Confluent can build better tools. There is also talk about how this deal shows that real-time data streaming is a big business now. Overall, the comment section is split between worry and hope about what IBM will do next.

---

## Deep dive on Nvidia circular funding

- 原文链接: [Deep dive on Nvidia circular funding](https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46196076)

This article looks at Nvidia’s recent big profits and questions if their growth is as solid as it seems. The writer checks Nvidia’s financial numbers, its ties with OpenAI and Oracle, and talks about “circular funding”—where money moves in a loop among these companies.

Nvidia’s revenue is way up, mostly from selling chips for AI data centers. But there are some worries: Nvidia’s cash flow is lower than its reported income; its inventory has doubled, which is a risk; and it’s taking longer to get paid by customers. The article says Nvidia is betting big on its next chip, Blackwell, to sell fast.

The “circular funding” idea is that Nvidia invests in OpenAI, OpenAI signs a huge cloud deal with Oracle, and Oracle then buys billions of dollars of Nvidia chips. Some, like Michael Burry, think this could mean Nvidia’s sales aren’t as strong as they look, since the money just cycles around. If Nvidia stopped investing, would the other deals still happen?

The article also says OpenAI is trying to depend less on Nvidia. OpenAI is buying memory chips directly, hiring chip experts, and working with Broadcom to make custom hardware. This could let OpenAI run its AI on its own chips, not just Nvidia’s.

There’s a suggestion that Oracle should buy Groq, a startup making faster, cheaper chips using SRAM instead of HBM memory. This could solve supply problems and help Oracle’s profits, as renting Nvidia chips is expensive. But it’s unclear if Nvidia would allow this, since it could break the current loop of deals.

The article finishes by saying all three companies—Nvidia, OpenAI, and Oracle—are tightly linked, but each is trying to get an edge. The future of AI hardware is uncertain and very competitive.

People in the comments have many views. Some agree that Nvidia’s growth is risky and may not last, especially if OpenAI or Oracle switch to new chips. Others think Nvidia’s tech lead is real and their chips will stay in high demand for years. A few readers are skeptical about the “circular funding” claims, saying these types of deals are common in the industry. Some worry about too much power in one company’s hands, while others say that’s just how fast-moving tech works. There are comments about how hard it is to make new chips and how long it will take OpenAI or Groq to catch up. One person points out that even if OpenAI uses its own chips, many others will still need Nvidia hardware. Overall, readers are curious but cautious, watching to see if Nvidia can keep its position as more rivals try to break into the AI hardware game.

---

## Has the cost of building software dropped 90%?

- 原文链接: [Has the cost of building software dropped 90%?](https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46196228)

This article says new AI coding tools can make building software much cheaper and faster. The writer has built software for 20 years and thinks 2026 will surprise many people because of these new AI agents.

Before, making software took a long time and needed many people. Teams spent lots of time not just coding, but talking, fixing problems, and waiting for others. Even simple projects had big costs because of work like creating tests, making dashboards, and connecting different services. In the past, things like open source made software a bit cheaper, but it still was slow and complex. Cloud computing may have helped, but not as much as people say.

Now, the writer says AI coding agents can do a huge part of the work. For example, an AI can write hundreds of tests in a few hours—something that would take a person days. AI tools can turn business ideas into working APIs and services very quickly. Projects that took a month now take a week. With fewer people needed, there is less time wasted on meetings and handoffs.

The article says this does not mean fewer jobs, but more demand. Like how cheaper electric lights made people use more light, cheaper software means more people will want software. Many businesses have old Excel sheets that could become apps if it was cheaper to build them.

The writer thinks that domain knowledge—knowing the business and the right technical choices—matters more than ever. A developer who knows the business and uses AI well can work much faster and better. Teams can be smaller—maybe just a developer and a business expert, not big squads. This makes it easy to try things, throw away ideas that don't work, and start again quickly.

AI tools are getting better fast, but many developers are still unsure. Some say AI makes mistakes or can't understand old code. The writer says these problems are going away quickly, and not seeing the change is like people who ignored the iPhone at first. People who learn and use these tools now will have a big advantage.

In the comments, many people agree AI tools make things faster, but some say real projects are still hard. Some developers worry that AI can’t handle complex business needs or old, messy code. Others say the biggest savings are for simple, new projects, but big companies with lots of rules move slower and may not see the benefits soon.

A few commenters say that even with AI, you still need good thinking and planning. If you don’t know what you want, AI can’t help much. Some think the real value is in knowing the business, not just typing code faster. People point out that software is often slow because of company rules and meetings, not just coding itself.

Others are excited, saying small teams can now do what big teams did before. Some share stories of building tools with AI in just days. A few warn that if everyone can make software, we might see lots of low-quality apps. Some ask if the cost will really drop 90% everywhere, or just for some types of work.

Overall, the comment section shows hope and excitement, but also some doubt. Many agree that AI will change software, but not everyone thinks it will be easy or fast for every company.

---

## Launch HN: Nia (YC S25) – Give better context to coding agents

- 原文链接: [Launch HN: Nia (YC S25) – Give better context to coding agents](https://www.trynia.ai/)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46194828)

This post is about a new startup called Nia, which is part of Y Combinator’s Summer 2025 group. Nia wants to help coding agents—like AI tools that write code—by giving them more context about the codebase they’re working on. The goal is to make AI agents smarter and more helpful so they can solve harder coding problems.

The founders noticed that coding agents often make mistakes because they don’t understand the whole project or its history. Nia gives these agents extra information, like how parts of the code are used, why changes were made, or what bugs were fixed in the past. This can help the AI make better choices, like using the right functions or following project rules.

Nia does this by connecting to tools that teams already use. For example, it can look at GitHub for code changes, or link to documentation and bug trackers. The system then puts all this information together and gives it to coding agents in a way they can understand. The hope is that everyone—from solo developers to big teams—can save time and avoid annoying mistakes.

Some technical points: Nia can scan big codebases and keep track of updates. It works with different languages and frameworks. The founders say it’s easy to set up, and you can control what information is shared with the AI. They also focus on privacy, so sensitive data is protected.

In the Hacker News comments, some people like the idea and say that AI coding agents really need better context. They think this could help AI write code that actually works and is easier to read. Others are more careful—some worry that giving too much information to AI agents could be a security risk, especially with private code. A few developers say they have tried similar tools, but the AI still made silly mistakes. There are questions about how well Nia works with very large or messy codebases. Some people are excited to try it, while others want to see more live demos or case studies before they trust it in real projects. Overall, the main feeling is that this is a problem worth solving, but it will take more time and testing to get it right.

---

## A series of tricks and techniques I learned doing tiny GLSL demos

- 原文链接: [A series of tricks and techniques I learned doing tiny GLSL demos](https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html)
- HN链接: [Hacker News讨论](https://news.ycombinator.com/item?id=46194477)

This article talks about small code “demos” written in GLSL, a language for graphics on computers. The author shares tricks and lessons learned while making four tiny demos, each fitting in about 512 characters.

First, the “Moonlight” demo uses a simple way to make glowing effects. Instead of a big, complex formula, the author uses a trick: at every step, the brightness is set to 1 divided by the distance from the object. This looks nice and, as the author explains, is close to real physics. The article shows how this “1/d” rule comes from how light spreads out, using math and simple code examples. For objects you can see through, the author changes the formula to control how much light passes and gets absorbed.

Next, in “Entrance 3,” the author talks about using cubes for all shapes, which makes the math easier. The demo has real lighting: after finding where the viewer looks, it checks if light from a lamp can reach that spot. There were some small problems with graphics drivers on phones, but the author found ways to fix them, like using code macros. The article also explains how to set up a camera for isometric (angled) views, and includes math tricks for this.

The “Archipelago” demo creates endless islands using a special noise formula. The author uses “domain warping” to make the coastlines look interesting. The method is based on combining waves and rotating them, which mixes the land and water shapes.

In the “Cutie” demo, the author tries to make a cute character from rounded shapes. Here, a smooth “min” function lets two spheres blend smoothly, making the limbs look natural. For animation, the author uses simple math for moving the legs like real joints. The number of steps in the drawing loop is used to make the outlines look better.

The author likes the small code size (512 characters) because it forces focus, helps finish projects, and turns code itself into art. The size limit comes from the size of a post on Mastodon, a social network. The author enjoys the challenge and plans to make more demos.

In the comments, some people say these demos are amazing and show a lot of creativity in a small space. Others ask technical questions, like how the author makes the code so short, or how to learn more about these tricks. A few commenters share their own stories about making tiny shaders and talk about the fun and frustration of fitting ideas into a small space. Some mention that the “1/d” trick is clever and ask if it always looks good; others talk about their favorite coding tricks or share links to similar projects.

There are comments about the difficulty of reading such dense code. Some people wish the code was easier to understand, but others enjoy the puzzle. A few discuss the beauty of small, well-made programs, and how constraints can drive creativity. Some mention that more people should try making tiny demos, as it’s a good way to learn graphics programming. Others ask about the math used for isometric views, saying it helps them understand 3D graphics better.

Overall, commenters agree that these tiny GLSL demos are both fun and impressive, and they appreciate the author sharing tips and code.

---

